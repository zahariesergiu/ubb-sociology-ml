{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOUZ09PkEIXFnvzskR6adFl"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import numpy as np"],"metadata":{"id":"uMn-eTbKltES"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 1. Dummy gradient descent"],"metadata":{"id":"aTLu93arl1e3"}},{"cell_type":"markdown","source":["f(x) = ax+b"],"metadata":{"id":"tMqe_ZyG8D2b"}},{"cell_type":"code","source":["# weight = 0.5\n","weight = np.random.rand()\n","print('initial weight: ', weight)\n","\n","input = 0.5\n","goal_prediction = 0.8\n","alpha = 1.5\n","for iteration in range(100):\n","  prediction = input * weight\n","  loss = (goal_prediction - prediction) ** 2 # MSE\n","  derivative = input * (prediction - goal_prediction)\n","  weight = weight - alpha*derivative\n","\n","  if iteration % 1==0:\n","    print(f\"Iteration {iteration + 1}: Weight: {weight:.5f}, Prediction: {prediction:.5f}, Loss: {loss :.5f},\")\n","\n","  if abs(goal_prediction - prediction) < abs(0.01):\n","        break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kihsvie_l7_t","executionInfo":{"status":"ok","timestamp":1738088261792,"user_tz":-120,"elapsed":340,"user":{"displayName":"Zaharie Sergiu","userId":"05122149963084415420"}},"outputId":"21249d8e-ae1c-4511-a921-de067eaaf77b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["initial weight:  0.45139541123313764\n","Iteration 1: Weight: 0.88212, Prediction: 0.22570, Loss: 0.32982,\n","Iteration 2: Weight: 1.15133, Prediction: 0.44106, Loss: 0.12884,\n","Iteration 3: Weight: 1.31958, Prediction: 0.57566, Loss: 0.05033,\n","Iteration 4: Weight: 1.42474, Prediction: 0.65979, Loss: 0.01966,\n","Iteration 5: Weight: 1.49046, Prediction: 0.71237, Loss: 0.00768,\n","Iteration 6: Weight: 1.53154, Prediction: 0.74523, Loss: 0.00300,\n","Iteration 7: Weight: 1.55721, Prediction: 0.76577, Loss: 0.00117,\n","Iteration 8: Weight: 1.57326, Prediction: 0.77861, Loss: 0.00046,\n","Iteration 9: Weight: 1.58329, Prediction: 0.78663, Loss: 0.00018,\n","Iteration 10: Weight: 1.58955, Prediction: 0.79164, Loss: 0.00007,\n"]}]},{"cell_type":"markdown","source":["#2 Add bias"],"metadata":{"id":"oLbB7_odmYGF"}},{"cell_type":"code","source":["# Initialize weight and bias\n","weight = np.random.rand()  # Random initial weight\n","bias = np.random.rand()    # Random initial bias\n","print('Initial weight:', weight)\n","print('Initial bias:', bias)\n","\n","# Inputs and settings\n","input = 0.5\n","goal_prediction = 0.8\n","alpha = 1  # Learning rate (experiment with 10 > 0.0001)\n","\n","# Training loop\n","for iteration in range(20):\n","    # Prediction with bias\n","    prediction = (input * weight) + bias\n","\n","    # Error calculation\n","    error = (goal_prediction - prediction) ** 2\n","\n","    # Gradients (derivatives)\n","    derivative_weight = input * (prediction - goal_prediction)  # Gradient for weight\n","    derivative_bias = (prediction - goal_prediction)           # Gradient for bias\n","\n","    # Update weight and bias\n","    weight = weight - (alpha * derivative_weight)\n","    bias = bias - (alpha * derivative_bias)\n","\n","    # Print results\n","    print(f\"Iteration {iteration + 1}: Weight: {weight:.5f}, Bias: {bias:.5f}, Error: {error:.5f}, Prediction: {prediction:.5f}\")\n","\n","    # Convergence condition\n","    if abs(goal_prediction - prediction) < abs(0.01):\n","        break\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NzdC7Hocl2B5","executionInfo":{"status":"ok","timestamp":1738085895705,"user_tz":-120,"elapsed":236,"user":{"displayName":"Zaharie Sergiu","userId":"05122149963084415420"}},"outputId":"1320f839-67ae-42da-d7f2-37c1a6e1a4aa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Initial weight: 0.9061397206615539\n","Initial bias: 0.49544008369394765\n","Iteration 1: Weight: 0.83188, Bias: 0.34693, Error: 0.02206, Prediction: 0.94851\n","Iteration 2: Weight: 0.85045, Bias: 0.38406, Error: 0.00138, Prediction: 0.76287\n","Iteration 3: Weight: 0.84581, Bias: 0.37478, Error: 0.00009, Prediction: 0.80928\n"]}]},{"cell_type":"code","source":["0.5*0.84581+0.37478"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4dWOMKMa_iOV","executionInfo":{"status":"ok","timestamp":1738085946777,"user_tz":-120,"elapsed":205,"user":{"displayName":"Zaharie Sergiu","userId":"05122149963084415420"}},"outputId":"6854f567-c76e-4930-90f1-3438c875697c"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.797685"]},"metadata":{},"execution_count":59}]},{"cell_type":"code","source":["abs(0.01)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FiTUgkoBpg0J","executionInfo":{"status":"ok","timestamp":1738080131752,"user_tz":-120,"elapsed":222,"user":{"displayName":"Zaharie Sergiu","userId":"05122149963084415420"}},"outputId":"bef1e3fb-6c84-4463-e265-8635580cb3c2"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.01"]},"metadata":{},"execution_count":28}]},{"cell_type":"markdown","source":["# 3 More losses, metrics\n","\n","\n"],"metadata":{"id":"_8SfdlzF1-4P"}},{"cell_type":"code","source":["# Mean Squared Error (MSE)\n","def mse(y, y_pred):\n","    return (y - y_pred) ** 2\n","\n","def mse_derivative(x, y, y_pred):\n","    return -2 * x * (y - y_pred)\n","\n","# Mean Absolute Error (MAE)\n","def mae(y, y_pred):\n","    return abs(y - y_pred)\n","\n","def mae_derivative(x, y, y_pred):\n","    return -x if (y - y_pred) > 0 else x if (y - y_pred) < 0 else 0\n","\n","# Root Mean Squared Error (RMSE)\n","def rmse(y, y_pred):\n","    return ((y - y_pred) ** 2) ** 0.5\n","\n","def rmse_derivative(x, y, y_pred):\n","    return -x * (y - y_pred) / abs(y - y_pred) if (y - y_pred) != 0 else 0\n","\n","losses = {\n","    'mse': {\n","        'loss': mse,\n","        'derivative': mse_derivative\n","    },\n","    'mae': {\n","        'loss': mae,\n","        'derivative': mae_derivative\n","    },\n","    'rmse': {\n","        'loss': rmse,\n","        'derivative': rmse_derivative\n","    }\n","}\n","\n","metrics = {\n","    'rmse': rmse,\n","    'mse': mse,\n","}\n","\n","loss_name = 'rmse'\n","metric_name = 'rmse'\n","\n","loss = losses[loss_name]['loss']\n","loss_derivative = losses[loss_name]['derivative']\n","\n","metric = metrics[metric_name]\n","\n","weight = 0.5\n","weight = np.random.rand()\n","print('initial weight: ', weight)\n","\n","input = 0.5 # our data, value of 1 feature, for 1 record\n","real = 0.8 #\n","alpha = 0.1 # learning_rate, make it crazy small or crazy big. What happens? 10 > 0.0001\n","\n","for iteration in range(100):\n","  predicted = input * weight\n","  error = loss(real, predicted)\n","  derivative = loss_derivative(input, real, predicted)\n","  weight = weight - (alpha * derivative) # Add learning rate into formula\n","\n","  metric_score = metric(real, predicted)\n","\n","  print(f\"Iteration {iteration + 1}: Weight: {weight:.5f}, Loss: {error:.5f}, Prediction: {predicted:.5f}, Metric: {metric_score:.5f}\")\n","  if metric_score < 0.01:\n","    break"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Yc4VNYB62CP3","executionInfo":{"status":"ok","timestamp":1738088228193,"user_tz":-120,"elapsed":237,"user":{"displayName":"Zaharie Sergiu","userId":"05122149963084415420"}},"outputId":"2f285921-a6c5-4b1d-b4ba-b1bac2a034e9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["initial weight:  0.14463949277181343\n","Iteration 1: Weight: 0.19464, Loss: 0.72768, Prediction: 0.07232, Metric: 0.72768\n","Iteration 2: Weight: 0.24464, Loss: 0.70268, Prediction: 0.09732, Metric: 0.70268\n","Iteration 3: Weight: 0.29464, Loss: 0.67768, Prediction: 0.12232, Metric: 0.67768\n","Iteration 4: Weight: 0.34464, Loss: 0.65268, Prediction: 0.14732, Metric: 0.65268\n","Iteration 5: Weight: 0.39464, Loss: 0.62768, Prediction: 0.17232, Metric: 0.62768\n","Iteration 6: Weight: 0.44464, Loss: 0.60268, Prediction: 0.19732, Metric: 0.60268\n","Iteration 7: Weight: 0.49464, Loss: 0.57768, Prediction: 0.22232, Metric: 0.57768\n","Iteration 8: Weight: 0.54464, Loss: 0.55268, Prediction: 0.24732, Metric: 0.55268\n","Iteration 9: Weight: 0.59464, Loss: 0.52768, Prediction: 0.27232, Metric: 0.52768\n","Iteration 10: Weight: 0.64464, Loss: 0.50268, Prediction: 0.29732, Metric: 0.50268\n","Iteration 11: Weight: 0.69464, Loss: 0.47768, Prediction: 0.32232, Metric: 0.47768\n","Iteration 12: Weight: 0.74464, Loss: 0.45268, Prediction: 0.34732, Metric: 0.45268\n","Iteration 13: Weight: 0.79464, Loss: 0.42768, Prediction: 0.37232, Metric: 0.42768\n","Iteration 14: Weight: 0.84464, Loss: 0.40268, Prediction: 0.39732, Metric: 0.40268\n","Iteration 15: Weight: 0.89464, Loss: 0.37768, Prediction: 0.42232, Metric: 0.37768\n","Iteration 16: Weight: 0.94464, Loss: 0.35268, Prediction: 0.44732, Metric: 0.35268\n","Iteration 17: Weight: 0.99464, Loss: 0.32768, Prediction: 0.47232, Metric: 0.32768\n","Iteration 18: Weight: 1.04464, Loss: 0.30268, Prediction: 0.49732, Metric: 0.30268\n","Iteration 19: Weight: 1.09464, Loss: 0.27768, Prediction: 0.52232, Metric: 0.27768\n","Iteration 20: Weight: 1.14464, Loss: 0.25268, Prediction: 0.54732, Metric: 0.25268\n","Iteration 21: Weight: 1.19464, Loss: 0.22768, Prediction: 0.57232, Metric: 0.22768\n","Iteration 22: Weight: 1.24464, Loss: 0.20268, Prediction: 0.59732, Metric: 0.20268\n","Iteration 23: Weight: 1.29464, Loss: 0.17768, Prediction: 0.62232, Metric: 0.17768\n","Iteration 24: Weight: 1.34464, Loss: 0.15268, Prediction: 0.64732, Metric: 0.15268\n","Iteration 25: Weight: 1.39464, Loss: 0.12768, Prediction: 0.67232, Metric: 0.12768\n","Iteration 26: Weight: 1.44464, Loss: 0.10268, Prediction: 0.69732, Metric: 0.10268\n","Iteration 27: Weight: 1.49464, Loss: 0.07768, Prediction: 0.72232, Metric: 0.07768\n","Iteration 28: Weight: 1.54464, Loss: 0.05268, Prediction: 0.74732, Metric: 0.05268\n","Iteration 29: Weight: 1.59464, Loss: 0.02768, Prediction: 0.77232, Metric: 0.02768\n","Iteration 30: Weight: 1.64464, Loss: 0.00268, Prediction: 0.79732, Metric: 0.00268\n"]}]},{"cell_type":"markdown","source":["# 4. Use calories data"],"metadata":{"id":"TGm7UNTf14Zh"}},{"cell_type":"code","source":["import numpy as np\n","\n","# Mean Squared Error (MSE)\n","def mse(y, y_pred):\n","    return np.mean((y - y_pred) ** 2)\n","\n","def mse_derivative(x, y, y_pred):\n","    return -2 * np.mean(x * (y - y_pred), axis=0)\n","\n","def mse_bias_derivative(y, y_pred):\n","    return -2 * np.mean(y - y_pred)\n","\n","# Mean Absolute Error (MAE)\n","def mae(y, y_pred):\n","    return np.mean(np.abs(y - y_pred))\n","\n","def mae_derivative(x, y, y_pred):\n","    diff = y - y_pred\n","    return np.mean(-x * np.sign(diff), axis=0)\n","\n","def mae_bias_derivative(y, y_pred):\n","    diff = y - y_pred\n","    return np.mean(-np.sign(diff))\n","\n","# Root Mean Squared Error (RMSE)\n","def rmse(y, y_pred):\n","    return np.sqrt(np.mean((y - y_pred) ** 2))\n","\n","def rmse_derivative(x, y, y_pred):\n","    diff = y - y_pred\n","    return np.mean(-x * diff / np.abs(diff), axis=0) if np.any(diff != 0) else 0\n","\n","def rmse_bias_derivative(y, y_pred):\n","    diff = y - y_pred\n","    return np.mean(-diff / np.abs(diff)) if np.any(diff != 0) else 0\n","\n","# Loss and Metric functions\n","losses = {\n","    'mse': {\n","        'loss': mse,\n","        'derivative': mse_derivative,\n","        'bias_derivative': mse_bias_derivative\n","    },\n","    'mae': {\n","        'loss': mae,\n","        'derivative': mae_derivative,\n","        'bias_derivative': mae_bias_derivative\n","    },\n","    'rmse': {\n","        'loss': rmse,\n","        'derivative': rmse_derivative,\n","        'bias_derivative': rmse_bias_derivative\n","    }\n","}\n","\n","metrics = {\n","    'rmse': rmse,\n","    'mse': mse,\n","}\n","\n","# Select loss and metric\n","loss_name = 'rmse'\n","metric_name = 'rmse'\n","\n","loss = losses[loss_name]['loss']\n","loss_derivative = losses[loss_name]['derivative']\n","bias_derivative = losses[loss_name]['bias_derivative']\n","metric = metrics[metric_name]\n","\n","# Data\n","weights_feature = np.array([10, 24, 40, 52, 60, 77, 82, 100, 120, 130, 145, 155, 170, 180, 200]).reshape(-1, 1)\n","calories_target = np.array([148, 154, 167, 176, 176, 191, 203, 207, 227, 230, 245, 251, 259, 290, 503]).reshape(-1, 1)\n","weights_feature = (weights_feature - np.mean(weights_feature)) / np.std(weights_feature)  # Standardization\n","\n","\n","# Initialization\n","weight = np.random.rand(1)\n","bias = np.random.rand(1)\n","print('Initial weight:', weight)\n","print('Initial bias:', bias)\n","\n","alpha = 0.001  # Learning rate\n","\n","# Training loop\n","for iteration in range(100000):\n","    predicted = weights_feature * weight + bias\n","    error = loss(calories_target, predicted)\n","    derivative_weight = loss_derivative(weights_feature, calories_target, predicted)\n","    derivative_bias = bias_derivative(calories_target, predicted)\n","\n","    # max_gradient = 1.1  # Cap the gradient size\n","    # derivative_weight = np.clip(derivative_weight, -max_gradient, max_gradient)\n","    # derivative_bias = np.clip(derivative_bias, -max_gradient, max_gradient)\n","\n","    weight -= alpha * derivative_weight\n","    bias -= alpha * derivative_bias\n","\n","    metric_score = metric(calories_target, predicted)\n","\n","    if iteration % 1000==0:\n","      print(f\"Iteration {iteration + 1}: Weight: {weight[0]:.5f}, Bias: {bias[0]:.5f}, Loss: {error:.5f}, Metric: {metric_score:.5f}, Predicted: {predicted[0][0]:.5f}\")\n","\n","    if metric_score < 0.01:\n","        break\n","print(metric_score)\n","print(predicted)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RzxVyr0-EcUp","executionInfo":{"status":"ok","timestamp":1738088178245,"user_tz":-120,"elapsed":9245,"user":{"displayName":"Zaharie Sergiu","userId":"05122149963084415420"}},"outputId":"73e77ffa-03f3-4fc8-c495-6ad5fb59523c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Initial weight: [0.74520716]\n","Initial bias: [0.50932639]\n","Iteration 1: Weight: 0.74521, Bias: 0.51033, Loss: 242.57485, Metric: 242.57485, Predicted: -0.69410\n","Iteration 1001: Weight: 0.74521, Bias: 1.51033, Loss: 241.63535, Metric: 241.63535, Predicted: 0.30590\n","Iteration 2001: Weight: 0.74521, Bias: 2.51033, Loss: 240.69634, Metric: 240.69634, Predicted: 1.30590\n","Iteration 3001: Weight: 0.74521, Bias: 3.51033, Loss: 239.75782, Metric: 239.75782, Predicted: 2.30590\n","Iteration 4001: Weight: 0.74521, Bias: 4.51033, Loss: 238.81980, Metric: 238.81980, Predicted: 3.30590\n","Iteration 5001: Weight: 0.74521, Bias: 5.51033, Loss: 237.88229, Metric: 237.88229, Predicted: 4.30590\n","Iteration 6001: Weight: 0.74521, Bias: 6.51033, Loss: 236.94528, Metric: 236.94528, Predicted: 5.30590\n","Iteration 7001: Weight: 0.74521, Bias: 7.51033, Loss: 236.00880, Metric: 236.00880, Predicted: 6.30590\n","Iteration 8001: Weight: 0.74521, Bias: 8.51033, Loss: 235.07284, Metric: 235.07284, Predicted: 7.30590\n","Iteration 9001: Weight: 0.74521, Bias: 9.51033, Loss: 234.13740, Metric: 234.13740, Predicted: 8.30590\n","Iteration 10001: Weight: 0.74521, Bias: 10.51033, Loss: 233.20251, Metric: 233.20251, Predicted: 9.30590\n","Iteration 11001: Weight: 0.74521, Bias: 11.51033, Loss: 232.26815, Metric: 232.26815, Predicted: 10.30590\n","Iteration 12001: Weight: 0.74521, Bias: 12.51033, Loss: 231.33435, Metric: 231.33435, Predicted: 11.30590\n","Iteration 13001: Weight: 0.74521, Bias: 13.51033, Loss: 230.40110, Metric: 230.40110, Predicted: 12.30590\n","Iteration 14001: Weight: 0.74521, Bias: 14.51033, Loss: 229.46841, Metric: 229.46841, Predicted: 13.30590\n","Iteration 15001: Weight: 0.74521, Bias: 15.51033, Loss: 228.53629, Metric: 228.53629, Predicted: 14.30590\n","Iteration 16001: Weight: 0.74521, Bias: 16.51033, Loss: 227.60475, Metric: 227.60475, Predicted: 15.30590\n","Iteration 17001: Weight: 0.74521, Bias: 17.51033, Loss: 226.67379, Metric: 226.67379, Predicted: 16.30590\n","Iteration 18001: Weight: 0.74521, Bias: 18.51033, Loss: 225.74342, Metric: 225.74342, Predicted: 17.30590\n","Iteration 19001: Weight: 0.74521, Bias: 19.51033, Loss: 224.81365, Metric: 224.81365, Predicted: 18.30590\n","Iteration 20001: Weight: 0.74521, Bias: 20.51033, Loss: 223.88448, Metric: 223.88448, Predicted: 19.30590\n","Iteration 21001: Weight: 0.74521, Bias: 21.51033, Loss: 222.95593, Metric: 222.95593, Predicted: 20.30590\n","Iteration 22001: Weight: 0.74521, Bias: 22.51033, Loss: 222.02800, Metric: 222.02800, Predicted: 21.30590\n","Iteration 23001: Weight: 0.74521, Bias: 23.51033, Loss: 221.10070, Metric: 221.10070, Predicted: 22.30590\n","Iteration 24001: Weight: 0.74521, Bias: 24.51033, Loss: 220.17403, Metric: 220.17403, Predicted: 23.30590\n","Iteration 25001: Weight: 0.74521, Bias: 25.51033, Loss: 219.24801, Metric: 219.24801, Predicted: 24.30590\n","Iteration 26001: Weight: 0.74521, Bias: 26.51033, Loss: 218.32264, Metric: 218.32264, Predicted: 25.30590\n","Iteration 27001: Weight: 0.74521, Bias: 27.51033, Loss: 217.39793, Metric: 217.39793, Predicted: 26.30590\n","Iteration 28001: Weight: 0.74521, Bias: 28.51033, Loss: 216.47389, Metric: 216.47389, Predicted: 27.30590\n","Iteration 29001: Weight: 0.74521, Bias: 29.51033, Loss: 215.55053, Metric: 215.55053, Predicted: 28.30590\n","Iteration 30001: Weight: 0.74521, Bias: 30.51033, Loss: 214.62785, Metric: 214.62785, Predicted: 29.30590\n","Iteration 31001: Weight: 0.74521, Bias: 31.51033, Loss: 213.70587, Metric: 213.70587, Predicted: 30.30590\n","Iteration 32001: Weight: 0.74521, Bias: 32.51033, Loss: 212.78460, Metric: 212.78460, Predicted: 31.30590\n","Iteration 33001: Weight: 0.74521, Bias: 33.51033, Loss: 211.86404, Metric: 211.86404, Predicted: 32.30590\n","Iteration 34001: Weight: 0.74521, Bias: 34.51033, Loss: 210.94420, Metric: 210.94420, Predicted: 33.30590\n","Iteration 35001: Weight: 0.74521, Bias: 35.51033, Loss: 210.02510, Metric: 210.02510, Predicted: 34.30590\n","Iteration 36001: Weight: 0.74521, Bias: 36.51033, Loss: 209.10674, Metric: 209.10674, Predicted: 35.30590\n","Iteration 37001: Weight: 0.74521, Bias: 37.51033, Loss: 208.18913, Metric: 208.18913, Predicted: 36.30590\n","Iteration 38001: Weight: 0.74521, Bias: 38.51033, Loss: 207.27228, Metric: 207.27228, Predicted: 37.30590\n","Iteration 39001: Weight: 0.74521, Bias: 39.51033, Loss: 206.35621, Metric: 206.35621, Predicted: 38.30590\n","Iteration 40001: Weight: 0.74521, Bias: 40.51033, Loss: 205.44091, Metric: 205.44091, Predicted: 39.30590\n","Iteration 41001: Weight: 0.74521, Bias: 41.51033, Loss: 204.52641, Metric: 204.52641, Predicted: 40.30590\n","Iteration 42001: Weight: 0.74521, Bias: 42.51033, Loss: 203.61272, Metric: 203.61272, Predicted: 41.30590\n","Iteration 43001: Weight: 0.74521, Bias: 43.51033, Loss: 202.69984, Metric: 202.69984, Predicted: 42.30590\n","Iteration 44001: Weight: 0.74521, Bias: 44.51033, Loss: 201.78778, Metric: 201.78778, Predicted: 43.30590\n","Iteration 45001: Weight: 0.74521, Bias: 45.51033, Loss: 200.87657, Metric: 200.87657, Predicted: 44.30590\n","Iteration 46001: Weight: 0.74521, Bias: 46.51033, Loss: 199.96620, Metric: 199.96620, Predicted: 45.30590\n","Iteration 47001: Weight: 0.74521, Bias: 47.51033, Loss: 199.05669, Metric: 199.05669, Predicted: 46.30590\n","Iteration 48001: Weight: 0.74521, Bias: 48.51033, Loss: 198.14805, Metric: 198.14805, Predicted: 47.30590\n","Iteration 49001: Weight: 0.74521, Bias: 49.51033, Loss: 197.24030, Metric: 197.24030, Predicted: 48.30590\n","Iteration 50001: Weight: 0.74521, Bias: 50.51033, Loss: 196.33345, Metric: 196.33345, Predicted: 49.30590\n","Iteration 51001: Weight: 0.74521, Bias: 51.51033, Loss: 195.42750, Metric: 195.42750, Predicted: 50.30590\n","Iteration 52001: Weight: 0.74521, Bias: 52.51033, Loss: 194.52247, Metric: 194.52247, Predicted: 51.30590\n","Iteration 53001: Weight: 0.74521, Bias: 53.51033, Loss: 193.61838, Metric: 193.61838, Predicted: 52.30590\n","Iteration 54001: Weight: 0.74521, Bias: 54.51033, Loss: 192.71524, Metric: 192.71524, Predicted: 53.30590\n","Iteration 55001: Weight: 0.74521, Bias: 55.51033, Loss: 191.81306, Metric: 191.81306, Predicted: 54.30590\n","Iteration 56001: Weight: 0.74521, Bias: 56.51033, Loss: 190.91185, Metric: 190.91185, Predicted: 55.30590\n","Iteration 57001: Weight: 0.74521, Bias: 57.51033, Loss: 190.01163, Metric: 190.01163, Predicted: 56.30590\n","Iteration 58001: Weight: 0.74521, Bias: 58.51033, Loss: 189.11241, Metric: 189.11241, Predicted: 57.30590\n","Iteration 59001: Weight: 0.74521, Bias: 59.51033, Loss: 188.21421, Metric: 188.21421, Predicted: 58.30590\n","Iteration 60001: Weight: 0.74521, Bias: 60.51033, Loss: 187.31704, Metric: 187.31704, Predicted: 59.30590\n","Iteration 61001: Weight: 0.74521, Bias: 61.51033, Loss: 186.42092, Metric: 186.42092, Predicted: 60.30590\n","Iteration 62001: Weight: 0.74521, Bias: 62.51033, Loss: 185.52586, Metric: 185.52586, Predicted: 61.30590\n","Iteration 63001: Weight: 0.74521, Bias: 63.51033, Loss: 184.63188, Metric: 184.63188, Predicted: 62.30590\n","Iteration 64001: Weight: 0.74521, Bias: 64.51033, Loss: 183.73899, Metric: 183.73899, Predicted: 63.30590\n","Iteration 65001: Weight: 0.74521, Bias: 65.51033, Loss: 182.84721, Metric: 182.84721, Predicted: 64.30590\n","Iteration 66001: Weight: 0.74521, Bias: 66.51033, Loss: 181.95655, Metric: 181.95655, Predicted: 65.30590\n","Iteration 67001: Weight: 0.74521, Bias: 67.51033, Loss: 181.06704, Metric: 181.06704, Predicted: 66.30590\n","Iteration 68001: Weight: 0.74521, Bias: 68.51033, Loss: 180.17868, Metric: 180.17868, Predicted: 67.30590\n","Iteration 69001: Weight: 0.74521, Bias: 69.51033, Loss: 179.29150, Metric: 179.29150, Predicted: 68.30590\n","Iteration 70001: Weight: 0.74521, Bias: 70.51033, Loss: 178.40552, Metric: 178.40552, Predicted: 69.30590\n","Iteration 71001: Weight: 0.74521, Bias: 71.51033, Loss: 177.52074, Metric: 177.52074, Predicted: 70.30590\n","Iteration 72001: Weight: 0.74521, Bias: 72.51033, Loss: 176.63720, Metric: 176.63720, Predicted: 71.30590\n","Iteration 73001: Weight: 0.74521, Bias: 73.51033, Loss: 175.75490, Metric: 175.75490, Predicted: 72.30590\n","Iteration 74001: Weight: 0.74521, Bias: 74.51033, Loss: 174.87387, Metric: 174.87387, Predicted: 73.30590\n","Iteration 75001: Weight: 0.74521, Bias: 75.51033, Loss: 173.99412, Metric: 173.99412, Predicted: 74.30590\n","Iteration 76001: Weight: 0.74521, Bias: 76.51033, Loss: 173.11568, Metric: 173.11568, Predicted: 75.30590\n","Iteration 77001: Weight: 0.74521, Bias: 77.51033, Loss: 172.23857, Metric: 172.23857, Predicted: 76.30590\n","Iteration 78001: Weight: 0.74521, Bias: 78.51033, Loss: 171.36281, Metric: 171.36281, Predicted: 77.30590\n","Iteration 79001: Weight: 0.74521, Bias: 79.51033, Loss: 170.48841, Metric: 170.48841, Predicted: 78.30590\n","Iteration 80001: Weight: 0.74521, Bias: 80.51033, Loss: 169.61539, Metric: 169.61539, Predicted: 79.30590\n","Iteration 81001: Weight: 0.74521, Bias: 81.51033, Loss: 168.74379, Metric: 168.74379, Predicted: 80.30590\n","Iteration 82001: Weight: 0.74521, Bias: 82.51033, Loss: 167.87362, Metric: 167.87362, Predicted: 81.30590\n","Iteration 83001: Weight: 0.74521, Bias: 83.51033, Loss: 167.00490, Metric: 167.00490, Predicted: 82.30590\n","Iteration 84001: Weight: 0.74521, Bias: 84.51033, Loss: 166.13766, Metric: 166.13766, Predicted: 83.30590\n","Iteration 85001: Weight: 0.74521, Bias: 85.51033, Loss: 165.27192, Metric: 165.27192, Predicted: 84.30590\n","Iteration 86001: Weight: 0.74521, Bias: 86.51033, Loss: 164.40771, Metric: 164.40771, Predicted: 85.30590\n","Iteration 87001: Weight: 0.74521, Bias: 87.51033, Loss: 163.54504, Metric: 163.54504, Predicted: 86.30590\n","Iteration 88001: Weight: 0.74521, Bias: 88.51033, Loss: 162.68394, Metric: 162.68394, Predicted: 87.30590\n","Iteration 89001: Weight: 0.74521, Bias: 89.51033, Loss: 161.82444, Metric: 161.82444, Predicted: 88.30590\n","Iteration 90001: Weight: 0.74521, Bias: 90.51033, Loss: 160.96656, Metric: 160.96656, Predicted: 89.30590\n","Iteration 91001: Weight: 0.74521, Bias: 91.51033, Loss: 160.11034, Metric: 160.11034, Predicted: 90.30590\n","Iteration 92001: Weight: 0.74521, Bias: 92.51033, Loss: 159.25579, Metric: 159.25579, Predicted: 91.30590\n","Iteration 93001: Weight: 0.74521, Bias: 93.51033, Loss: 158.40294, Metric: 158.40294, Predicted: 92.30590\n","Iteration 94001: Weight: 0.74521, Bias: 94.51033, Loss: 157.55182, Metric: 157.55182, Predicted: 93.30590\n","Iteration 95001: Weight: 0.74521, Bias: 95.51033, Loss: 156.70246, Metric: 156.70246, Predicted: 94.30590\n","Iteration 96001: Weight: 0.74521, Bias: 96.51033, Loss: 155.85489, Metric: 155.85489, Predicted: 95.30590\n","Iteration 97001: Weight: 0.74521, Bias: 97.51033, Loss: 155.00914, Metric: 155.00914, Predicted: 96.30590\n","Iteration 98001: Weight: 0.74521, Bias: 98.51033, Loss: 154.16523, Metric: 154.16523, Predicted: 97.30590\n","Iteration 99001: Weight: 0.74521, Bias: 99.51033, Loss: 153.32320, Metric: 153.32320, Predicted: 98.30590\n","152.4839136598251\n","[[ 99.30490484]\n"," [ 99.48606507]\n"," [ 99.69310534]\n"," [ 99.84838554]\n"," [ 99.95190567]\n"," [100.17188595]\n"," [100.23658604]\n"," [100.46950634]\n"," [100.72830667]\n"," [100.85770684]\n"," [101.05180709]\n"," [101.18120725]\n"," [101.3753075 ]\n"," [101.50470767]\n"," [101.763508  ]]\n"]}]},{"cell_type":"markdown","source":["original bias and intercept:  (106.21633973873615, 1.1869684212248448)\n","\n","scaled bias and intercept: (228.47408712489516, 68.35674087849118)"],"metadata":{"id":"MK_fxOujHkbD"}}]}