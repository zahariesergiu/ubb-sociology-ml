{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52c3a043",
   "metadata": {},
   "source": [
    "# Final Project Task 3 - Census Modelling Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c75e29",
   "metadata": {},
   "source": [
    "#### Student: Iacob Mihai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b133d84c",
   "metadata": {},
   "source": [
    "Requirements\n",
    "- Create a regression model on the Census dataset, with 'hours-per-week' target\n",
    "\n",
    "- You can use models (estmators) from sklearn, but feel free to use any library for traditional ML. \n",
    "    - Note: in sklearn, the LinearRegression estimator is based on OLS, a statistical method. Please use the SGDRegressor estimator, since this is based on gradient descent. \n",
    "    - You can use LinearRegression estimator, but only as comparison with the SGDRegressor - Optional.\n",
    "\n",
    "- Model Selection and Setup **2p**:\n",
    "    - Implement multiple models, to solve a regression problem using traditional ML: \n",
    "        - Linear Regression\n",
    "        - Decision Tree Regression\n",
    "        - Random Forest Regression - Optional\n",
    "        - Ridge Regression - Optional\n",
    "        - Lasso Regression - Optional\n",
    "    - Choose a loss (or experiment with different losses) for the model and justify the choice. *1p*\n",
    "        - MSE, MAE, RMSE, Huber Loss or others\n",
    "    - Justify model choices based on dataset characteristics and task requirements; specify model pros and cons. *1p*\n",
    "\n",
    "\n",
    "- Data Preparation\n",
    "    - Use the preprocessed datasets from Task 1.\n",
    "    - From the train set, create an extra validation set, if necesarry. So in total there will be: train, validation and test datasets.\n",
    "    - Be sure all models have their data preprocessed as needed. Some models require different, or no encoding for some features.\n",
    "\n",
    "\n",
    "- Model Training and Experimentation **10p**\n",
    "    - Establish a Baseline Model *2p*\n",
    "        - For each model type, train a simple model with default settings as a baseline.\n",
    "        - Evaluate its performance to establish a benchmark for comparison.\n",
    "    - Make plots with train, validation loss and metric on epochs (or on steps), if applicable. - Optional\n",
    "    - Feature Selection: - Optional\n",
    "        - Use insights from EDA in Task 2 to identify candidate features by analyzing patterns, relationships, and distributions.\n",
    "    - Experimentation: *8p*\n",
    "        - For each baseline model type, iteratively experiment with different combinations of features and transformations.\n",
    "        - Experiment with feature engineering techniques such as interaction terms, polynomial features, or scaling transformations.\n",
    "        - Identify the best model which have the best performance metrics on test set.\n",
    "        - You may need multiple preprocessed datasets preprocessed\n",
    "- Hyperparameter Tuning - Optional\n",
    "  - Perform hyperparameter tuning only on the best-performing model after evaluating all model types and experiments. \n",
    "  - Consider using techniques like Grid Search for exhaustive tuning, Random Search for quicker exploration, or Bayesian Optimization for an intelligent, efficient search of hyperparameters.\n",
    "  - Avoid tuning models that do not show strong baseline performance or are unlikely to outperform others based on experimentation.\n",
    "  - Ensure that hyperparameter tuning is done after completing feature selection, baseline modeling, and experimentation, ensuring that the model is stable and representative of the dataset.\n",
    "\n",
    "\n",
    "- Model Evaluation **3p**\n",
    "    - Evaluate models on the test dataset using regression metrics: *1p*\n",
    "        - Mean Absolute Error (MAE)\n",
    "        - Mean Squared Error (MSE)\n",
    "        - Root Mean Squared Error (RMSE)\n",
    "        - R² Score\n",
    "    - Choose one metric for model comparison and explain your choice *1p*\n",
    "    - Compare the results across different models. Save all experiment results  into a table. *1p*\n",
    "\n",
    "Feature Importance - Optional\n",
    "- For applicable models (e.g., Decision Tree Regression), analyze feature importance and discuss its relevance to the problem.\n",
    "\n",
    "\n",
    "\n",
    "Deliverables\n",
    "\n",
    "- Notebook code with no errors.\n",
    "- Code and results from experiments. Create a table with all experiments results, include experiment name, metrics results.\n",
    "- Explain findings, choices, results.\n",
    "- Potential areas for improvement or further exploration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a36718",
   "metadata": {},
   "source": [
    "##### 1. Sanity checks and train validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68d95f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18695eba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes:\n",
      "X_train: (26029, 116)  y_train: (26029, 1)\n",
      "X_test : (6508, 116)  y_test : (6508, 1)\n"
     ]
    }
   ],
   "source": [
    "# Loading the preprocessed datasets\n",
    "\n",
    "X_train = pd.read_csv(\"D:/Facultate/ADC/Machine Learning/X_train.csv\")\n",
    "X_test  = pd.read_csv(\"D:/Facultate/ADC/Machine Learning/X_test.csv\")\n",
    "y_train = pd.read_csv(\"D:/Facultate/ADC/Machine Learning/y_train.csv\")\n",
    "y_test  = pd.read_csv(\"D:/Facultate/ADC/Machine Learning/y_test.csv\")\n",
    "\n",
    "TARGET = \"hours-per-week\"\n",
    "\n",
    "print(\"Shapes:\")\n",
    "print(\"X_train:\", X_train.shape, \" y_train:\", y_train.shape)\n",
    "print(\"X_test :\", X_test.shape,  \" y_test :\", y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95439c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature columns match.\n"
     ]
    }
   ],
   "source": [
    "# Checking for column consistency\n",
    "\n",
    "assert list(X_train.columns) == list(X_test.columns), \"X_train and X_test have different columns!\"\n",
    "print(\"Feature columns match.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3bcfd49e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Target quick stats (train):\n",
      "count    26029.000000\n",
      "mean        40.393868\n",
      "std         12.341250\n",
      "min          1.000000\n",
      "25%         40.000000\n",
      "50%         40.000000\n",
      "75%         45.000000\n",
      "max         99.000000\n",
      "Name: hours-per-week, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Checking for target sanity\n",
    "\n",
    "assert TARGET in y_train.columns and TARGET in y_test.columns, \"Target column missing in y files\"\n",
    "y_train[TARGET] = pd.to_numeric(y_train[TARGET], errors=\"coerce\")\n",
    "y_test[TARGET]  = pd.to_numeric(y_test[TARGET], errors=\"coerce\")\n",
    "\n",
    "print(\"\\nTarget quick stats (train):\")\n",
    "print(y_train[TARGET].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76b4039f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values:\n",
      "X_train: 0 | y_train: 0\n",
      "X_test : 0 | y_test : 0\n"
     ]
    }
   ],
   "source": [
    "# Missing value check\n",
    "\n",
    "missing_X_train = X_train.isna().sum().sum()\n",
    "missing_y_train = y_train.isna().sum().sum()\n",
    "missing_X_test  = X_test.isna().sum().sum()\n",
    "missing_y_test  = y_test.isna().sum().sum()\n",
    "\n",
    "print(\"\\nMissing values:\")\n",
    "print(\"X_train:\", missing_X_train, \"| y_train:\", missing_y_train)\n",
    "print(\"X_test :\", missing_X_test,  \"| y_test :\", missing_y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c14b5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Split sizes:\n",
      "Train: (20823, 116) Val: (5206, 116) Test: (6508, 116)\n"
     ]
    }
   ],
   "source": [
    "# Splitting the validation set from the training set\n",
    "\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    X_train, y_train[TARGET],\n",
    "    test_size=0.20,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"\\nSplit sizes:\")\n",
    "print(\"Train:\", X_tr.shape, \"Val:\", X_val.shape, \"Test:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be396c5",
   "metadata": {},
   "source": [
    "##### 2. Baseline Regression Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f410c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining metrics for model performance\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "\n",
    "def regression_metrics(y_true, y_pred):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return mae, mse, rmse, r2\n",
    "\n",
    "results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c8a78aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SDGRegressor\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sgd_baseline = Pipeline( [\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"model\", SGDRegressor(random_state=42))\n",
    "])\n",
    "\n",
    "# train\n",
    "sgd_baseline.fit(X_tr, y_tr)\n",
    "\n",
    "# predictions\n",
    "pred_tr = sgd_baseline.predict(X_tr)\n",
    "pred_val = sgd_baseline.predict(X_val)\n",
    "pred_te = sgd_baseline.predict(X_test)\n",
    "\n",
    "# metrics\n",
    "mae_tr, mse_tr, rmse_tr, r2_tr = regression_metrics(y_tr, pred_tr)\n",
    "mae_v, mse_v, rmse_v, r2_v = regression_metrics(y_val, pred_val)\n",
    "mae_te, mse_te, rmse_te, r2_te = regression_metrics(y_test[TARGET], pred_te)\n",
    "\n",
    "results.append({\n",
    "    \"model\": \"SGDRegressor_baseline\",\n",
    "    \"train_MAE\": mae_tr, \"val_MAE\": mae_v, \"test_MAE\": mae_te,\n",
    "    \"train_RMSE\": rmse_tr, \"val_RMSE\": rmse_v, \"test_RMSE\": rmse_te,\n",
    "    \"train_R2\": r2_tr, \"val_R2\": r2_v, \"test_R2\": r2_te\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a26fb98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression with OLS\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_baseline = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"model\", LinearRegression())\n",
    "])\n",
    "\n",
    "lin_baseline.fit(X_tr, y_tr)\n",
    "\n",
    "pred_tr = lin_baseline.predict(X_tr)\n",
    "pred_val = lin_baseline.predict(X_val)\n",
    "pred_te = lin_baseline.predict(X_test)\n",
    "\n",
    "mae_tr, mse_tr, rmse_tr, r2_tr = regression_metrics(y_tr, pred_tr)\n",
    "mae_v, mse_v, rmse_v, r2_v = regression_metrics(y_val, pred_val)\n",
    "mae_te, mse_te, rmse_te, r2_te = regression_metrics(y_test[TARGET], pred_te)\n",
    "\n",
    "results.append({\n",
    "    \"model\": \"LinearRegression_baseline\",\n",
    "    \"train_MAE\": mae_tr, \"val_MAE\": mae_v, \"test_MAE\": mae_te,\n",
    "    \"train_RMSE\": rmse_tr, \"val_RMSE\": rmse_v, \"test_RMSE\": rmse_te,\n",
    "    \"train_R2\": r2_tr, \"val_R2\": r2_v, \"test_R2\": r2_te\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "379cc16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Trees\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "dt_baseline = DecisionTreeRegressor(random_state=42)\n",
    "\n",
    "dt_baseline.fit(X_tr, y_tr)\n",
    "\n",
    "pred_tr = dt_baseline.predict(X_tr)\n",
    "pred_val = dt_baseline.predict(X_val)\n",
    "pred_te = dt_baseline.predict(X_test)\n",
    "\n",
    "mae_tr, mse_tr, rmse_tr, r2_tr = regression_metrics(y_tr, pred_tr)\n",
    "mae_v, mse_v, rmse_v, r2_v = regression_metrics(y_val, pred_val)\n",
    "mae_te, mse_te, rmse_te, r2_te = regression_metrics(y_test[TARGET], pred_te)\n",
    "\n",
    "results.append({\n",
    "    \"model\": \"DecisionTree_baseline\",\n",
    "    \"train_MAE\": mae_tr, \"val_MAE\": mae_v, \"test_MAE\": mae_te,\n",
    "    \"train_RMSE\": rmse_tr, \"val_RMSE\": rmse_v, \"test_RMSE\": rmse_te,\n",
    "    \"train_R2\": r2_tr, \"val_R2\": r2_v, \"test_R2\": r2_te\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "63f4d39a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>train_MAE</th>\n",
       "      <th>val_MAE</th>\n",
       "      <th>test_MAE</th>\n",
       "      <th>train_RMSE</th>\n",
       "      <th>val_RMSE</th>\n",
       "      <th>test_RMSE</th>\n",
       "      <th>train_R2</th>\n",
       "      <th>val_R2</th>\n",
       "      <th>test_R2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SGDRegressor_baseline</td>\n",
       "      <td>742113.498240</td>\n",
       "      <td>1.023573e+06</td>\n",
       "      <td>916955.894320</td>\n",
       "      <td>3.969109e+07</td>\n",
       "      <td>4.976201e+07</td>\n",
       "      <td>4.480700e+07</td>\n",
       "      <td>-1.030760e+13</td>\n",
       "      <td>-1.649148e+13</td>\n",
       "      <td>-1.312550e+13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LinearRegression_baseline</td>\n",
       "      <td>7.594150</td>\n",
       "      <td>7.611115e+00</td>\n",
       "      <td>7.641111</td>\n",
       "      <td>1.083042e+01</td>\n",
       "      <td>1.083733e+01</td>\n",
       "      <td>1.088800e+01</td>\n",
       "      <td>2.325294e-01</td>\n",
       "      <td>2.178169e-01</td>\n",
       "      <td>2.249675e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DecisionTree_baseline</td>\n",
       "      <td>0.028974</td>\n",
       "      <td>1.003169e+01</td>\n",
       "      <td>10.316201</td>\n",
       "      <td>6.254487e-01</td>\n",
       "      <td>1.485355e+01</td>\n",
       "      <td>1.527505e+01</td>\n",
       "      <td>9.974405e-01</td>\n",
       "      <td>-4.693464e-01</td>\n",
       "      <td>-5.254201e-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       model      train_MAE       val_MAE       test_MAE  \\\n",
       "0      SGDRegressor_baseline  742113.498240  1.023573e+06  916955.894320   \n",
       "1  LinearRegression_baseline       7.594150  7.611115e+00       7.641111   \n",
       "2      DecisionTree_baseline       0.028974  1.003169e+01      10.316201   \n",
       "\n",
       "     train_RMSE      val_RMSE     test_RMSE      train_R2        val_R2  \\\n",
       "0  3.969109e+07  4.976201e+07  4.480700e+07 -1.030760e+13 -1.649148e+13   \n",
       "1  1.083042e+01  1.083733e+01  1.088800e+01  2.325294e-01  2.178169e-01   \n",
       "2  6.254487e-01  1.485355e+01  1.527505e+01  9.974405e-01 -4.693464e-01   \n",
       "\n",
       "        test_R2  \n",
       "0 -1.312550e+13  \n",
       "1  2.249675e-01  \n",
       "2 -5.254201e-01  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Baseline models results\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a99af5e",
   "metadata": {},
   "source": [
    "Linear Regression oferă performanțe stabile pe toate seturile de date, cu erori similare pe train, validation și test, indicând o relație liniară moderată între variabilele explicative și hours-per-week. \n",
    "\n",
    "DecisionTreeRegressor obține performanță foarte bună pe setul de antrenare, dar generalizează slab, cu erori ridicate și valori R² negative pe validation și test, semnalând overfitting semnificativ. \n",
    "\n",
    "În cazul SGDRegressor, configurația default produce rezultate instabile, cu erori foarte mari și R² extrem și negativ, ceea ce indică divergența optimizării. Astfel, devine necesară ajustarea funcției de pierdere și a hiperparametrilor.\n",
    "\n",
    "Am evaluat modelele folosind MAE, MSE, RMSE și R², pentru a surprinde atât erorile medii absolute, cât și penalizarea erorilor mari și capacitatea explicativă a modelelor. Pentru comparația modelelor am folosit MAE, deoarece este mai robustă la valori extreme ale variabilei hours-per-week, care prezintă outlieri și heaping. Spre deosebire de MSE sau RMSE, MAE penalizează liniar erorile. Folosesc Linear Regression ca benchmark , deoarece oferă performanțe stabile pe toate seturile de date și surprinde relațiile liniare de bază dintre variabilele explicative și hours-per-week. Toate modelele ulterioare sunt evaluate prin comparație cu acest benchmark."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa8ac46",
   "metadata": {},
   "source": [
    "##### 3. Experimentation and Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a6219bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimentation setup\n",
    "\n",
    "def metrics(y_true, y_pred):\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    return mae, mse, rmse, r2\n",
    "\n",
    "\n",
    "exp_results = []\n",
    "\n",
    "def run_and_log(name, model, Xtr, Xval, Xte, ytr, yval, yte):\n",
    "    model.fit(Xtr, ytr)\n",
    "\n",
    "    pred_tr  = model.predict(Xtr)\n",
    "    pred_val = model.predict(Xval)\n",
    "    pred_te  = model.predict(Xte)\n",
    "\n",
    "    mae_tr, mse_tr, rmse_tr, r2_tr = metrics(ytr, pred_tr)\n",
    "    mae_v,  mse_v,  rmse_v,  r2_v  = metrics(yval, pred_val)\n",
    "    mae_te, mse_te, rmse_te, r2_te = metrics(yte, pred_te)\n",
    "\n",
    "    exp_results.append({\n",
    "        \"experiment\": name,\n",
    "        \"train_MAE\": mae_tr, \"val_MAE\": mae_v, \"test_MAE\": mae_te,\n",
    "        \"train_MSE\": mse_tr, \"val_MSE\": mse_v, \"test_MSE\": mse_te,\n",
    "        \"train_RMSE\": rmse_tr, \"val_RMSE\": rmse_v, \"test_RMSE\": rmse_te,\n",
    "        \"train_R2\": r2_tr, \"val_R2\": r2_v, \"test_R2\": r2_te\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dfdd6af5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yte min/max: 1.0 99.0\n"
     ]
    }
   ],
   "source": [
    "TARGET = \"hours-per-week\"\n",
    "yte = y_test[TARGET]  \n",
    "print(\"yte min/max:\", float(yte.min()), float(yte.max()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f92a0fa",
   "metadata": {},
   "source": [
    "Am implementat un cadru standard de evaluare care calculează MAE, MSE, RMSE și R² pe seturile de train, validation și test și care stochează rezultatele tuturor experimentelor într-un tabel comparabil."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9063e643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALL features: 116\n",
      "EDA subset features: 68\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((20823, 68), (5206, 68), (6508, 68))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feature sets (all + EDA)\n",
    "\n",
    "# All\n",
    "Xtr_all, Xval_all, Xte_all = X_tr.copy(), X_val.copy(), X_test.copy()\n",
    "\n",
    "# EDA-based subset\n",
    "eda_cols = [c for c in X_tr.columns if (\n",
    "    (\"education\" in c) or\n",
    "    (\"occupation\" in c) or\n",
    "    (\"workclass\" in c) or\n",
    "    (\"marital\" in c) or\n",
    "    (\"relationship\" in c) or\n",
    "    (\"race\" in c) or\n",
    "    (\"sex\" in c) or\n",
    "    (c == \"age\") or\n",
    "    (\"capital-gain\" in c) or\n",
    "    (\"capital-loss\" in c)\n",
    ")]\n",
    "\n",
    "print(\"ALL features:\", X_tr.shape[1])\n",
    "print(\"EDA subset features:\", len(eda_cols))\n",
    "\n",
    "# subset dataframes\n",
    "Xtr_eda = X_tr[eda_cols].copy()\n",
    "Xval_eda = X_val[eda_cols].copy()\n",
    "Xte_eda  = X_test[eda_cols].copy()\n",
    "\n",
    "# target for test\n",
    "yte = y_test[TARGET]\n",
    "\n",
    "Xtr_eda.shape, Xval_eda.shape, Xte_eda.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdccd8c2",
   "metadata": {},
   "source": [
    "Am definit două seturi de feature-uri pentru experimentare: ALL features (toate variabilele preprocesate) și un subset EDA-informed (educație, ocupație, workclass, variabile socio-demografice și capital gain/loss). Scopul este să evaluăm dacă reducerea dimensionalității și concentrarea pe variabile relevante îmbunătățește generalizarea pe setul de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc8a00b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20823, 118), (20823, 116))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using Log for highly skewed variables\n",
    "\n",
    "def add_log1p_capitals(X):\n",
    "    X2 = X.copy()\n",
    "    for col in [\"capital-gain\", \"capital-loss\"]:\n",
    "        if col in X2.columns:\n",
    "            X2[f\"log1p_{col}\"] = np.log1p(\n",
    "                pd.to_numeric(X2[col], errors=\"coerce\").fillna(0)\n",
    "            )\n",
    "    return X2\n",
    "\n",
    "# log-transformed datasets\n",
    "\n",
    "Xtr_log = add_log1p_capitals(Xtr_all)\n",
    "Xval_log = add_log1p_capitals(Xval_all)\n",
    "Xte_log  = add_log1p_capitals(Xte_all)\n",
    "\n",
    "Xtr_log.shape, Xtr_all.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc74463",
   "metadata": {},
   "source": [
    "Am folosit transformări logaritmice pentru variabile extrem de skewed cu scopul de a reduce influența valorilor extreme și de a îmbunătăți stabilitatea modelelor liniare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6bbe9ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polynomial base variables: ['age', 'capital-gain', 'capital-loss']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((20823, 122), (20823, 116))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Polynomial features\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly_base = [c for c in [\"age\", \"capital-gain\", \"capital-loss\"] if c in Xtr_all.columns]\n",
    "print(\"Polynomial base variables:\", poly_base)\n",
    "\n",
    "def make_poly_dataset(X, base_cols, degree=2):\n",
    "    if len(base_cols) == 0:\n",
    "        return X.copy()\n",
    "\n",
    "    rest_cols = [c for c in X.columns if c not in base_cols]\n",
    "    X_base = X[base_cols]\n",
    "\n",
    "    poly = PolynomialFeatures(degree=degree, include_bias=False)\n",
    "    X_poly = poly.fit_transform(X_base)\n",
    "\n",
    "    poly_names = poly.get_feature_names_out(base_cols)\n",
    "    X_poly_df = pd.DataFrame(X_poly, columns=poly_names, index=X.index)\n",
    "\n",
    "    return pd.concat([X_poly_df, X[rest_cols]], axis=1)\n",
    "\n",
    "# Polynomes and interactions datasets \n",
    "\n",
    "Xtr_poly = make_poly_dataset(Xtr_all, poly_base, degree=2)\n",
    "Xval_poly = make_poly_dataset(Xval_all, poly_base, degree=2)\n",
    "Xte_poly  = make_poly_dataset(Xte_all,  poly_base, degree=2)\n",
    "\n",
    "Xtr_poly.shape, Xtr_all.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38871bb",
   "metadata": {},
   "source": [
    "Am testat features polinomiale și interacțiuni de ordinul doi pentru un subset restrâns de variabile numerice (age, capital-gain, capital-loss), pentru a surprinde posibile relații neliniare. Transformarea a fost aplicată controlat, pentru a evita creșterea excesivă a dimensionalității."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c85cd0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression (benchmark, all + EDA)\n",
    "\n",
    "# ALL features\n",
    "\n",
    "lin_all = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"model\", LinearRegression())\n",
    "])\n",
    "run_and_log(\"LinearRegression_scaled_ALL\",\n",
    "            lin_all, Xtr_all, Xval_all, Xte_all, y_tr, y_val, yte)\n",
    "\n",
    "# EDA subset\n",
    "\n",
    "lin_eda = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"model\", LinearRegression())\n",
    "])\n",
    "run_and_log(\"LinearRegression_scaled_EDA\",\n",
    "            lin_eda, Xtr_eda, Xval_eda, Xte_eda, y_tr, y_val, yte)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b2d6c1",
   "metadata": {},
   "source": [
    "Folosesc regresia liniară ca benchmark. Antrenarea pe un subset EDA-informed permite evaluarea impactului reducerii dimensionalității asupra performanței."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "072367f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge (L2) Stable for Colinearity\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "ridge_a1 = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"model\", Ridge(alpha=1.0))\n",
    "])\n",
    "run_and_log(\"Ridge_alpha1_ALL\",\n",
    "            ridge_a1, Xtr_all, Xval_all, Xte_all, y_tr, y_val, yte)\n",
    "\n",
    "ridge_a10 = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"model\", Ridge(alpha=10.0))\n",
    "])\n",
    "run_and_log(\"Ridge_alpha10_ALL\",\n",
    "            ridge_a10, Xtr_all, Xval_all, Xte_all, y_tr, y_val, yte)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d23dbe87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso (L1) Implicit Feature Selection\n",
    "\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso_1 = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"model\", Lasso(alpha=0.001, max_iter=5000))\n",
    "])\n",
    "run_and_log(\"Lasso_alpha0.001_ALL\",\n",
    "            lasso_1, Xtr_all, Xval_all, Xte_all, y_tr, y_val, yte)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d037f8d",
   "metadata": {},
   "source": [
    "Ridge și Lasso au fost folosite pentru a evalua efectul regularizării asupra modelelor liniare, în special în contextul numărului mare de variabile codate cu one-hot-encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "842e53e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SGDRegressor with Huber loss - outlier resistant\n",
    "\n",
    "sgd_huber_all = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"model\", SGDRegressor(\n",
    "        loss=\"huber\",\n",
    "        epsilon=1.35,\n",
    "        alpha=1e-4,\n",
    "        learning_rate=\"invscaling\",\n",
    "        eta0=0.01,\n",
    "        max_iter=5000,\n",
    "        tol=1e-3,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "run_and_log(\"SGD_Huber_ALL\",\n",
    "            sgd_huber_all, Xtr_all, Xval_all, Xte_all, y_tr, y_val, yte)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8c7d6a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SGDRegressor Huber on EDA Subset\n",
    "\n",
    "sgd_huber_eda = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"model\", SGDRegressor(\n",
    "        loss=\"huber\",\n",
    "        epsilon=1.35,\n",
    "        alpha=1e-4,\n",
    "        learning_rate=\"invscaling\",\n",
    "        eta0=0.01,\n",
    "        max_iter=5000,\n",
    "        tol=1e-3,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "run_and_log(\"SGD_Huber_EDA\",\n",
    "            sgd_huber_eda, Xtr_eda, Xval_eda, Xte_eda, y_tr, y_val, yte)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "102142dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SGDRegressor Huber with polynomial features\n",
    "\n",
    "sgd_huber_poly = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"model\", SGDRegressor(\n",
    "        loss=\"huber\",\n",
    "        epsilon=1.35,\n",
    "        alpha=1e-4,\n",
    "        learning_rate=\"invscaling\",\n",
    "        eta0=0.01,\n",
    "        max_iter=5000,\n",
    "        tol=1e-3,\n",
    "        random_state=42\n",
    "    ))\n",
    "])\n",
    "run_and_log(\"SGD_Huber_Polynomial\",\n",
    "            sgd_huber_poly, Xtr_poly, Xval_poly, Xte_poly, y_tr, y_val, yte)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "99c74ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree baseline vs regularizat\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "dt_base = DecisionTreeRegressor(random_state=42)\n",
    "run_and_log(\"DecisionTree_baseline\",\n",
    "            dt_base, Xtr_all, Xval_all, Xte_all, y_tr, y_val, yte)\n",
    "\n",
    "dt_reg = DecisionTreeRegressor(\n",
    "    random_state=42,\n",
    "    max_depth=12,\n",
    "    min_samples_leaf=20\n",
    ")\n",
    "run_and_log(\"DecisionTree_regularized\",\n",
    "            dt_reg, Xtr_all, Xval_all, Xte_all, y_tr, y_val, yte)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a1f2cc73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf = RandomForestRegressor(\n",
    "    n_estimators=400,\n",
    "    min_samples_leaf=5,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "run_and_log(\"RandomForest_400_leaf5\",\n",
    "            rf, Xtr_all, Xval_all, Xte_all, y_tr, y_val, yte)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d627d3",
   "metadata": {},
   "source": [
    "Am folosit modele de tip decision tree si random forest deoarece permit captarea relațiilor non-liniare și a interacțiunilor dintre variabile. Mai mult, random forest reduce overfitting-ul prin agregarea mai multor arbori."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "adf94f2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experiment</th>\n",
       "      <th>val_MAE</th>\n",
       "      <th>test_MAE</th>\n",
       "      <th>val_RMSE</th>\n",
       "      <th>test_RMSE</th>\n",
       "      <th>val_R2</th>\n",
       "      <th>test_R2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SGD_Huber_Polynomial</td>\n",
       "      <td>7.238068</td>\n",
       "      <td>7.265582</td>\n",
       "      <td>11.035579</td>\n",
       "      <td>11.119830</td>\n",
       "      <td>0.188938</td>\n",
       "      <td>0.191611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SGD_Huber_ALL</td>\n",
       "      <td>7.296948</td>\n",
       "      <td>7.337199</td>\n",
       "      <td>11.197766</td>\n",
       "      <td>11.262833</td>\n",
       "      <td>0.164923</td>\n",
       "      <td>0.170686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>RandomForest_400_leaf5</td>\n",
       "      <td>7.296031</td>\n",
       "      <td>7.338819</td>\n",
       "      <td>10.617388</td>\n",
       "      <td>10.685355</td>\n",
       "      <td>0.249243</td>\n",
       "      <td>0.253548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>SGD_Huber_EDA</td>\n",
       "      <td>7.355714</td>\n",
       "      <td>7.426453</td>\n",
       "      <td>11.516921</td>\n",
       "      <td>11.605958</td>\n",
       "      <td>0.116643</td>\n",
       "      <td>0.119385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>DecisionTree_regularized</td>\n",
       "      <td>7.450267</td>\n",
       "      <td>7.498203</td>\n",
       "      <td>10.846666</td>\n",
       "      <td>10.950247</td>\n",
       "      <td>0.216469</td>\n",
       "      <td>0.216080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lasso_alpha0.001_ALL</td>\n",
       "      <td>7.610670</td>\n",
       "      <td>7.640544</td>\n",
       "      <td>10.837263</td>\n",
       "      <td>10.887714</td>\n",
       "      <td>0.217827</td>\n",
       "      <td>0.225008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ridge_alpha10_ALL</td>\n",
       "      <td>7.610909</td>\n",
       "      <td>7.640901</td>\n",
       "      <td>10.837326</td>\n",
       "      <td>10.887945</td>\n",
       "      <td>0.217818</td>\n",
       "      <td>0.224975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ridge_alpha1_ALL</td>\n",
       "      <td>7.611095</td>\n",
       "      <td>7.641090</td>\n",
       "      <td>10.837331</td>\n",
       "      <td>10.887991</td>\n",
       "      <td>0.217817</td>\n",
       "      <td>0.224968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LinearRegression_scaled_ALL</td>\n",
       "      <td>7.611115</td>\n",
       "      <td>7.641111</td>\n",
       "      <td>10.837331</td>\n",
       "      <td>10.887996</td>\n",
       "      <td>0.217817</td>\n",
       "      <td>0.224967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LinearRegression_scaled_EDA</td>\n",
       "      <td>7.760068</td>\n",
       "      <td>7.796661</td>\n",
       "      <td>11.156257</td>\n",
       "      <td>11.218165</td>\n",
       "      <td>0.171103</td>\n",
       "      <td>0.177251</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     experiment   val_MAE  test_MAE   val_RMSE  test_RMSE  \\\n",
       "7          SGD_Huber_Polynomial  7.238068  7.265582  11.035579  11.119830   \n",
       "5                 SGD_Huber_ALL  7.296948  7.337199  11.197766  11.262833   \n",
       "10       RandomForest_400_leaf5  7.296031  7.338819  10.617388  10.685355   \n",
       "6                 SGD_Huber_EDA  7.355714  7.426453  11.516921  11.605958   \n",
       "9      DecisionTree_regularized  7.450267  7.498203  10.846666  10.950247   \n",
       "4          Lasso_alpha0.001_ALL  7.610670  7.640544  10.837263  10.887714   \n",
       "3             Ridge_alpha10_ALL  7.610909  7.640901  10.837326  10.887945   \n",
       "2              Ridge_alpha1_ALL  7.611095  7.641090  10.837331  10.887991   \n",
       "0   LinearRegression_scaled_ALL  7.611115  7.641111  10.837331  10.887996   \n",
       "1   LinearRegression_scaled_EDA  7.760068  7.796661  11.156257  11.218165   \n",
       "\n",
       "      val_R2   test_R2  \n",
       "7   0.188938  0.191611  \n",
       "5   0.164923  0.170686  \n",
       "10  0.249243  0.253548  \n",
       "6   0.116643  0.119385  \n",
       "9   0.216469  0.216080  \n",
       "4   0.217827  0.225008  \n",
       "3   0.217818  0.224975  \n",
       "2   0.217817  0.224968  \n",
       "0   0.217817  0.224967  \n",
       "1   0.171103  0.177251  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Final table\n",
    "\n",
    "exp_df = pd.DataFrame(exp_results)\n",
    "\n",
    "exp_df_sorted = exp_df.sort_values(\"test_MAE\")\n",
    "\n",
    "exp_df_sorted[[\n",
    "    \"experiment\",\n",
    "    \"val_MAE\", \"test_MAE\",\n",
    "    \"val_RMSE\", \"test_RMSE\",\n",
    "    \"val_R2\", \"test_R2\"\n",
    "]].head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9d5f3467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model: SGD_Huber_Polynomial\n",
      "Test MAE: 7.265582156228628  | Test R2: 0.19161115773600423\n"
     ]
    }
   ],
   "source": [
    "# Best model from the experiments\n",
    "\n",
    "best = exp_df_sorted.iloc[0]\n",
    "print(\"Best model:\", best[\"experiment\"])\n",
    "print(\"Test MAE:\", best[\"test_MAE\"], \" | Test R2:\", best[\"test_R2\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d96533",
   "metadata": {},
   "source": [
    "Compararea experimentelor pe baza MAE pe setul de test a indicat faptul că modelul SGDRegressor cu Huber loss și polynomial features de ordinul doi oferă cea mai bună performanță predictivă, cu o MAE de 7.27 ore și un R² de 0.19. Rezultatul sugerează că o optimizare robustă la outlieri, combinată cu introducerea controlată a non-liniarităților, îmbunătățește capacitatea de generalizare față de modelele liniare standard și față de cele non-liniare."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9024688",
   "metadata": {},
   "source": [
    "##### 4. Limitations and future work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce19c39",
   "metadata": {},
   "source": [
    "Din punct de vedere al modelării, performanța predictivă este constrânsă de caracterul discret, zero-inflated și heteroscedastic al variabilei hours-per-week, care reduce eficiența metodelor de regresie standard optimizate pentru erori continue. Deși modelele bazate pe gradient descent și transformări nonn-liniare surprind tipare locale, o parte substanțială a variației rămâne neexplicată din cauza factorilor instituționali și organizaționali neobservați, precum regimul contractual, normele ocupaționale sau practicile informale de muncă. Direcții viitoare pot include optimizarea hiperparametrică focalizată, utilizarea unor loss functions adaptate distribuțiilor discrete și explorarea unor formulări alternative ale problemei (de exemplu, regresie ordinală sau modele ierarhice), care ar permite integrarea mai explicită a constrângerilor sociale ce structurează timpul de muncă."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
