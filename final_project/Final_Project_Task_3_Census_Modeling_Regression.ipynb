{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydHb_ZL5yy6f"
      },
      "source": [
        "# **Final Project Task 3 - Census Modeling Regression**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnzXS8Oo9jwY"
      },
      "source": [
        "Requirements\n",
        "\n",
        "- You can use models (estmators) from sklearn, but feel free to use any library for traditional ML. \n",
        "    - Note: in sklearn, the LinearRegression estimator is based on OLS, a statistical method. Please use the SGDRegressor estimator, since this is based on gradient descent. \n",
        "    - You can use LinearRegression estimator, but only as comparison with the SGDRegressor - Optional.\n",
        "\n",
        "- Model Selection and Setup:\n",
        "    - Implement multiple models, to solve a regression problem using traditional ML:\n",
        "        - Linear Regression\n",
        "        - Decision Tree Regression\n",
        "        - Random Forest Regression - Optional\n",
        "        - Ridge Regression - Optional\n",
        "        - Lasso Regression - Optional\n",
        "    - Choose a loss (or experiment with different losses) for the model and justify the choice.\n",
        "        - MSE, MAE, RMSE, Huber Loss or others\n",
        "    - Justify model choices based on dataset characteristics and task requirements; specify model pros and cons.\n",
        "\n",
        "\n",
        "- Data Preparation\n",
        "    - Use the preprocessed datasets from Task 1.\n",
        "    - From the train set, create an extra validation set, if necesarry. So in total there will be: train, validation and test datasets.\n",
        "    - Be sure all models have their data preprocessed as needed. Some models require different, or no encoding for some features.\n",
        "\n",
        "\n",
        "- Model Training and Experimentation\n",
        "    - Establish a Baseline Model:\n",
        "        - For each model type, train a simple model with default settings as a baseline.\n",
        "        - Evaluate its performance to establish a benchmark for comparison.\n",
        "    - Make plots with train, validation loss and metric on epochs (or on steps), if applicable. - Optional\n",
        "    - Feature Selection:\n",
        "        - Use insights from EDA in Task 2 to identify candidate features by analyzing patterns, relationships, and distributions.\n",
        "    - Experimentation:\n",
        "        - For each baseline model type, iteratively experiment with different combinations of features and transformations.\n",
        "        - Experiment with feature engineering techniques such as interaction terms, polynomial features, or scaling transformations.\n",
        "        - Identify the best model which have the best performance metrics on test set.\n",
        "    - Hyperparameter Tuning:\n",
        "        - Perform hyperparameter tuning only on the best-performing model after evaluating all model types and experiments.\n",
        "        - Avoid tuning models that do not show strong baseline performance or are unlikely to outperform others based on experimentation.\n",
        "        - Ensure that hyperparameter tuning is done after completing feature selection, baseline modeling, and experimentation, ensuring that the model is stable and representative of the dataset.\n",
        "\n",
        "\n",
        "- Model Evaluation\n",
        "    - Evaluate models on the test dataset using regression metrics:\n",
        "        - Mean Absolute Error (MAE)\n",
        "        - Mean Squared Error (MSE)\n",
        "        - Root Mean Squared Error (RMSE)\n",
        "        - RÂ² Score\n",
        "    - Compare the results across different models. Save all experiment results into a table.\n",
        "\n",
        "Feature Importance - Optional\n",
        "- For applicable models (e.g., Decision Tree Regression), analyze feature importance and discuss its relevance to the problem.\n",
        "\n",
        "\n",
        "\n",
        "Deliverables\n",
        "\n",
        "- Notebook code with no errors.\n",
        "- Code and results from experiments. Create a table with all experiments results, include experiment name, metrics results.\n",
        "- Explain findings, choices, results.\n",
        "- Potential areas for improvement or further exploration.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "id": "xifylnglyn2W"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import  GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
        "from sklearn.linear_model import LinearRegression, SGDRegressor, HuberRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.linear_model import Ridge, Lasso\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        },
        "id": "CzWyJfHkyn-8",
        "outputId": "633344dd-56a2-44a7-a237-a26873898c80"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>fnlwgt</th>\n",
              "      <th>education</th>\n",
              "      <th>education-num</th>\n",
              "      <th>sex</th>\n",
              "      <th>capital-gain</th>\n",
              "      <th>capital-loss</th>\n",
              "      <th>hours-per-week</th>\n",
              "      <th>income</th>\n",
              "      <th>workclass_Local-gov</th>\n",
              "      <th>...</th>\n",
              "      <th>native-country_Thailand</th>\n",
              "      <th>native-country_Trinadad&amp;Tobago</th>\n",
              "      <th>native-country_United-States</th>\n",
              "      <th>native-country_Vietnam</th>\n",
              "      <th>native-country_Yugoslavia</th>\n",
              "      <th>capital_balance</th>\n",
              "      <th>capital_gain_loss_ratio</th>\n",
              "      <th>high_hour_worker</th>\n",
              "      <th>part_time_worker</th>\n",
              "      <th>experience_years</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.030390</td>\n",
              "      <td>-1.063569</td>\n",
              "      <td>Bachelors</td>\n",
              "      <td>1.134777</td>\n",
              "      <td>1</td>\n",
              "      <td>0.611335</td>\n",
              "      <td>-0.218867</td>\n",
              "      <td>-0.029928</td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "      <td>...</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>0.830202</td>\n",
              "      <td>2.062817</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.836973</td>\n",
              "      <td>-1.008668</td>\n",
              "      <td>Bachelors</td>\n",
              "      <td>1.134777</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.251583</td>\n",
              "      <td>-0.218867</td>\n",
              "      <td>-2.273618</td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "      <td>...</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>-0.032716</td>\n",
              "      <td>0.958118</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.042936</td>\n",
              "      <td>0.245040</td>\n",
              "      <td>HS-grad</td>\n",
              "      <td>-0.420679</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.251583</td>\n",
              "      <td>-0.218867</td>\n",
              "      <td>-0.029928</td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "      <td>...</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>-0.032716</td>\n",
              "      <td>0.958118</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.056950</td>\n",
              "      <td>0.425752</td>\n",
              "      <td>11th</td>\n",
              "      <td>-1.198407</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.251583</td>\n",
              "      <td>-0.218867</td>\n",
              "      <td>-0.029928</td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "      <td>...</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>-0.032716</td>\n",
              "      <td>0.958118</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.776193</td>\n",
              "      <td>1.408066</td>\n",
              "      <td>Bachelors</td>\n",
              "      <td>1.134777</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.251583</td>\n",
              "      <td>-0.218867</td>\n",
              "      <td>-0.029928</td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "      <td>...</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>-0.032716</td>\n",
              "      <td>0.958118</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>-0.116262</td>\n",
              "      <td>0.898122</td>\n",
              "      <td>Masters</td>\n",
              "      <td>1.523641</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.251583</td>\n",
              "      <td>-0.218867</td>\n",
              "      <td>-0.029928</td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "      <td>...</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>-0.032716</td>\n",
              "      <td>0.958118</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.763647</td>\n",
              "      <td>-0.280365</td>\n",
              "      <td>9th</td>\n",
              "      <td>-1.976134</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.251583</td>\n",
              "      <td>-0.218867</td>\n",
              "      <td>-2.024319</td>\n",
              "      <td>0</td>\n",
              "      <td>False</td>\n",
              "      <td>...</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>-0.032716</td>\n",
              "      <td>0.958118</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.983625</td>\n",
              "      <td>0.188160</td>\n",
              "      <td>HS-grad</td>\n",
              "      <td>-0.420679</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.251583</td>\n",
              "      <td>-0.218867</td>\n",
              "      <td>0.385570</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>...</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>-0.032716</td>\n",
              "      <td>0.958118</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>-0.556216</td>\n",
              "      <td>-1.364218</td>\n",
              "      <td>Masters</td>\n",
              "      <td>1.523641</td>\n",
              "      <td>0</td>\n",
              "      <td>5.338724</td>\n",
              "      <td>-0.218867</td>\n",
              "      <td>0.801069</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>...</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>5.557591</td>\n",
              "      <td>8.114783</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.250367</td>\n",
              "      <td>-0.287356</td>\n",
              "      <td>Bachelors</td>\n",
              "      <td>1.134777</td>\n",
              "      <td>1</td>\n",
              "      <td>1.803701</td>\n",
              "      <td>-0.218867</td>\n",
              "      <td>-0.029928</td>\n",
              "      <td>1</td>\n",
              "      <td>False</td>\n",
              "      <td>...</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>2.022568</td>\n",
              "      <td>3.589275</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10 rows Ã 89 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        age    fnlwgt  education  education-num  sex  capital-gain  \\\n",
              "0  0.030390 -1.063569  Bachelors       1.134777    1      0.611335   \n",
              "1  0.836973 -1.008668  Bachelors       1.134777    1     -0.251583   \n",
              "2 -0.042936  0.245040    HS-grad      -0.420679    1     -0.251583   \n",
              "3  1.056950  0.425752       11th      -1.198407    1     -0.251583   \n",
              "4 -0.776193  1.408066  Bachelors       1.134777    0     -0.251583   \n",
              "5 -0.116262  0.898122    Masters       1.523641    0     -0.251583   \n",
              "6  0.763647 -0.280365        9th      -1.976134    0     -0.251583   \n",
              "7  0.983625  0.188160    HS-grad      -0.420679    1     -0.251583   \n",
              "8 -0.556216 -1.364218    Masters       1.523641    0      5.338724   \n",
              "9  0.250367 -0.287356  Bachelors       1.134777    1      1.803701   \n",
              "\n",
              "   capital-loss  hours-per-week  income  workclass_Local-gov  ...  \\\n",
              "0     -0.218867       -0.029928       0                False  ...   \n",
              "1     -0.218867       -2.273618       0                False  ...   \n",
              "2     -0.218867       -0.029928       0                False  ...   \n",
              "3     -0.218867       -0.029928       0                False  ...   \n",
              "4     -0.218867       -0.029928       0                False  ...   \n",
              "5     -0.218867       -0.029928       0                False  ...   \n",
              "6     -0.218867       -2.024319       0                False  ...   \n",
              "7     -0.218867        0.385570       1                False  ...   \n",
              "8     -0.218867        0.801069       1                False  ...   \n",
              "9     -0.218867       -0.029928       1                False  ...   \n",
              "\n",
              "   native-country_Thailand  native-country_Trinadad&Tobago  \\\n",
              "0                    False                           False   \n",
              "1                    False                           False   \n",
              "2                    False                           False   \n",
              "3                    False                           False   \n",
              "4                    False                           False   \n",
              "5                    False                           False   \n",
              "6                    False                           False   \n",
              "7                    False                           False   \n",
              "8                    False                           False   \n",
              "9                    False                           False   \n",
              "\n",
              "   native-country_United-States  native-country_Vietnam  \\\n",
              "0                          True                   False   \n",
              "1                          True                   False   \n",
              "2                          True                   False   \n",
              "3                          True                   False   \n",
              "4                         False                   False   \n",
              "5                          True                   False   \n",
              "6                         False                   False   \n",
              "7                          True                   False   \n",
              "8                          True                   False   \n",
              "9                          True                   False   \n",
              "\n",
              "   native-country_Yugoslavia  capital_balance  capital_gain_loss_ratio  \\\n",
              "0                      False         0.830202                 2.062817   \n",
              "1                      False        -0.032716                 0.958118   \n",
              "2                      False        -0.032716                 0.958118   \n",
              "3                      False        -0.032716                 0.958118   \n",
              "4                      False        -0.032716                 0.958118   \n",
              "5                      False        -0.032716                 0.958118   \n",
              "6                      False        -0.032716                 0.958118   \n",
              "7                      False        -0.032716                 0.958118   \n",
              "8                      False         5.557591                 8.114783   \n",
              "9                      False         2.022568                 3.589275   \n",
              "\n",
              "   high_hour_worker  part_time_worker  experience_years  \n",
              "0                 0                 1                 0  \n",
              "1                 0                 1                 0  \n",
              "2                 0                 1                 0  \n",
              "3                 0                 1                 0  \n",
              "4                 0                 1                 0  \n",
              "5                 0                 1                 0  \n",
              "6                 0                 1                 0  \n",
              "7                 0                 1                 0  \n",
              "8                 0                 1                 0  \n",
              "9                 0                 1                 0  \n",
              "\n",
              "[10 rows x 89 columns]"
            ]
          },
          "execution_count": 127,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Import data from task1\n",
        "data = pd.read_csv(r\"C:\\Users\\Giulia\\OneDrive - Universitatea BabeÅ-Bolyai\\ADC\\sem3\\ML\\ubb-sociology-ml\\final_project/data_task1.csv\")\n",
        "data.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Index(['age', 'fnlwgt', 'education', 'education-num', 'sex', 'capital-gain',\n",
              "       'capital-loss', 'hours-per-week', 'income', 'workclass_Local-gov',\n",
              "       'workclass_Never-worked', 'workclass_Private', 'workclass_Self-emp-inc',\n",
              "       'workclass_Self-emp-not-inc', 'workclass_State-gov',\n",
              "       'workclass_Without-pay', 'marital-status_Married-AF-spouse',\n",
              "       'marital-status_Married-civ-spouse',\n",
              "       'marital-status_Married-spouse-absent', 'marital-status_Never-married',\n",
              "       'marital-status_Separated', 'marital-status_Widowed',\n",
              "       'occupation_Armed-Forces', 'occupation_Craft-repair',\n",
              "       'occupation_Exec-managerial', 'occupation_Farming-fishing',\n",
              "       'occupation_Handlers-cleaners', 'occupation_Machine-op-inspct',\n",
              "       'occupation_Other-service', 'occupation_Priv-house-serv',\n",
              "       'occupation_Prof-specialty', 'occupation_Protective-serv',\n",
              "       'occupation_Sales', 'occupation_Tech-support',\n",
              "       'occupation_Transport-moving', 'relationship_Not-in-family',\n",
              "       'relationship_Other-relative', 'relationship_Own-child',\n",
              "       'relationship_Unmarried', 'relationship_Wife',\n",
              "       'race_Asian-Pac-Islander', 'race_Black', 'race_Other', 'race_White',\n",
              "       'native-country_Canada', 'native-country_China',\n",
              "       'native-country_Columbia', 'native-country_Cuba',\n",
              "       'native-country_Dominican-Republic', 'native-country_Ecuador',\n",
              "       'native-country_El-Salvador', 'native-country_England',\n",
              "       'native-country_France', 'native-country_Germany',\n",
              "       'native-country_Greece', 'native-country_Guatemala',\n",
              "       'native-country_Haiti', 'native-country_Holand-Netherlands',\n",
              "       'native-country_Honduras', 'native-country_Hong',\n",
              "       'native-country_Hungary', 'native-country_India', 'native-country_Iran',\n",
              "       'native-country_Ireland', 'native-country_Italy',\n",
              "       'native-country_Jamaica', 'native-country_Japan', 'native-country_Laos',\n",
              "       'native-country_Mexico', 'native-country_Nicaragua',\n",
              "       'native-country_Outlying-US(Guam-USVI-etc)', 'native-country_Peru',\n",
              "       'native-country_Philippines', 'native-country_Poland',\n",
              "       'native-country_Portugal', 'native-country_Puerto-Rico',\n",
              "       'native-country_Scotland', 'native-country_South',\n",
              "       'native-country_Taiwan', 'native-country_Thailand',\n",
              "       'native-country_Trinadad&Tobago', 'native-country_United-States',\n",
              "       'native-country_Vietnam', 'native-country_Yugoslavia',\n",
              "       'capital_balance', 'capital_gain_loss_ratio', 'high_hour_worker',\n",
              "       'part_time_worker', 'experience_years'],\n",
              "      dtype='object')"
            ]
          },
          "execution_count": 128,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>fnlwgt</th>\n",
              "      <th>education-num</th>\n",
              "      <th>sex</th>\n",
              "      <th>capital-gain</th>\n",
              "      <th>capital-loss</th>\n",
              "      <th>hours-per-week</th>\n",
              "      <th>income</th>\n",
              "      <th>capital_balance</th>\n",
              "      <th>capital_gain_loss_ratio</th>\n",
              "      <th>high_hour_worker</th>\n",
              "      <th>part_time_worker</th>\n",
              "      <th>experience_years</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>3.253700e+04</td>\n",
              "      <td>3.253700e+04</td>\n",
              "      <td>3.253700e+04</td>\n",
              "      <td>32537.000000</td>\n",
              "      <td>3.253700e+04</td>\n",
              "      <td>3.253700e+04</td>\n",
              "      <td>3.253700e+04</td>\n",
              "      <td>32537.000000</td>\n",
              "      <td>3.253700e+04</td>\n",
              "      <td>32537.000000</td>\n",
              "      <td>32537.0</td>\n",
              "      <td>32537.0</td>\n",
              "      <td>32537.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>-7.883515e-17</td>\n",
              "      <td>-1.241490e-16</td>\n",
              "      <td>5.481336e-17</td>\n",
              "      <td>0.669238</td>\n",
              "      <td>-2.107366e-17</td>\n",
              "      <td>-4.957224e-17</td>\n",
              "      <td>-3.712459e-17</td>\n",
              "      <td>0.240926</td>\n",
              "      <td>1.681525e-17</td>\n",
              "      <td>1.242050</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>1.000015e+00</td>\n",
              "      <td>1.000015e+00</td>\n",
              "      <td>1.000015e+00</td>\n",
              "      <td>0.470495</td>\n",
              "      <td>1.000015e+00</td>\n",
              "      <td>1.000015e+00</td>\n",
              "      <td>1.000015e+00</td>\n",
              "      <td>0.427652</td>\n",
              "      <td>1.452650e+00</td>\n",
              "      <td>1.301265</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>-1.582777e+00</td>\n",
              "      <td>-1.681551e+00</td>\n",
              "      <td>-3.531590e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-2.515826e-01</td>\n",
              "      <td>-2.188670e-01</td>\n",
              "      <td>-3.270814e+00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-5.240014e+00</td>\n",
              "      <td>0.124977</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>-7.761933e-01</td>\n",
              "      <td>-6.816726e-01</td>\n",
              "      <td>-4.206787e-01</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-2.515826e-01</td>\n",
              "      <td>-2.188670e-01</td>\n",
              "      <td>-2.992782e-02</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-3.271557e-02</td>\n",
              "      <td>0.958118</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>-1.162616e-01</td>\n",
              "      <td>-1.082361e-01</td>\n",
              "      <td>-3.181473e-02</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-2.515826e-01</td>\n",
              "      <td>-2.188670e-01</td>\n",
              "      <td>-2.992782e-02</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-3.271557e-02</td>\n",
              "      <td>0.958118</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>6.903217e-01</td>\n",
              "      <td>4.472760e-01</td>\n",
              "      <td>7.459132e-01</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-2.515826e-01</td>\n",
              "      <td>-2.188670e-01</td>\n",
              "      <td>3.855704e-01</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>-3.271557e-02</td>\n",
              "      <td>0.958118</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>3.770003e+00</td>\n",
              "      <td>1.226778e+01</td>\n",
              "      <td>2.301369e+00</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>5.702309e+00</td>\n",
              "      <td>4.988431e+00</td>\n",
              "      <td>3.294058e+00</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>5.921176e+00</td>\n",
              "      <td>8.580240</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                age        fnlwgt  education-num           sex  capital-gain  \\\n",
              "count  3.253700e+04  3.253700e+04   3.253700e+04  32537.000000  3.253700e+04   \n",
              "mean  -7.883515e-17 -1.241490e-16   5.481336e-17      0.669238 -2.107366e-17   \n",
              "std    1.000015e+00  1.000015e+00   1.000015e+00      0.470495  1.000015e+00   \n",
              "min   -1.582777e+00 -1.681551e+00  -3.531590e+00      0.000000 -2.515826e-01   \n",
              "25%   -7.761933e-01 -6.816726e-01  -4.206787e-01      0.000000 -2.515826e-01   \n",
              "50%   -1.162616e-01 -1.082361e-01  -3.181473e-02      1.000000 -2.515826e-01   \n",
              "75%    6.903217e-01  4.472760e-01   7.459132e-01      1.000000 -2.515826e-01   \n",
              "max    3.770003e+00  1.226778e+01   2.301369e+00      1.000000  5.702309e+00   \n",
              "\n",
              "       capital-loss  hours-per-week        income  capital_balance  \\\n",
              "count  3.253700e+04    3.253700e+04  32537.000000     3.253700e+04   \n",
              "mean  -4.957224e-17   -3.712459e-17      0.240926     1.681525e-17   \n",
              "std    1.000015e+00    1.000015e+00      0.427652     1.452650e+00   \n",
              "min   -2.188670e-01   -3.270814e+00      0.000000    -5.240014e+00   \n",
              "25%   -2.188670e-01   -2.992782e-02      0.000000    -3.271557e-02   \n",
              "50%   -2.188670e-01   -2.992782e-02      0.000000    -3.271557e-02   \n",
              "75%   -2.188670e-01    3.855704e-01      0.000000    -3.271557e-02   \n",
              "max    4.988431e+00    3.294058e+00      1.000000     5.921176e+00   \n",
              "\n",
              "       capital_gain_loss_ratio  high_hour_worker  part_time_worker  \\\n",
              "count             32537.000000           32537.0           32537.0   \n",
              "mean                  1.242050               0.0               1.0   \n",
              "std                   1.301265               0.0               0.0   \n",
              "min                   0.124977               0.0               1.0   \n",
              "25%                   0.958118               0.0               1.0   \n",
              "50%                   0.958118               0.0               1.0   \n",
              "75%                   0.958118               0.0               1.0   \n",
              "max                   8.580240               0.0               1.0   \n",
              "\n",
              "       experience_years  \n",
              "count           32537.0  \n",
              "mean                0.0  \n",
              "std                 0.0  \n",
              "min                 0.0  \n",
              "25%                 0.0  \n",
              "50%                 0.0  \n",
              "75%                 0.0  \n",
              "max                 0.0  "
            ]
          },
          "execution_count": 129,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Identifying and removing redundant features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {},
      "outputs": [],
      "source": [
        "data.drop(columns=['education', 'high_hour_worker', 'part_time_worker'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Strongly correlated feature pairs (threshold > 0.85):\n",
            "         Feature 1                Feature 2  Correlation\n",
            "0     capital-gain  capital_gain_loss_ratio     0.991195\n",
            "1  capital_balance  capital_gain_loss_ratio     0.810400\n"
          ]
        }
      ],
      "source": [
        "#Compute the correlation matrix for the dataset to assess relationships between features\n",
        "corr_matrix = data.corr()\n",
        "\n",
        "#Define a threshold to flag strong correlations that may indicate multicollinearity\n",
        "corr_threshold = 0.80\n",
        "\n",
        "#Identify pairs of features with high correlation, excluding self-correlation\n",
        "high_correlations = [\n",
        "    (feature_1, feature_2, corr_matrix.loc[feature_1, feature_2])\n",
        "    for idx_1, feature_1 in enumerate(corr_matrix.columns)\n",
        "    for idx_2, feature_2 in enumerate(corr_matrix.columns[idx_1 + 1:])\n",
        "    if abs(corr_matrix.loc[feature_1, feature_2]) > corr_threshold\n",
        "]\n",
        "\n",
        "# Convert the identified feature pairs into a DataFrame for easier analysis\n",
        "df_high_corr = pd.DataFrame(high_correlations, columns=[\"Feature 1\", \"Feature 2\", \"Correlation\"])\n",
        "\n",
        "\n",
        "print('Strongly correlated feature pairs (threshold > 0.85):')\n",
        "print(df_high_corr)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index(['age', 'fnlwgt', 'education-num', 'sex', 'capital-gain', 'capital-loss',\n",
            "       'hours-per-week', 'income', 'workclass_Local-gov',\n",
            "       'workclass_Never-worked', 'workclass_Private', 'workclass_Self-emp-inc',\n",
            "       'workclass_Self-emp-not-inc', 'workclass_State-gov',\n",
            "       'workclass_Without-pay', 'marital-status_Married-AF-spouse',\n",
            "       'marital-status_Married-civ-spouse',\n",
            "       'marital-status_Married-spouse-absent', 'marital-status_Never-married',\n",
            "       'marital-status_Separated', 'marital-status_Widowed',\n",
            "       'occupation_Armed-Forces', 'occupation_Craft-repair',\n",
            "       'occupation_Exec-managerial', 'occupation_Farming-fishing',\n",
            "       'occupation_Handlers-cleaners', 'occupation_Machine-op-inspct',\n",
            "       'occupation_Other-service', 'occupation_Priv-house-serv',\n",
            "       'occupation_Prof-specialty', 'occupation_Protective-serv',\n",
            "       'occupation_Sales', 'occupation_Tech-support',\n",
            "       'occupation_Transport-moving', 'relationship_Not-in-family',\n",
            "       'relationship_Other-relative', 'relationship_Own-child',\n",
            "       'relationship_Unmarried', 'relationship_Wife',\n",
            "       'race_Asian-Pac-Islander', 'race_Black', 'race_Other', 'race_White',\n",
            "       'native-country_Canada', 'native-country_China',\n",
            "       'native-country_Columbia', 'native-country_Cuba',\n",
            "       'native-country_Dominican-Republic', 'native-country_Ecuador',\n",
            "       'native-country_El-Salvador', 'native-country_England',\n",
            "       'native-country_France', 'native-country_Germany',\n",
            "       'native-country_Greece', 'native-country_Guatemala',\n",
            "       'native-country_Haiti', 'native-country_Holand-Netherlands',\n",
            "       'native-country_Honduras', 'native-country_Hong',\n",
            "       'native-country_Hungary', 'native-country_India', 'native-country_Iran',\n",
            "       'native-country_Ireland', 'native-country_Italy',\n",
            "       'native-country_Jamaica', 'native-country_Japan', 'native-country_Laos',\n",
            "       'native-country_Mexico', 'native-country_Nicaragua',\n",
            "       'native-country_Outlying-US(Guam-USVI-etc)', 'native-country_Peru',\n",
            "       'native-country_Philippines', 'native-country_Poland',\n",
            "       'native-country_Portugal', 'native-country_Puerto-Rico',\n",
            "       'native-country_Scotland', 'native-country_South',\n",
            "       'native-country_Taiwan', 'native-country_Thailand',\n",
            "       'native-country_Trinadad&Tobago', 'native-country_United-States',\n",
            "       'native-country_Vietnam', 'native-country_Yugoslavia'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "source": [
        "#Remove the identified redundant features from the dataset\n",
        "data.drop(columns=['experience_years','capital_balance','capital_gain_loss_ratio'], inplace=True)\n",
        "\n",
        "#Display the remaining columns to verify the changes\n",
        "print(data.columns)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Removing redundant variables is a key step in data preprocessing for machine learning and statistical modeling. Highly correlated features can cause multicollinearity making it harder to interpret model coefficients and leading to overfitting. They also add unnecessary complexity increasing computation time without improving predictive power. After analyzing the correlation matrix, I identified and removed the following redundant features: capital_balance, capital_gain_loss_ratio. I also removed the variable experience_years because the machine learning models achieved better performance metrics without it.\n",
        "\n",
        "The education column was removed because it had a direct numerical equivalent, education-num, which conveys the same information in a more practical format.I also dropped high_hour_worker and part_time_worker since they are directly derived from hours_per_week, our target variable. Keeping them could lead to data leakage and reduce the modelâs ability to learn meaningful relationships."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Define the target variable\n",
        "target = \"hours-per-week\"\n",
        "\n",
        "#Split features (x) and target variable (y)\n",
        "X = data.drop(columns=[target])\n",
        "y = data[target]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Split the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A validation set is essential when performing hyperparameter tuning as it helps assess different parameter choices without overfitting to the test set. By splitting the data into train (70%), validation (15%) and test (15%), we ensure that the model is trained on one subset, fine-tuned on another and finally evaluated on unseen data. This approach prevents data leakage and improves generalization. Since I will be working on hyperparameter tuning, I choose to include a validation set in my data split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train set: (22775, 82), Validation set: (4881, 82), Test set: (4881, 82)\n"
          ]
        }
      ],
      "source": [
        "#Split data into train (70%) and temp (30%) sets\n",
        "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "#Split temp set into validation (15%) and test (15%) sets\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "#Print the shapes of the resulting sets\n",
        "print(f\"Train set: {X_train.shape}, Validation set: {X_val.shape}, Test set: {X_test.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preprocess transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Features summary: Mean and standard deviation of training data\n",
            "                   Mean  Standard Deviation\n",
            "age           -0.001947            0.998329\n",
            "fnlwgt         0.007067            0.999578\n",
            "education-num  0.001497            0.998436\n",
            "sex            0.667398            0.471155\n",
            "capital-gain  -0.010661            0.980499\n",
            "capital-loss   0.005340            1.013206\n",
            "income         0.240703            0.427519\n"
          ]
        }
      ],
      "source": [
        "#Calculate the mean and std for each feature in the training dataset\n",
        "feature_mean = X_train.mean()\n",
        "feature_std = X_train.std()\n",
        "\n",
        "#Store the computed values in a DataFrame for easy inspection\n",
        "features_summary = pd.DataFrame({\"Mean\": feature_mean, \"Standard Deviation\": feature_std})\n",
        "\n",
        "# Display the first few rows to verify the standardization process\n",
        "print(\"Features summary: Mean and standard deviation of training data\")\n",
        "print(features_summary.head(7))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Fnlwgt has a very high mean and variance so StandardScaler will be used to normalize it with a mean of 0 and a standard deviation of 1. Capital-gain and capital-loss both exhibit extreme variability, suggesting the presence of outliers, making RobustScaler the better choice since it is less sensitive to outliers. Age has a relatively normal distribution, so either StandardScaler (for centering) or MinMaxScaler (for scaling between 0 and 1) can be used. Education-num has small values, making scaling optional, but StandardScaler is preferred for models sensitive to feature scaling."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scaled training set preview:\n",
            "            age    fnlwgt  education-num  sex  capital-gain  capital-loss  \\\n",
            "1216   0.693443  0.503319      -0.033365    1        7688.0           0.0   \n",
            "27941 -0.555208  0.481795       1.135080    1           0.0           0.0   \n",
            "23063  0.840343 -1.391011       2.303525    1       15024.0           0.0   \n",
            "19670 -1.216259 -0.006054      -0.422846    1           0.0           0.0   \n",
            "19172 -0.775558 -0.787768      -0.422846    0           0.0           0.0   \n",
            "\n",
            "       income  workclass_Local-gov  workclass_Never-worked  workclass_Private  \\\n",
            "1216        1                False                   False              False   \n",
            "27941       0                False                   False               True   \n",
            "23063       1                False                   False               True   \n",
            "19670       0                False                   False               True   \n",
            "19172       0                False                   False               True   \n",
            "\n",
            "       ...  native-country_Portugal  native-country_Puerto-Rico  \\\n",
            "1216   ...                    False                       False   \n",
            "27941  ...                    False                       False   \n",
            "23063  ...                    False                       False   \n",
            "19670  ...                    False                       False   \n",
            "19172  ...                    False                       False   \n",
            "\n",
            "       native-country_Scotland  native-country_South  native-country_Taiwan  \\\n",
            "1216                     False                 False                  False   \n",
            "27941                    False                 False                  False   \n",
            "23063                    False                 False                  False   \n",
            "19670                    False                 False                  False   \n",
            "19172                    False                 False                  False   \n",
            "\n",
            "       native-country_Thailand  native-country_Trinadad&Tobago  \\\n",
            "1216                     False                           False   \n",
            "27941                    False                           False   \n",
            "23063                    False                           False   \n",
            "19670                    False                           False   \n",
            "19172                    False                           False   \n",
            "\n",
            "       native-country_United-States  native-country_Vietnam  \\\n",
            "1216                           True                   False   \n",
            "27941                          True                   False   \n",
            "23063                          True                   False   \n",
            "19670                          True                   False   \n",
            "19172                          True                   False   \n",
            "\n",
            "       native-country_Yugoslavia  \n",
            "1216                       False  \n",
            "27941                      False  \n",
            "23063                      False  \n",
            "19670                      False  \n",
            "19172                      False  \n",
            "\n",
            "[5 rows x 82 columns]\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "def apply_scaling(X_train, X_val, X_test, standard_columns, robust_columns):\n",
        "    \"\"\"Apply StandardScaler to some columns and RobustScaler to others in train, validation, and test sets.\"\"\"\n",
        "    \n",
        "    #Create copies to avoid modifying the original datasets\n",
        "    X_train_scaled = X_train.copy()\n",
        "    X_val_scaled = X_val.copy()\n",
        "    X_test_scaled = X_test.copy()\n",
        "\n",
        "    #Apply StandardScaler to selected columns\n",
        "    ss = StandardScaler()\n",
        "    X_train_scaled[standard_columns] = ss.fit_transform(X_train[standard_columns])\n",
        "    X_val_scaled[standard_columns] = ss.transform(X_val[standard_columns])\n",
        "    X_test_scaled[standard_columns] = ss.transform(X_test[standard_columns])\n",
        "\n",
        "    #Apply RobustScaler to selected columns\n",
        "    rs = RobustScaler()\n",
        "    X_train_scaled[robust_columns] = rs.fit_transform(X_train[robust_columns])\n",
        "    X_val_scaled[robust_columns] = rs.transform(X_val[robust_columns])\n",
        "    X_test_scaled[robust_columns] = rs.transform(X_test[robust_columns])\n",
        "\n",
        "    return X_train_scaled, X_val_scaled, X_test_scaled\n",
        "\n",
        "#Define which columns to scale with each method\n",
        "standard_columns = [\"fnlwgt\", \"age\", \"education-num\"]\n",
        "robust_columns = [\"capital-gain\", \"capital-loss\"]\n",
        "\n",
        "#Apply scaling to training, validation, and test sets\n",
        "X_train_scaled, X_val_scaled, X_test_scaled = apply_scaling(X_train, X_val, X_test, standard_columns, robust_columns)\n",
        "\n",
        "#Verify transformation\n",
        "print(\"Scaled training set preview:\")\n",
        "print(X_train_scaled.head())\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "StandardScaler Features (Mean ~ 0, Std ~ 1):\n",
            "                age        fnlwgt  education-num\n",
            "count  2.277500e+04  2.277500e+04   2.277500e+04\n",
            "mean   2.296200e-16  6.629652e-17   9.671493e-17\n",
            "std    1.000022e+00  1.000022e+00   1.000022e+00\n",
            "min   -1.583509e+00 -1.689368e+00  -3.538699e+00\n",
            "25%   -7.755585e-01 -6.835197e-01  -4.228462e-01\n",
            "50%   -1.145079e-01 -1.113356e-01  -3.336459e-02\n",
            "75%    6.934428e-01  4.533088e-01   7.455986e-01\n",
            "max    3.778345e+00  1.226616e+01   2.303525e+00\n",
            "\n",
            "RobustScaler Features (IQR-based scaling):\n",
            "       capital-gain  capital-loss\n",
            "count  22775.000000  22775.000000\n",
            "mean     607.404610     85.908101\n",
            "std     2472.774508    388.146136\n",
            "min        0.000000      0.000000\n",
            "25%        0.000000      0.000000\n",
            "50%        0.000000      0.000000\n",
            "75%        0.000000      0.000000\n",
            "max    15024.000000   1980.000000\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "#Verify mean and standard deviation for StandardScaler features\n",
        "print(\"StandardScaler Features (Mean ~ 0, Std ~ 1):\")\n",
        "print(X_train_scaled[[\"age\", \"fnlwgt\", \"education-num\"]].describe())\n",
        "\n",
        "#Verify capital-gain and capital-loss scaling with RobustScaler\n",
        "print(\"\\nRobustScaler Features (IQR-based scaling):\")\n",
        "print(X_train_scaled[[\"capital-gain\", \"capital-loss\"]].describe())\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "       capital-gain  capital-loss\n",
            "count  22775.000000  22775.000000\n",
            "mean     607.404610     85.908101\n",
            "std     2472.774508    388.146136\n",
            "min        0.000000      0.000000\n",
            "25%        0.000000      0.000000\n",
            "50%        0.000000      0.000000\n",
            "75%        0.000000      0.000000\n",
            "max    15024.000000   1980.000000\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "\n",
        "#Initialize RobustScaler\n",
        "rs = RobustScaler()\n",
        "\n",
        "#Fit and transform capital-gain and capital-loss\n",
        "X_train_scaled = X_train.copy()\n",
        "X_val_scaled = X_val.copy()\n",
        "X_test_scaled = X_test.copy()\n",
        "\n",
        "X_train_scaled[[\"capital-gain\", \"capital-loss\"]] = rs.fit_transform(X_train[[\"capital-gain\", \"capital-loss\"]])\n",
        "X_val_scaled[[\"capital-gain\", \"capital-loss\"]] = rs.transform(X_val[[\"capital-gain\", \"capital-loss\"]])\n",
        "X_test_scaled[[\"capital-gain\", \"capital-loss\"]] = rs.transform(X_test[[\"capital-gain\", \"capital-loss\"]])\n",
        "\n",
        "#Check transformed output\n",
        "print(X_train_scaled[[\"capital-gain\", \"capital-loss\"]].describe())\n",
        "\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Before Scaling - Unique Values in capital-gain:\n",
            "[ 7688     0 15024  3325  6849  4650  5013  2105  2829  7298]\n",
            "\n",
            "After Scaling - Unique Values in capital-gain:\n",
            "[ 7688.     0. 15024.  3325.  6849.  4650.  5013.  2105.  2829.  7298.]\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "print(\"Before Scaling - Unique Values in capital-gain:\")\n",
        "print(X_train[\"capital-gain\"].unique()[:10]) \n",
        "\n",
        "X_train_scaled[[\"capital-gain\", \"capital-loss\"]] = rs.fit_transform(X_train[[\"capital-gain\", \"capital-loss\"]])\n",
        "\n",
        "print(\"\\nAfter Scaling - Unique Values in capital-gain:\")\n",
        "print(X_train_scaled[\"capital-gain\"].unique()[:10])\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Percentage of Zero Values:\n",
            "capital-gain    92.017563\n",
            "capital-loss    95.227223\n",
            "dtype: float64\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "print(\"Percentage of Zero Values:\")\n",
        "print((X_train[[\"capital-gain\", \"capital-loss\"]] == 0).sum() / len(X_train) * 100)\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "count     1818.000000\n",
            "mean      7609.262926\n",
            "std       4830.251552\n",
            "min        114.000000\n",
            "25%       3411.000000\n",
            "50%       7298.000000\n",
            "75%      14084.000000\n",
            "max      15024.000000\n",
            "Name: capital-gain, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "print(X_train_scaled[X_train_scaled[\"capital-gain\"] > 0][\"capital-gain\"].describe())\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I attempted to apply RobustScaler to the capital-gain and capital-loss features, but due to the high percentage of zero values (92% and 95% respectively), the transformation had little to no effect. Since RobustScaler scales based on the interquartile range (IQR), the large number of zeros caused the scaling to be ineffective. To confirm this, I checked the unique values before and after scaling, and the distribution remained nearly identical. As an alternative, I will use MinMaxScaler which is better suited for handling skewed data with many zero values. Though I decided to proceed with a different scaling approach, I kept the original code in my notebook for reference to demonstrate that this method was tested."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scaled training set preview:\n",
            "            age    fnlwgt  education-num  sex  capital-gain  capital-loss  \\\n",
            "1216   0.693443  0.503319      -0.033365    1      0.511715           0.0   \n",
            "27941 -0.555208  0.481795       1.135080    1      0.000000           0.0   \n",
            "23063  0.840343 -1.391011       2.303525    1      1.000000           0.0   \n",
            "19670 -1.216259 -0.006054      -0.422846    1      0.000000           0.0   \n",
            "19172 -0.775558 -0.787768      -0.422846    0      0.000000           0.0   \n",
            "\n",
            "       income  workclass_Local-gov  workclass_Never-worked  workclass_Private  \\\n",
            "1216        1                False                   False              False   \n",
            "27941       0                False                   False               True   \n",
            "23063       1                False                   False               True   \n",
            "19670       0                False                   False               True   \n",
            "19172       0                False                   False               True   \n",
            "\n",
            "       ...  native-country_Portugal  native-country_Puerto-Rico  \\\n",
            "1216   ...                    False                       False   \n",
            "27941  ...                    False                       False   \n",
            "23063  ...                    False                       False   \n",
            "19670  ...                    False                       False   \n",
            "19172  ...                    False                       False   \n",
            "\n",
            "       native-country_Scotland  native-country_South  native-country_Taiwan  \\\n",
            "1216                     False                 False                  False   \n",
            "27941                    False                 False                  False   \n",
            "23063                    False                 False                  False   \n",
            "19670                    False                 False                  False   \n",
            "19172                    False                 False                  False   \n",
            "\n",
            "       native-country_Thailand  native-country_Trinadad&Tobago  \\\n",
            "1216                     False                           False   \n",
            "27941                    False                           False   \n",
            "23063                    False                           False   \n",
            "19670                    False                           False   \n",
            "19172                    False                           False   \n",
            "\n",
            "       native-country_United-States  native-country_Vietnam  \\\n",
            "1216                           True                   False   \n",
            "27941                          True                   False   \n",
            "23063                          True                   False   \n",
            "19670                          True                   False   \n",
            "19172                          True                   False   \n",
            "\n",
            "       native-country_Yugoslavia  \n",
            "1216                       False  \n",
            "27941                      False  \n",
            "23063                      False  \n",
            "19670                      False  \n",
            "19172                      False  \n",
            "\n",
            "[5 rows x 82 columns]\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "def apply_scaling(X_train, X_val, X_test, standard_columns, minmax_columns):\n",
        "    \"\"\"Apply StandardScaler to some columns and MinMaxScaler to others in train, validation, and test sets.\"\"\"\n",
        "    \n",
        "    #Create copies to avoid modifying the original datasets\n",
        "    X_train_scaled = X_train.copy()\n",
        "    X_val_scaled = X_val.copy()\n",
        "    X_test_scaled = X_test.copy()\n",
        "\n",
        "    #Apply StandardScaler to selected columns\n",
        "    ss = StandardScaler()\n",
        "    X_train_scaled[standard_columns] = ss.fit_transform(X_train[standard_columns])\n",
        "    X_val_scaled[standard_columns] = ss.transform(X_val[standard_columns])\n",
        "    X_test_scaled[standard_columns] = ss.transform(X_test[standard_columns])\n",
        "\n",
        "    #Apply MinMaxScaler to selected columns\n",
        "    mms = MinMaxScaler()\n",
        "    X_train_scaled[minmax_columns] = mms.fit_transform(X_train[minmax_columns])\n",
        "    X_val_scaled[minmax_columns] = mms.transform(X_val[minmax_columns])\n",
        "    X_test_scaled[minmax_columns] = mms.transform(X_test[minmax_columns])\n",
        "\n",
        "    return X_train_scaled, X_val_scaled, X_test_scaled\n",
        "\n",
        "#Define which columns to scale with each method\n",
        "standard_columns = [\"fnlwgt\", \"age\", \"education-num\"]\n",
        "minmax_columns = [\"capital-gain\", \"capital-loss\"]\n",
        "\n",
        "#Apply scaling to training, validation, and test sets\n",
        "X_train_scaled, X_val_scaled, X_test_scaled = apply_scaling(X_train, X_val, X_test, standard_columns, minmax_columns)\n",
        "\n",
        "#Verify transformation\n",
        "print(\"Scaled training set preview:\")\n",
        "print(X_train_scaled.head())\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scaled training set preview:\n",
            "            age    fnlwgt  education-num  sex  capital-gain  capital-loss  \\\n",
            "1216   0.693443  0.503319      -0.033365    1      1.879828     -0.219713   \n",
            "27941 -0.555208  0.481795       1.135080    1     -0.204409     -0.219713   \n",
            "23063  0.840343 -1.391011       2.303525    1      3.868637     -0.219713   \n",
            "19670 -1.216259 -0.006054      -0.422846    1     -0.204409     -0.219713   \n",
            "19172 -0.775558 -0.787768      -0.422846    0     -0.204409     -0.219713   \n",
            "\n",
            "       income  workclass_Local-gov  workclass_Never-worked  workclass_Private  \\\n",
            "1216        1                False                   False              False   \n",
            "27941       0                False                   False               True   \n",
            "23063       1                False                   False               True   \n",
            "19670       0                False                   False               True   \n",
            "19172       0                False                   False               True   \n",
            "\n",
            "       ...  native-country_Puerto-Rico  native-country_Scotland  \\\n",
            "1216   ...                       False                    False   \n",
            "27941  ...                       False                    False   \n",
            "23063  ...                       False                    False   \n",
            "19670  ...                       False                    False   \n",
            "19172  ...                       False                    False   \n",
            "\n",
            "       native-country_South  native-country_Taiwan  native-country_Thailand  \\\n",
            "1216                  False                  False                    False   \n",
            "27941                 False                  False                    False   \n",
            "23063                 False                  False                    False   \n",
            "19670                 False                  False                    False   \n",
            "19172                 False                  False                    False   \n",
            "\n",
            "       native-country_Trinadad&Tobago  native-country_United-States  \\\n",
            "1216                            False                          True   \n",
            "27941                           False                          True   \n",
            "23063                           False                          True   \n",
            "19670                           False                          True   \n",
            "19172                           False                          True   \n",
            "\n",
            "       native-country_Vietnam  native-country_Yugoslavia  experience_years  \n",
            "1216                    False                      False                 0  \n",
            "27941                   False                      False                 0  \n",
            "23063                   False                      False                 0  \n",
            "19670                   False                      False                 0  \n",
            "19172                   False                      False                 0  \n",
            "\n",
            "[5 rows x 83 columns]\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "def apply_scaling(X_train, X_val, X_test, columns):\n",
        "    \"\"\"Apply StandardScaler to selected columns in train, validation, and test sets.\"\"\"\n",
        "    \n",
        "    #Create copies to avoid modifying the original dataset\n",
        "    X_train_scaled = X_train.copy()\n",
        "    X_val_scaled = X_val.copy()\n",
        "    X_test_scaled = X_test.copy()\n",
        "    \n",
        "    #Apply StandardScaler only to specified columns\n",
        "    ss = StandardScaler()\n",
        "    X_train_scaled[columns] = ss.fit_transform(X_train[columns])\n",
        "    X_val_scaled[columns] = ss.transform(X_val[columns])\n",
        "    X_test_scaled[columns] = ss.transform(X_test[columns])\n",
        "    \n",
        "    return X_train_scaled, X_val_scaled, X_test_scaled\n",
        "\n",
        "# Define columns to scale\n",
        "columns_to_scale = [\"fnlwgt\", \"age\", \"education-num\", \"capital-gain\", \"capital-loss\"]\n",
        "\n",
        "# Apply scaling to training, validation, and test sets\n",
        "X_train_scaled, X_val_scaled, X_test_scaled = apply_scaling(X_train, X_val, X_test, columns_to_scale)\n",
        "\n",
        "# Verify transformation\n",
        "print(\"Scaled training set preview:\")\n",
        "print(X_train_scaled.head())\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Features summary: Mean and standard deviation of training data\n",
            "                       Mean  Standard Deviation\n",
            "age           -2.027894e-17            1.000022\n",
            "fnlwgt        -7.019632e-18            1.000022\n",
            "education-num -2.495869e-17            1.000022\n",
            "sex            6.673985e-01            0.471155\n",
            "capital-gain   1.903100e-17            1.000022\n",
            "capital-loss  -3.868597e-17            1.000022\n",
            "income         2.407025e-01            0.427519\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "#Calculate the mean and std for each feature in the training dataset\n",
        "feature_mean = X_train_scaled.mean()\n",
        "feature_std = X_train_scaled.std()\n",
        "\n",
        "#Store the computed values in a DataFrame for easy inspection\n",
        "features_summary = pd.DataFrame({\"Mean\": feature_mean, \"Standard Deviation\": feature_std})\n",
        "\n",
        "# Display the first few rows to verify the standardization process\n",
        "print(\"Features summary: Mean and standard deviation of training data\")\n",
        "print(features_summary.head(7))\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "I tested all the scaling methods mentioned above, but the model metrics were not favorable in any case. However, I previously applied scaling in task 1 and the model performance improved. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model training and experimentation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>MAE</th>\n",
              "      <th>MSE</th>\n",
              "      <th>RMSE</th>\n",
              "      <th>R2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>LinearRegression</th>\n",
              "      <td>0.635098</td>\n",
              "      <td>0.811954</td>\n",
              "      <td>0.901085</td>\n",
              "      <td>0.180752</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SGDRegressor</th>\n",
              "      <td>0.637655</td>\n",
              "      <td>0.813540</td>\n",
              "      <td>0.901965</td>\n",
              "      <td>0.179151</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>HuberRegressor</th>\n",
              "      <td>0.614613</td>\n",
              "      <td>0.828097</td>\n",
              "      <td>0.909998</td>\n",
              "      <td>0.164464</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DecisionTreeRegressor</th>\n",
              "      <td>0.813775</td>\n",
              "      <td>1.427130</td>\n",
              "      <td>1.194626</td>\n",
              "      <td>-0.439951</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>RandomForestRegressor</th>\n",
              "      <td>0.627742</td>\n",
              "      <td>0.792422</td>\n",
              "      <td>0.890181</td>\n",
              "      <td>0.200459</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>RidgeRegression</th>\n",
              "      <td>0.634998</td>\n",
              "      <td>0.811766</td>\n",
              "      <td>0.900981</td>\n",
              "      <td>0.180941</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>LassoRegression</th>\n",
              "      <td>0.613671</td>\n",
              "      <td>0.971518</td>\n",
              "      <td>0.985656</td>\n",
              "      <td>0.019755</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                            MAE       MSE      RMSE        R2\n",
              "LinearRegression       0.635098  0.811954  0.901085  0.180752\n",
              "SGDRegressor           0.637655  0.813540  0.901965  0.179151\n",
              "HuberRegressor         0.614613  0.828097  0.909998  0.164464\n",
              "DecisionTreeRegressor  0.813775  1.427130  1.194626 -0.439951\n",
              "RandomForestRegressor  0.627742  0.792422  0.890181  0.200459\n",
              "RidgeRegression        0.634998  0.811766  0.900981  0.180941\n",
              "LassoRegression        0.613671  0.971518  0.985656  0.019755"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#Define models\n",
        "models = {\n",
        "    \"LinearRegression\": LinearRegression(),\n",
        "    \"SGDRegressor\": SGDRegressor(max_iter=1000, tol=1e-3, random_state=42),\n",
        "    \"HuberRegressor\": HuberRegressor(max_iter=5000),\n",
        "    \"DecisionTreeRegressor\": DecisionTreeRegressor(random_state=42),\n",
        "    \"RandomForestRegressor\": RandomForestRegressor(n_estimators=100, random_state=42),\n",
        "    \"RidgeRegression\": Ridge(alpha=1.0),\n",
        "    \"LassoRegression\": Lasso(alpha=0.1)\n",
        "}\n",
        "\n",
        "#Function to evaluate models\n",
        "\n",
        "def evaluate_model(model, X_train, X_val, y_train, y_val):\n",
        "    model.fit(X_train, y_train)\n",
        "    y_train_pred = model.predict(X_train)\n",
        "    y_val_pred = model.predict(X_val)\n",
        "    \n",
        "    metrics = {\n",
        "        \"MAE\": mean_absolute_error(y_val, y_val_pred),\n",
        "        \"MSE\": mean_squared_error(y_val, y_val_pred),\n",
        "        \"RMSE\": np.sqrt(mean_squared_error(y_val, y_val_pred)),\n",
        "        \"R2\": r2_score(y_val, y_val_pred)\n",
        "    }\n",
        "    return metrics\n",
        "\n",
        "\n",
        "#Store results\n",
        "results = {}\n",
        "\n",
        "#Loop through models and evaluate\n",
        "for name, model in models.items():\n",
        "    results[name] = evaluate_model(model, X_train, X_val, y_train, y_val)\n",
        "\n",
        "#Convert results to DataFrame\n",
        "results_df = pd.DataFrame(results).T\n",
        "\n",
        "#Display results\n",
        "display(results_df)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### LinearRegression vs. SGDRegressor\n",
        "\n",
        "Comparing LinearRegression and SGDRegressor, I observe that LinearRegression slightly outperforms SGDRegressor across all metrics. It has a lower MAE (0.6351 vs. 0.6377), MSE (0.8119 vs. 0.8135), and RMSE (0.9011 vs. 0.9020), indicating marginally better error minimization. Additionally, RÂ² (0.1808 vs. 0.1792) is slightly higher for LinearRegression, meaning it explains the variance in the data a bit better. The differences are minimal, but LinearRegression appears more stable and precise."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hyperparameter tuning for the best-performing model (Random Forest)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
            "Best hyperparameters for Random Forest: {'n_estimators': 200, 'min_samples_split': 5, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': 20}\n",
            "Best MSE from tuning: 0.7253989505781069\n"
          ]
        }
      ],
      "source": [
        "#Define the parameter grid\n",
        "param_grid_RandFor = {\n",
        "    'n_estimators': [50, 100, 200, 500],\n",
        "    'max_depth': [None, 5, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'max_features': ['sqrt', 'log2', None]\n",
        "}\n",
        "\n",
        "#Initialize the model\n",
        "rf_model = RandomForestRegressor(random_state=42)\n",
        "\n",
        "#Perform RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=rf_model,\n",
        "    param_distributions=param_grid_RandFor,\n",
        "    n_iter=20, \n",
        "    scoring='neg_mean_squared_error',  #We use neg_MSE because RandomizedSearchCV maximizes scores\n",
        "    cv=5,\n",
        "    verbose=1,\n",
        "    random_state=42,\n",
        "    n_jobs=-1 \n",
        ")\n",
        "\n",
        "#Fit on the training data (assuming X_train, y_train are defined)\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "#Get best parameters\n",
        "best_params = random_search.best_params_\n",
        "best_mse_rf = -random_search.best_score_\n",
        "\n",
        "print(\"Best hyperparameters for Random Forest:\", best_params)\n",
        "print(\"Best MSE from tuning:\", best_mse_rf)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train the model using the best parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>MAE</th>\n",
              "      <th>MSE</th>\n",
              "      <th>RMSE</th>\n",
              "      <th>R2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>LinearRegression</th>\n",
              "      <td>0.635098</td>\n",
              "      <td>0.811954</td>\n",
              "      <td>0.901085</td>\n",
              "      <td>0.180752</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SGDRegressor</th>\n",
              "      <td>0.637655</td>\n",
              "      <td>0.813540</td>\n",
              "      <td>0.901965</td>\n",
              "      <td>0.179151</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>HuberRegressor</th>\n",
              "      <td>0.614613</td>\n",
              "      <td>0.828097</td>\n",
              "      <td>0.909998</td>\n",
              "      <td>0.164464</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DecisionTreeRegressor</th>\n",
              "      <td>0.813775</td>\n",
              "      <td>1.427130</td>\n",
              "      <td>1.194626</td>\n",
              "      <td>-0.439951</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>RandomForestRegressor</th>\n",
              "      <td>0.627742</td>\n",
              "      <td>0.792422</td>\n",
              "      <td>0.890181</td>\n",
              "      <td>0.200459</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>RidgeRegression</th>\n",
              "      <td>0.634998</td>\n",
              "      <td>0.811766</td>\n",
              "      <td>0.900981</td>\n",
              "      <td>0.180941</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>LassoRegression</th>\n",
              "      <td>0.613671</td>\n",
              "      <td>0.971518</td>\n",
              "      <td>0.985656</td>\n",
              "      <td>0.019755</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Tuned RandomForestRegressor</th>\n",
              "      <td>0.590542</td>\n",
              "      <td>0.736844</td>\n",
              "      <td>0.858396</td>\n",
              "      <td>0.256536</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                  MAE       MSE      RMSE        R2\n",
              "LinearRegression             0.635098  0.811954  0.901085  0.180752\n",
              "SGDRegressor                 0.637655  0.813540  0.901965  0.179151\n",
              "HuberRegressor               0.614613  0.828097  0.909998  0.164464\n",
              "DecisionTreeRegressor        0.813775  1.427130  1.194626 -0.439951\n",
              "RandomForestRegressor        0.627742  0.792422  0.890181  0.200459\n",
              "RidgeRegression              0.634998  0.811766  0.900981  0.180941\n",
              "LassoRegression              0.613671  0.971518  0.985656  0.019755\n",
              "Tuned RandomForestRegressor  0.590542  0.736844  0.858396  0.256536"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#Train the Random Forest model using the best hyperparameters from RandomizedSearchCV\n",
        "best_rf_model = RandomForestRegressor(**best_params, random_state=42)\n",
        "best_rf_model.fit(X_train, y_train)\n",
        "\n",
        "#Predict on validation set\n",
        "y_val_pred = best_rf_model.predict(X_val)\n",
        "\n",
        "#Compute final evaluation metrics\n",
        "best_rf_mae = mean_absolute_error(y_val, y_val_pred)\n",
        "best_rf_mse = mean_squared_error(y_val, y_val_pred)\n",
        "best_rf_rmse = np.sqrt(best_rf_mse)\n",
        "best_rf_r2 = r2_score(y_val, y_val_pred)\n",
        "\n",
        "results[\"Tuned RandomForestRegressor\"] = {\n",
        "    \"MAE\": best_rf_mae,\n",
        "    \"MSE\": best_rf_mse,\n",
        "    \"RMSE\": best_rf_rmse,\n",
        "    \"R2\": best_rf_r2\n",
        "}\n",
        "\n",
        "#Convert results to DataFrame\n",
        "results_df = pd.DataFrame(results).T\n",
        "\n",
        "#Display results\n",
        "display(results_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature importance for the tuned RandomForestRegressor model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Feature importance - tuned Random Forest:\n",
            "                              Feature  Importance\n",
            "0                                 age    0.279388\n",
            "1                              fnlwgt    0.088337\n",
            "2                       education-num    0.075280\n",
            "3                                 sex    0.066373\n",
            "6                              income    0.058256\n",
            "35             relationship_Own-child    0.055334\n",
            "15  marital-status_Married-civ-spouse    0.031771\n",
            "26           occupation_Other-service    0.026168\n",
            "17       marital-status_Never-married    0.024795\n",
            "22         occupation_Exec-managerial    0.022295\n",
            "10             workclass_Self-emp-inc    0.021549\n",
            "4                        capital-gain    0.019543\n",
            "33         relationship_Not-in-family    0.017769\n",
            "28          occupation_Prof-specialty    0.017459\n",
            "23         occupation_Farming-fishing    0.017070\n"
          ]
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABCMAAAIjCAYAAAA5n9HyAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAApFtJREFUeJzt3QmcleP///FP+76XNintizYlWiiKRGRJhFYqRJK0oE1apM2uLGWJ7CFkSZtKSIX2VSFK+6L9/B/v6/u7z/8+pzPTmWnmzNLr+XgczdznPvd93cuMuT735/pcGQKBQMAAAAAAAABiJGOsdgQAAAAAACAEIwAAAAAAQEwRjAAAAAAAADFFMAIAAAAAAMQUwQgAAAAAABBTBCMAAAAAAEBMEYwAAAAAAAAxRTACAAAAAADEFMEIAAAAAAAQUwQjAAA4w02ePNkyZMhgmzZtSumm4P+sXbvWrrjiCsuXL5+7NtOmTUvpJqUagwcPduckrSpTpox17NgxpZuBGNHvVd2v+j2bULNnz3af1b9AekQwAgBwxna+I7369euXLPtcsGCB60Tt3r07WbZ/Jjt48KA7t+npD/YOHTrYr7/+asOGDbM33njD6tatm2z7+uuvv9z5W7p0qaUn6vD7f7azZctmFStWtIEDB9qhQ4dSunmp9jz5XzNmzLDUJr3er8CZKHNKNwAAgJTy2GOP2bnnnhuy7Lzzzku2YMSQIUPcH/758+e31KRdu3Z2yy23uM5aWg1G6NxKkyZNLK3777//bOHChfbII4/YvffeG5POnc6fntjXqlXL0hPd0y+//LL7es+ePfbxxx/b0KFDbf369TZlypSUbl6qPE9+NWvWtNQmPd+vwJmGYAQA4IzVokWLZH3iHAsHDhywXLlyndY2MmXK5F5pzYkTJ+zIkSOW3mzfvt39m5RBq6S4T9KizJkz2+233x78/p577rEGDRrY22+/bWPHjrWiRYumaPtS63lK6mBhzpw5k2XbANI2hmkAABCHL774wi6++GLXicuTJ49dffXVtnz58pB1fvnlF5ftULZsWcuePbsVK1bMOnfubDt27Aiuo5Tihx56yH2tTAwvBVpjieMbT6zl+qx/O1q2YsUKu/XWW61AgQLWqFGj4Ptvvvmm1alTx3LkyGEFCxZ02Q5btmxJVM0IPXVs2bKlG/qggI22Wb169eBQiA8//NB9r2PWPpcsWRKyTZ2T3Llz24YNG6x58+buHJYoUcJlowQCgZM6yg8++KCVKlXKPaGtVKmSjR49+qT11EZlCuiJdrVq1dy6L774ohUpUsS9r6el3rn1zls018d/btetWxfMXlG9hk6dOrnOVDid63r16rlOlq7DJZdcYl999VWC759wakfp0qXd17pn1CZdC4/Os4JoefPmdee3adOm9v3330e8nnPmzHGd77POOsvOPvvsiPvT9bzgggvc1zpW7/x592Nc9Q2UgeLPQvHGtr/77rtuaIn2p/Ot9umchlu0aJFdeeWV7hzrHDZu3Njmz59/0nrfffeda5+2Va5cOZswYYKdDrVRPzO6t3Rven7//Xd3rnTv6V4vVKiQ3XTTTSfVUfHOrdraq1cvd+/p+l5//fXBIJJH+3j88cfdudAxXnrppXFef7VF+9PPrda96KKL7LPPPgtZx3+Oda+XLFnS3VetW7d2WR+HDx+2nj17uuute0PXU8uSyvPPPx/8udPPcvfu3U8adqZ7Qtllixcvdj8TOpaHH37Yvae2DBo0yMqXL++2oZ/3Pn36nNTGr7/+2l0j/QzqOHRNvG2c6n6NxPvZXrNmjQu46J7TdRswYIC7Rvod2apVK/czpd8PY8aMOWkb27ZtszvuuMMFr3QvKmPktddeO2k9nQ/9vGgfar+GW8U1NG/VqlXu2umaa5v6PfvJJ59EdS2A9ILMCADAGUt/wP/7778hywoXLuz+1Th9/SGpjvQTTzzhOqQvvPCC+yNZHUKvg6g/nNWR0B/G+kNWnY2JEye6f9VJ1B/BN9xwg/tDWE9jx40bF9yH/iAO78BEQ52WChUq2PDhw4MddnUA9cd1mzZt7M4773TbfeaZZ1yHQO1NzFN2dSIV9OjWrZv7I14BgmuuucYFANQ5UOdNRowY4fa7evVqy5jx/z/nOH78uOtwqmM1atQoN/5cnZFjx465oISo/ddee63NmjXL/bGvtOsvv/zSdcT//PNPd778vv32W9cZU1BC51GdAl2Xu+++23UIda6lRo0aUV8fPx2HAkY6pp9//tmlrqtzp3vAo46gOjh6wq7jyJo1q+tcq20qOpmQ+yec2q9r9cADD1jbtm3tqquuch0yUZsV3FCnSZ24LFmyuM65OoAKPFx44YUh29L10T2mGgkK+ERSpUoVdwxap2vXrm77omNLjJEjR7p7oHfv3u7nS9f9tttuc+fHfw0VUFEQS/eD1p80aZJddtllNm/ePBfkEdXM0PnUMeh8677R+qebzeAFGBRE8vz4449uKJUCeAoeaB1dL51bBf/Cn+zfd9997vNqj9YdP368uyffeeed4Do6pwpG6BrqpftJxxOezfPPP/+48617pEePHi4Qoo6ufi7ef/99d1/76d5UwET1bfQzqp9z3Qs6j7t27XLnSve2Oui6l9WOaIT/LtQ21akWbVP3fbNmzdzPmn7WdX503hSY0boeBfp0fXUu9XtD10tZTDoeBZd0n+m+0/XVz7d+N3oFWnWPKwiqn1/dlwpa6Bi9QNXp3K8333yz+7zuUQV6dG0UCNDPkO49/Zwq0Kl7VwEP/e70hk3pPlA7dI11Tt977z0XdFCg4f777w/+LlNQQ8d41113uX199NFH7vdAOB1nw4YNXUBJ11EBLf1eu+666+yDDz446ZoD6VYAAIAzzKRJk9SDj/iSffv2BfLnzx/o0qVLyOf+/vvvQL58+UKWHzx48KTtv/32225bc+fODS578skn3bKNGzeGrKvvtVxtCqflgwYNCn6vr7Wsbdu2Iett2rQpkClTpsCwYcNClv/666+BzJkzn7Q8rvPhb1vp0qXdsgULFgSXffnll25Zjhw5Ar///ntw+YQJE9zyWbNmBZd16NDBLbvvvvuCy06cOBG4+uqrA1mzZg1s377dLZs2bZpb7/HHHw9pU+vWrQMZMmQIrFu3LuR8ZMyYMbB8+fKQdbWt8HOV0OvjndvOnTuHrHv99dcHChUqFPx+7dq1rg1afvz48ZB1dXwJvX8i8e4J3TN+1113nTt369evDy7766+/Anny5AlccsklJ13PRo0aBY4dOxY4lR9//DHOe1D3ga5luMaNG7uXR9de26hSpUrg8OHDweVPPfWUW6570TtHFSpUCDRv3jx4vrzrdO655wYuv/zykOPNnj17yL22YsUKd69H8yes2p0rVy53f+ile2n06NHuvjrvvPNO2n+4hQsXuv28/vrrJ53bZs2ahXz+gQcecO3avXu3+37btm3uWul+96/38MMPu8/7z2nPnj3dsnnz5gWX6R7S+ShTpkzwPvPOsdp+5MiR4Lr6faBjatGiRUj769ev765fNOcp0u9C7/p6x3LFFVeE3PPPPvusW+/VV18NLtNntOzFF18M2ccbb7zhfm78xyhaT+vPnz/ffT9u3Dj3vff7IaH3ayTez3bXrl2Dy/RzcfbZZ7vzNnLkyODyXbt2ud9v/uszfvx49/k333wzuEznX+c3d+7cgb1794b8Lhs1alTIfi6++OKT2tu0adNA9erVA4cOHQou033SoEED9/Ph8a65/3crkJ4wTAMAcMZ67rnn3JNz/0v0r5546cm0nhZ6L9VV0NNnPcX36AmlRxX6tZ4yAURPQpODnrr5aciEnjzqqb6/vcoEUAaFv70JUbVqVatfv37we+/Ju54innPOOSct96e9e/wFGL1hFnoy/M0337hln3/+uTuveiLsp2Ebij9oqIOf0vnVrmgl9PqEn1s9edWT3r1797rv9QRX51pPZv1ZIN7xJfT+iZayTDQMRE9ONeTEU7x4cZe9oqexXhs9Xbp0iXktEGWgKFPE4z259u4NzYCgaUvVZp1X79woc0NDOubOnevOr45XGTI6Xv+9pqfNyjaJlrarzAq9NDxAT731RFqFLP1ZMf775OjRo65tWl9ZKpHuEz2V939ex6k2a7iH6P7Wfa4MCv96GkYRTj8DygbxD7lSNoz2oawLZWb4tW/fPiQTQfeUflY0/MhPyzUEQRklp6JhAuG/C73hCt6xqO3+e173l7J0woeTKJtB94GfMgl07SpXrhzyM6HfJeL9THgZXLo+ug+SkjLGPPq50LAInTdlZHm0fw0L8f8u0/XR71L9PHt0/vU7a//+/S4ryVtPtTeUOeLfj+4Bv507d7rsIP2+3rdvX/Bc6J7Tva2fD2WFAWcChmkAAM5Y6gBEKmCpPwbF+0M5nP4A9/9hqfTlqVOnunHFfkpTTw7hM4CovfqjWoGHSPwdl4TwdwLFS9nWWO9Iy5Ui7qeOi7/jLJpa0Z8qr86bxp9r7LufOi7e+/Ed+6kk9PqEH7OXyq9j03XXLAw6rvgCIgm5f6KlYTdK41dHKZzOlTpu6nhqTH9iz1VSiO/8+c9NpNR1j1f/QOnxke5pnQN1/KKhTvann37qvv7jjz/csBHdB/7gg2hfGv6g4SLqCPrrlST0PvHft+HtV1DEPzzEWzd8iE34z4B/lp+E/FzqvlD7NfQjPuo0awhGJN6xhN97Cjrp5zv8Z1RDD/wBKe+6r1y5MljfJZz3s6mhFBoapcCBhi8oQKWhS6qtEB78S6hI5033hzdszr/cX1NGx6frGL7/8N9R+lfBQW9YlSf8vGm4h+4vDavTK67zofMIpHcEIwAACOM9kdO4fz0RC6enXx493dJYc9U4UL0D/SGqz6tWQjRP9sJrFnj0lDUu4R0p7UfbURZBpCfh4X8cRyuup+pxLQ8vOJkcwo/9VBJ6fZLi2BJy/6Smc5XQ+zPSuTrV+fPOzZNPPhnntIy6RklVeDG8k60nz3o6rzoo/mKBenqtQISe/isbSB1SHbvqHiTXfZJYqfHn8lT3nc6hCt5qBpNIvECKPqvsGGVKKONCdWZUh0OBPWUGnU6mT2Lu1+Tg3U/K0okry0dZOcCZgGAEAABhVLVfVLgwrqeF3lPQmTNnuifv/iJx3tPfaDp13lPS8Irr4U8bT9Ve/fGsJ+Fe5kFqoD+6le7sb5OK1YlXwFEzRygNXOnK/uwIVZr33j+VuM5tQq5PQs61jkup83F1pqO9fxJCT5RVRFGFA8PpXOmpbfiT8WjFdf68+zPSbAC6P8OzXqLhnRtlh8R3bnS86phGulaRzkG09ORaxUF1T6jIozdkR4Uila3hn0lBw3rimgnhVLz7Vu33nydluIRnEGnduK6rf1spxdu/2ug/Fg3d2LhxY1T3uK77smXLXKZDfPeb6F7WenopeKFCvY888ogLUGhfp/p8chy/ZuXRz70/OyL8+uhf/b7R0A1/ADj82nrnUBlrSfX7AUirqBkBAEAYPa1SZ0l/BGv8eDhvBgzvqVr4UzRV1g+naukS3rnRfpQmrKeB4dPoRUtpzGqLOljhbdH34dNYxtKzzz4b0hZ9rz/C1dEQzTKgp+z+9URV9tXpUFX+U/FmOgg/twm5PtFSDQN1SFTRP/yJubefaO+fhNCxaCYGjaX3TzepmRjeeustV28gMcM/4rs3vU6kOu3+GSCmT58e1ZSxkWgGDW1TM7Oo0xbfz5bOo2p0bN68Ofi+Uv1VS+J0KAtC94xmVfBof+H3iWapiC9DKT7qZOo+1zb824107+ln4IcffrCFCxeG1LrQrC8K2iWkRkpy0LFo2MXTTz8dciyvvPKKGwKiKWujyVDS8JeXXnrppPc0RMab7UXDqsJ5QT8vWya++zU56Pr8/fffITOlqA6Hrq2CDqpj462n5ZplxKP7R+v5KUip2Tk0i8fWrVuT5PcDkFaRGQEAQBh16vQHZbt27ez88893qdp6UqtOkVKHVQBPnWetp+nfNA5dnU6N8VUqsZ4WRuqEiZ7waXvqqGiaTP1hrfHR6hjpX9WwUGDCyyCIhjp3mqauf//+rqOqDrOyDNQOTS2nQnhKCY41jcdWmrWeOGtMvIaR6PxpWlBv7LjOwaWXXurOi9quqTp1DtXpVsq89yQ9PnqCrg6bOgvKwtB0fRpjr1e01ydaSp9WW4cOHeqKFioQpIJ9muJQtS9UdyDa+yehdI1VWFCBB03bqeEe6tCok6ZjTCydYxXu05Stum90T+p6KdNG96SyBjSsRR1K1cx48803o7oukSiQo5oACjKpvoUKHeq6qKOqJ986d16NBwXXdP/oPOt4vQ6gPqcn1Yml+gnarwJ+Cm5o7L+mk9SwGg3P0L2kwIAydk5VayEuut76mdP9oG2ro6opXfUzEF6jQLURNO2vzomKIur+1dSeuk81zePp1ko4XToW/W7R9dB9oCk69bRf509TYGr6zlPRz4KmrlSBWF1n/Qyoo67sAi1XgEm/+xTk0+8/BTiUaaDaCdqPplv1CnzGd78mB/3+1M+ZpvJcvHixCxDpZ0LTjSq45GV06XeZjkvXU7/LdB+puHCkmiMqnqzj0dAVFQJVtoQCi7rvVNtEWSTAGSGlp/MAACDWvOn5NEVcfDSdmqYg1HSMmmKwXLlygY4dOwZ++umn4Dp//PGHm+ZRUzlqvZtuuslNtxhpqsmhQ4cGSpYs6aa480+lqWkF77jjDvd5TdPYpk0bN51eXFN7xjXt3QcffOCmc9R0hnpVrlw50L1798Dq1asTNbWnpiUMp/W0zVNNRelNq6hpKDUlYM6cOQNFixZ1xxA+JaamMdTUiCVKlAhkyZLFTW2nbfmnRIxr3x5NQVqnTh03BaH/vEV7feI6t5HOjWg6w9q1aweyZcsWKFCggJvS8Ouvv07w/ZOQqT3l559/dtvUlII6p5deemnI9KsJub/9Pv7440DVqlXdVLDh0xCOGTPG3bc61oYNG7r2xzW153vvvRfxWMKnYVyyZEnghhtucNOmaru633Tfz5w5M2S9OXPmBK9r2bJl3VSQ3rU6Fe8ejET3pabi9KZw1JSOnTp1ChQuXNidW53jVatWnTS1aVznNtIUjLrPhwwZEihevLibLrJJkyaB3377LeJ0qWqPprPVfap7pV69eoHp06dH3Ef4OY6rTaf6fRHNefLTVJ76naKfUf0s33333e68+emeqFatWsTPazrMJ554wr3v/dzo2uoc7dmzx62j69+qVSv3u0DXXP9q6tI1a9ZEfb+Gi+s8xHXckY7hn3/+Cd4fapem5Yy0zx07dgTatWsXyJs3r/u519e61yO1Ude8ffv2gWLFirlzqp+xli1bBt5///3gOkztifQug/6T0gERAACQvugpop4eRkrFBwAAoGYEAAAAAACIKYIRAAAAAAAgpghGAAAAAACAmKJmBAAAAAAAiCkyIwAAAAAAQEwRjAAAAAAAADGVOba7A5AenThxwv766y/LkyePZciQIaWbAwAAACCFqBLEvn37rESJEpYxY9z5DwQjAJw2BSJKlSqV0s0AAAAAkEps2bLFzj777DjfJxgB4LQpI8L7hZM3b96Ubg4AAACAFLJ37173oNLrI8SFYASA0+YNzVAggmAEAAAAgAynGL5NAUsAAAAAABBTBCMAAAAAAEBMEYwAAAAAAAAxRTACAAAAAADEFMEIAAAAAAAQUwQjAAAAAABATBGMAAAAAAAAMUUwAgAAAAAAxBTBCAAAAAAAEFMEIwAAAAAAQEwRjAAAAAAAADFFMAIAAAAAAMQUwQgAAAAAABBTBCMAAAAAAEBMEYwAAAAAAAAxRTACAAAAAADEFMEIAAAAAAAQUwQjAAAAAABATGWO7e4ApGdjl+2w7LmPpHQzAAAAgDNGv9qFLS0iMwIAAAAAAMQUwQgAAAAAABBTBCMAAAAAAEBMEYwAAAAAAAAxRTACAAAAAADEFMEIAAAAAAAQUwQjAAAAAABATBGMAAAAAAAAMUUwAkjjZsyYYY0aNbL8+fNboUKFrGXLlrZ+/frg+wsWLLBatWpZ9uzZrW7dujZt2jTLkCGDLV26NLjOb7/9Zi1atLDcuXNb0aJFrV27dvbvv/+m0BEBAAAASO8IRgBp3IEDB6xXr172008/2cyZMy1jxox2/fXX24kTJ2zv3r12zTXXWPXq1e3nn3+2oUOHWt++fUM+v3v3brvsssusdu3abhsKbvzzzz/Wpk2bOPd5+PBht23/CwAAAACilTnqNQGkSjfeeGPI96+++qoVKVLEVqxYYd99953LgnjppZdcZkTVqlXtzz//tC5dugTXf/bZZ10gYvjw4SHbKFWqlK1Zs8YqVqx40j5HjBhhQ4YMSeYjAwAAAJBekRkBpHFr1661tm3bWtmyZS1v3rxWpkwZt3zz5s22evVqq1GjhgtEeOrVqxfy+WXLltmsWbPcEA3vVblyZfeef7iHX//+/W3Pnj3B15YtW5L1GAEAAACkL2RGAGmchmGULl3aZT+UKFHCDc8477zz7MiRI1F9fv/+/W4bTzzxxEnvFS9ePOJnsmXL5l4AAAAAkBgEI4A0bMeOHS77QYGIiy++2C3T0AxPpUqV7M0333Q1HrzgwY8//hiyjfPPP98++OADl1GROTO/EgAAAAAkP4ZpAGlYgQIF3AwaEydOtHXr1tm3337rill6br31Vpcp0bVrV1u5cqV9+eWXNnr0aPeeaklI9+7dbefOnW6ohwIVGpqh9Tp16mTHjx9PsWMDAAAAkH4RjADSMM2cMXXqVFu8eLEbmvHAAw/Yk08+GXxfNSQ+/fRTN42npvd85JFHbODAge49r46EhnbMnz/fBR6uuOIKN/NGz5493VSh2j4AAAAAJLUMgUAgkORbBZBqTZkyxWU9qPBkjhw5kmSbmtozX758NmjuBsueO0+SbBMAAADAqfWrXdhSE69voP6GHo7GhQHiQDr3+uuvu5k2SpYs6WbO6Nu3r7Vp0ybJAhEAAAAAkFAEI4B07u+//3ZDM/SvZse46aabbNiwYSndLAAAAABnMIIRQDrXp08f9wIAAACA1ILqdAAAAAAAIKbIjACQZHrVLBRvkRoAAAAAEDIjAAAAAABATBGMAAAAAAAAMUUwAgAAAAAAxBTBCAAAAAAAEFMEIwAAAAAAQEwxmwaAJDN22Q7LnvtISjcDQAT9ahdO6SYAAAAEkRkBAAAAAABiimAEAAAAAACIKYIRAAAAAAAgpghGAAAAAACAmCIYAQAAAAAAYopgBAAAAAAAiCmCEUAKCQQC1rVrVytYsKBlyJDBli5desrPaL1p06bFpH0AAAAAkFwIRgApZMaMGTZ58mSbPn26bd261c4777yUbpKVKVPGxo8fn9LNAAAAAJDOZU7pBgBnqvXr11vx4sWtQYMGKd0UAAAAAIgpMiOAFNCxY0e77777bPPmzW7ohTISmjRpYj169LA+ffq4oRvFihWzwYMHx7mN1q1b27333hv8vmfPnm5bq1atct8fOXLEcuXKZd988437ft++fXbbbbe5ZQqCjBs3zu1TnxN9/fvvv9sDDzzgtqNXXA4fPmx79+4NeQEAAABAtAhGACngqaeesscee8zOPvtsN0Tjxx9/dMtfe+01FyxYtGiRjRo1yq3z9ddfR9xG48aNbfbs2cHv58yZY4ULFw4u0zaPHj0azLzo1auXzZ8/3z755BO3zXnz5tnPP/8c/PyHH37o2qN9qk16xWXEiBGWL1++4KtUqVJJdm4AAAAApH8EI4AUoA58njx5LFOmTC4DokiRIm55jRo1bNCgQVahQgVr37691a1b12bOnBlxG8pkWLFihW3fvt127drlvr7//vuDwQj9e8EFF1jOnDldVoQCHaNHj7amTZu6+hSTJk2y48ePB7enbAy1R+1Sm/SKS//+/W3Pnj3B15YtW5L8HAEAAABIv6gZAaQiCkb4aTjFtm3bIq6rgIICCMqIyJo1q9WuXdtatmxpzz33nHtfyxWwkA0bNrgsiXr16oUERCpVqpSodmbLls29AAAAACAxCEYAqUiWLFlCvlfdhhMnTkRcV+9dcsklLgNCgQEFHhTMUD2H3377zRYsWGC9e/eOUcsBAAAAIHoM0wDSMK9uhF4KRmTMmNEFKJ588kkXlGjYsKFbr2zZsi7Q4dWmEA2vWLNmTcj2lGHhH7oBAAAAAMmBYASQhnl1I5YvX26NGjUKLpsyZYqrN6FimKI6EB06dLCHHnrIZs2a5da/4447XPDCP2uGZvWYO3eu/fnnn/bvv/+m2HEBAAAASN8IRgBpWPXq1S1//vxWq1Yty507dzAYoewGr16EZ+zYsVa/fn1XV6JZs2Yua6JKlSqWPXv24DqaSWPTpk1Wrly5YFFNAAAAAEhqGQKBQCDJtwog1Ttw4ICVLFnSxowZ47IkTsfevXtdQcxBczdY9tx5kqyNAJJOv9qFU7oJAADgDLD3//oGGhaeN2/eONejgCVwhliyZImtWrXKzaihXwzKgpBWrVqldNMAAAAAnGEIRgBnkNGjR9vq1atdoco6derYvHnzrHBhnpYCAAAAiC2CEcAZonbt2rZ48eKUbgYAAAAAUMASAAAAAADEFpkRAJJMr5qF4i1SAwAAAABCZgQAAAAAAIgpghEAAAAAACCmCEYAAAAAAICYIhgBAAAAAABiigKWAJLM2GU7LHvuIyndDCBV6Ve7cEo3AQAAINUhMwIAAAAAAMQUwQgAAAAAABBTBCMAAAAAAEBMEYwAAAAAAAAxRTACAAAAAADEFMEIAAAAAAAQUwQjkCbNnj3bMmTIYLt3707ppliZMmVs/PjxKd0MAAAAAEgzCEYAUZo8ebLlz5//pOU//vijde3aNUXaBAAAAABpUeaUbgCQ1hUpUiSlmwAAAAAAaQqZEUgVTpw4YSNGjLBzzz3XcuTIYTVr1rT3338/+P7nn39uFStWdO9deumltmnTppDPDx482GrVqhWyTEMnNITC79VXX7Vq1apZtmzZrHjx4nbvvfcG3xs7dqxVr17dcuXKZaVKlbJ77rnH9u/fHxwW0qlTJ9uzZ48bHqKX9hlpmMbmzZutVatWljt3bsubN6+1adPG/vnnn5Pa+sYbb7jP5suXz2655Rbbt29fvOdI6w4fPtw6d+5sefLksXPOOccmTpwY79CVpUuXumXe+fKyO6ZPn26VKlWynDlzWuvWre3gwYP22muvuX0UKFDAevToYcePHz/ldQMAAACAxCAYgVRBgYjXX3/dXnzxRVu+fLk98MADdvvtt9ucOXNsy5YtdsMNN9g111zjOtd33nmn9evXL8H7eOGFF6x79+5uSMWvv/5qn3zyiZUvXz74fsaMGe3pp592+1fH/Ntvv7U+ffq49xo0aOACDgoubN261b169+4dMaiiQMTOnTtd27/++mvbsGGD3XzzzSHrrV+/3qZNm+aCAnpp3ZEjR57yGMaMGWN169a1JUuWuGDJ3XffbatXr07QeVDgQcc5depUmzFjhgtiXH/99S7go5eCJBMmTAgJBoU7fPiw7d27N+QFAAAAANFimAZSnDq2euL/zTffWP369d2ysmXL2nfffec6xXpaX65cOdcRFz3RVzDhiSeeSNB+Hn/8cXvwwQft/vvvDy674IILgl/37Nkz+LX2qfXvuusue/755y1r1qwug0FZBsWKFYtzHzNnznRt27hxo8uuEAVZlI2h2hLe/hS0UJaCMhykXbt27rPDhg2L9xiuuuoqF4SQvn372rhx42zWrFnunETr6NGjLjCjcyrKjFAAQtkbyuaoWrWqyz7RdsODKP7g0ZAhQ6LeJwAAAAD4kRmBFLdu3Tr3tP7yyy93nWHvpU68MghWrlxpF154YchnvKBFtLZt22Z//fWXNW3aNM51FAzR+yVLlnRBAgUIduzY4doWLbVVQQgvECHq3GtohN7zBzu8QIRoyIjaKFOmTAk5D/PmzQuuV6NGjeDXXmDE+1y0NDTDC0RI0aJFXXu0L/+y+Lbbv39/N2TFeyl7BQAAAACiRWYEUpxXl+Gzzz5zgQA/1XZQ/YJT0RCLQCBwUgaAR7Um4qOaCi1btnTDHpSdULBgQZeZcccdd9iRI0dcBz4pZcmSJeR7BRaULSHXXnttSPDFf07i+5zOgfjPg/8cxLeN+LYbia6LXgAAAACQGAQjkOKUOaCOrQo/Nm7c+KT3q1Sp4uo7+H3//fcnzWjx999/u464OtKi+hIeZSHo6b+GQmgIQrjFixe7zreGgnid+nfffTdkHQ3VOFVRR7VVWQJ6edkRK1ascEUldZzRUFv9WRMJndVD9SxUhDL8HAAAAABAakEwAilOHW8Vg1TRSgUEGjVq5FL/58+f7wpGqm6DggQPPfSQK16pwIHqLfg1adLEtm/fbqNGjXI1EFSY8YsvvnCf989ioW2dddZZ1qJFCzd7hfZx3333uUKWyiJ45plnXKFMLVcxTT8FM5TFoYCGZvtQtkR4xkSzZs3cjBy33XabK3h57NgxV+NBQRYVnkxOOgYFQHScyu5Ys2ZNsM4GAAAAAKQm1IxAqjB06FAbMGCAK4yo7IIrr7zSDdvQVJ+awvKDDz5ws08oCKAggQpe+ukzKjT53HPPuXV++OGHk2a76NChgwsQaD0VlNSwjLVr17r39BlN7amimOedd56r26C2+GlGDQUzVNRRWQgKfIRTVsbHH3/sMhMuueQSF5xQMc533nnHkpuGWrz99tu2atUqV1tCx6IinAAAAACQ2mQIhA+0B4AE0tSemm1k0NwNlj13woeYAOlZv9qFU7oJAAAAMe8bKNvdn6kejswIAAAAAAAQUwQjAAAAAABATBGMAAAAAAAAMUUwAgAAAAAAxBRTewJIMr1qFoq3SA0AAAAACJkRAAAAAAAgpghGAAAAAACAmCIYAQAAAAAAYopgBAAAAAAAiCmCEQAAAAAAIKaYTQNAkhm7bIdlz30kpZsBJEi/2oVTugkAAABnHDIjAAAAAABATBGMAAAAAAAAMUUwAgAAAAAAxBTBCAAAAAAAEFMEIwAAAAAAQEwRjAAAAAAAADFFMAIAAAAAAMQUwQgAAAAAABBTBCOAdOL999+36tWrW44cOaxQoULWrFkzO3DggHvv5ZdftipVqlj27NmtcuXK9vzzzwc/17lzZ6tRo4YdPnzYfX/kyBGrXbu2tW/fPsWOBQAAAED6RjACSAe2bt1qbdu2dYGFlStX2uzZs+2GG26wQCBgU6ZMsYEDB9qwYcPce8OHD7cBAwbYa6+95j779NNPu6BFv3793PePPPKI7d6925599tk496fAxd69e0NeAAAAABCtzFGvCSBVByOOHTvmAhClS5d2y5QlIYMGDbIxY8a49+Tcc8+1FStW2IQJE6xDhw6WO3due/PNN61x48aWJ08eGz9+vM2aNcvy5s0b5/5GjBhhQ4YMidHRAQAAAEhvMgT06BRAmnb8+HFr3ry5/fDDD+7fK664wlq3bm1Zs2Z1wQYN3ciY8f8nQilwkS9fPvvnn3+Cyx5++GEXZOjbt6+NHDky3v0pM8Ib1iHKjChVqpQNmrvBsufOk0xHCSSPfrULp3QTAAAA0g31DdTX2LNnT7wPOMmMANKBTJky2ddff20LFiywr776yp555hk33OLTTz9177/00kt24YUXnvQZz4kTJ2z+/Plu2bp16065v2zZsrkXAAAAACQGNSOAdCJDhgzWsGFDN3xiyZIlLitCAYYSJUrYhg0brHz58iEvDdfwPPnkk7Zq1SqbM2eOzZgxwyZNmpSixwIAAAAgfSMzAkgHFi1aZDNnznTDM8466yz3/fbt290MGgpO9OjRw6VKXXnllW54xU8//WS7du2yXr16ucCFClxqNg4FM8aOHWv333+/qyFRtmzZlD40AAAAAOkQwQggHdBYrLlz57rikxqjpSKWKlrZokUL937OnDld9sNDDz1kuXLlcsUte/bsaYcOHbLbb7/dOnbsaNdcc41bt2vXrvbZZ59Zu3bt3Db9wzkAAAAAIClQwBJAkhWpoYAl0iIKWAIAAMS+gCU1IwAAAAAAQEwRjAAAAAAAADFFMAIAAAAAAMQUwQgAAAAAABBTBCMAAAAAAEBMMbUngCTTq2aheCvmAgAAAICQGQEAAAAAAGKKYAQAAAAAAIgpghEAAAAAACCmCEYAAAAAAICYooAlgCQzdtkOy577SEo3A+lYv9qFU7oJAAAASAJkRgAAAAAAgJgiGAEAAAAAAGKKYAQAAAAAAIgpghEAAAAAACCmCEYAAAAAAICYIhgBAAAAAABiimAEkEyaNGliPXv2TOlmAAAAAECqkzmlGwCkVx9++KFlyZIlpZsBAAAAAKkOwQggmRQsWDClmwAAAAAAqRLDNIAYDNMoU6aMDR8+3Dp37mx58uSxc845xyZOnBiy/h9//GFt27Z1QYxcuXJZ3bp1bdGiRcH3X3jhBStXrpxlzZrVKlWqZG+88UbI5zNkyGATJkywli1bWs6cOa1KlSq2cOFCW7dunWuLttmgQQNbv359yOc+/vhjO//88y179uxWtmxZGzJkiB07dixZzw0AAACAMxvBCCBGxowZ4wIMS5YssXvuucfuvvtuW716tXtv//791rhxY/vzzz/tk08+sWXLllmfPn3sxIkT7v2PPvrI7r//fnvwwQftt99+s27dulmnTp1s1qxZIfsYOnSotW/f3pYuXWqVK1e2W2+91a3bv39/++mnnywQCNi9994bXH/evHlufW17xYoVLpgxefJkGzZsWLzHcvjwYdu7d2/ICwAAAACilSGg3gmAJKdshFq1atn48eNdZsTFF18czGbQj12xYsVcFsJdd93lsiR69+5tmzZtiji8o2HDhlatWrWQbIo2bdrYgQMH7LPPPgtmRjz66KMuICHff/+91a9f31555RWXkSFTp051QYz//vvPfd+sWTNr2rSpC1Z43nzzTRcI+euvv+I8tsGDB7u2hxs0d4Nlz53nNM4aEL9+tQundBMAAAAQDz2ozJcvn+3Zs8fy5s0b53pkRgAxUqNGjeDXChwoGLFt2zb3vTIZateuHWediZUrV7qAhJ++1/K49lG0aFH3b/Xq1UOWHTp0KJjJoAyMxx57zHLnzh18denSxbZu3WoHDx6M81gUvNAvF++1ZcuWBJ4NAAAAAGcyClgCMRI+s4YCEt4wjBw5ciT5PrT9uJZ5+9XwEGU43HDDDSdtSzUk4pItWzb3AgAAAIDEIDMCSAWU0aDsiJ07d0Z8X8Uo58+fH7JM31etWvW09qvClapbUb58+ZNeGTPy6wEAAABA8iAzAkgFNIuGZtu47rrrbMSIEVa8eHFX6LJEiRKu7sNDDz3kakRoKIfqPHz66af24Ycf2jfffHNa+x04cKCbfUOze7Ru3doFIDR0Q0UyH3/88SQ7PgAAAADw49EnkApous6vvvrKzjrrLLvqqqtcnYeRI0dapkyZ3PsKUjz11FM2evRoV8hSs15MmjTJFck8Hc2bN7fp06e7fV9wwQV20UUX2bhx46x06dJJdGQAAAAAcDJm0wCQZBVzmU0DyY3ZNAAAAFI3ZtMAAAAAAACpEsEIAAAAAAAQUwQjAAAAAABATBGMAAAAAAAAMcXUngCSTK+aheItUgMAAAAAQmYEAAAAAACIKYIRAAAAAAAgpghGAAAAAACAmCIYAQAAAAAAYopgBAAAAAAAiClm0wCQZMYu22HZcx9J6WYgDetXu3BKNwEAAAAxQGYEAAAAAACIKYIRAAAAAAAgpghGAAAAAACAmCIYAQAAAAAAYopgBAAAAAAAiCmCEWeIJk2aWM+ePVPNdqI1ePBgq1WrVqpqU2qSmPNTpkwZGz9+fLyfyZAhg02bNi3J2gkAAAAAfkztiYhmz55tl156qe3atcvy588fXP7hhx9alixZLDVJjjYtWLDAHn/8cVu4cKH9999/VqFCBevUqZPdf//9lilTJktLUuM1AwAAAHBmIzMiHThy5EjM9lWwYEHLkyePpSZJ3aaPPvrIGjdubGeffbbNmjXLVq1a5YIQCk7ccsstFggELC1JjdcMAAAAwJmNYEQapLT7e++916XeFy5c2Jo3b26//fabtWjRwnLnzm1Fixa1du3a2b///hvnNt544w2rW7eu66QWK1bMbr31Vtu2bZt7b9OmTS4rQgoUKOBS9jt27Bgx5V+ZE+3bt3fr5cyZ07Vh7dq1wfcnT57sMiu+/PJLq1KlimvflVdeaVu3bg3JwqhXr57lypXLrduwYUP7/fffT2qvhhfky5fPBQT27dsX7zCEoUOHWtu2bd02S5Ysac8991xU5/bAgQPWpUsXu/baa23ixIluCIS2d+edd9prr71m77//vr377rtu3datW7vr4FEbdK4UvPCCRNr/N998E2xnjx49rE+fPi5AoPOuYRan8scff7hj0We0PV23RYsWJfr8hNP1uuSSSyx79uxWtWpV+/rrr6M6VwAAAACQWAQj0ih1jLNmzWrz58+3kSNH2mWXXWa1a9e2n376yWbMmGH//POPtWnTJs7PHz161HXYly1b5moDKADhBRxKlSplH3zwgft69erVLnDw1FNPRdyOPqN9fvLJJ25Ig7IGrrrqKrd9z8GDB2306NGuwzx37lzbvHmz9e7d27137Ngxu+6661wmwi+//OK20bVrV9ep96xfv961cfr06e41Z84cd8zxefLJJ61mzZq2ZMkS69evn8tsiKaT/dVXX9mOHTuC7fO75pprrGLFivb222+779VmBVI8apeCQ96yH3/80Z2HBg0ahFw3BRQUTBg1apQ99thj8bZr//79bj9//vmnO8e6XgpmnDhx4rTOj0fbueGGG9y9pDa9+OKL1rdv31N+7vDhw7Z3796QFwAAAABEi5oRaZRqGKgzKxo+oEDE8OHDg++/+uqrLqiwZs0a14EO17lz5+DXZcuWtaefftouuOAC1/lV9oKewstZZ50VUjMi/Im6OsgKiHgd7ilTprj9qnN80003uWXqkKuTW65cOfe9sgnUCRd1Yvfs2WMtW7YMvq8MivAOszIsvKEGyvqYOXOmDRs2LM7zo+wKBSFEx682jhs3zi6//PJ4z6vOV6Q2eCpXrhxcRxkHCnJs377dMmfObCtWrLABAwa4YMRdd93l/tU5VcaIp0aNGjZo0KDgNXz22WfdscTVrrfeesttX4EN75qUL1/+tM+PR1kbyuRQ5kqJEiXcMt1HynCJz4gRI2zIkCGn3D4AAAAAREJmRBpVp06d4Nd6Wq7aBgoieC91mr2n5pEsXrzYPek/55xzXCdWT99FWQvRWrlypeuEX3jhhcFlhQoVskqVKrn3POqMe4EGKV68eHBIiDrYyq7QUBO1RxkY/iEcouEH/poH/s/HpX79+id972/TqURTF+K8885z7Vcmwrx581xASEEVfS/6VwELPwUj/PzHogCG/xrK0qVL3Xa9QEQkiTk/Hp0TBY+8QESkcxdJ//79XRDJe23ZsiWq/QEAAACAEIxIo5Tq71E2gzry6rj6X14tgEh1EdT5z5s3r8tk0FN3FW1MrmKY4TM5aAiGv7M/adIkNzxD2RXvvPOOy2T4/vvv4/28f5hCUvKySOIKXGi5t47aofOrDAgv8KBgg4YwqIaHZuTwgjzRHIuyRfzXT3LkyHHKNsfy/HiyZcvm7h//CwAAAACiRTAiHTj//PNt+fLl7gm5Uvj9L3/QwqO0fNVFUF2Biy++2GVRhD9JVw0BOX78eJz71VAG1XzwF1PUdlVnQoUQE0JP//W0XR14ZRxoeMLp8AczvO/jGnrhd8UVV7gshDFjxpz0noakKMCjYpIer26EXgpGZMyY0QUoVLNCQQkNF4mWhsT4r50ouKHAxM6dOy056Jwoq8GfjRJ+7gAAAAAgqRGMSAe6d+/uOqvqJCvLQUMzVAOgU6dOEYMJGpqhYMMzzzxjGzZscJ1sFbP0K126tHvCroKIqlmg7ItwqnnQqlUrN/vEd99954aL3H777W72Ci2PxsaNG10QQpkRmkFDBSTV4Y8mcBAf1YhQTQ3Vd9BMGu+9956r73AqCt5MmDDBPv74Y1dIU0U1VdzzlVdeccNJNIOGvzCoAhCqFaFgUKNGjYLLlHGiWS8iBYMSQtdUs26oyKeOSddLxUV1vpJCs2bNXKZHhw4d3PXTcJNHHnkkSbYNAAAAAHEhGJEOaLy/OqoKPOjJfvXq1d1Ujio8qSf14YoUKeIKHqqDrgwGZUhotgs/BRRUoFBFIDVVqH8KSz8NsVD9CtVKUK0BDb/4/PPPTxo6EBfVk1Cmxo033ug6xQoAKLjSrVs3Ox0PPvigm+VDGRcq8Dl27Fg3NCUaCjioBofqZyhzRDUwVPxSnfSpU6eGzPShc63zrClAvToPCkboWoTXi0gMBY0UoFHWhGYp0f50vTJlymRJQfeHhuj8999/bnpVTWEaTeFLAAAAADgdGQLRVOoD0hANV1EwRi/EhmZFyZcvnw2au8Gy5/7/xTSBhOpXu3BKNwEAAABJ0DdQofv4asuRGQEAAAAAAGKKYATOKKrl4J8+0/+qVq1aSjcPAAAAAM4ImVO6AUBSU8HJuFx77bV24YUXRnwv2joXAAAAAIDTQzACZ5Q8efK4FwAAAAAg5RCMAJBketUsFG+RGgAAAAAQakYAAAAAAICYIhgBAAAAAABiimAEAAAAAACIKYIRAAAAAAAgpghGAAAAAACAmGI2DQBJZuyyHZY995GUbka60K924ZRuAgAAAJBsyIwAAAAAAAAxRTACAAAAAADEFMEIAAAAAAAQUwQjAAAAAABATBGMAAAAAAAAMUUwAgAAAAAAxNQZE4woU6aMjR8/PtVsB/Hr2LGjXXfddae1jdmzZ1uGDBls9+7did4G1xsAAAAAkt4ZE4z48ccfrWvXrsHv1UmdNm1azNuR2P3GulM8efJk19YqVaqc9N57773n3lObkstTTz3l2pDa7hsAAAAAwOlL98GII0eOuH+LFCliOXPmTOnmpCm5cuWybdu22cKFC0OWv/LKK3bOOeck2bXxO378uJ04ccLy5ctn+fPnt5TGfQMAAAAA6SwY0aRJE7vvvvusZ8+eVqBAAStatKi99NJLduDAAevUqZPlyZPHypcvb1988UWwo3rHHXfYueeeazly5LBKlSq5J+iR0vuHDRtmJUqUcOuEZxZ4T/Svv/76kCf869evt1atWrl25M6d2y644AL75ptvEtzBvvfee6148eKWPXt2K126tI0YMeK09qvz9Pvvv9sDDzzgPqeXDB482GrVqhWyfx2jP2NBQxXq1avnAgvq3Dds2NBtKxqZM2e2W2+91V599dXgsj/++MNtU8v9ojl3atfQoUOtffv2ljdvXpdxoOwHteuTTz6xqlWrWrZs2Wzz5s0nDdNQgELn0bv2NWvWtPfffz9k+59//rlVrFjRvX/ppZfapk2bojrOTz/91LVX16tw4cLu+vjb7N03Ouabb7455LNHjx51n3n99dcjbnvXrl122223uaCG2lWhQgWbNGmSe0/t07WcOnWqNWjQwO3/vPPOszlz5oRsQ9/rGurc6L7q16+fHTt2LGIbPbovdH9IIBBwXyuApG3o56JHjx7BdQ8fPmy9e/e2kiVLuvvkwgsvdNc4PvrM3r17Q14AAAAAkGYyI1577TXXmfvhhx9cYOLuu++2m266yXXOfv75Z7viiiusXbt2dvDgQdchPfvss90wgRUrVtjAgQPt4YcftnfffTdkmzNnzrTVq1fb119/bdOnT4+Yei/qFG7dujX4/f79++2qq65yn1+yZIldeeWVds0117jOcbSefvpp17FWm9SGKVOmBIMDid3vhx9+6I77sccec5/TKxrqsKpD37hxY/vll19choMCAF4wIxqdO3d2x6LzLwoeqH0KOvhFe+5Gjx7tAglaZ8CAAW6Ztv3EE0/Yyy+/bMuXL7ezzjrrpHYoEKEO/4svvujWUWDm9ttvD3bct2zZYjfccIPb59KlS+3OO+90nfZT+eyzz1zwQW1Xm9R+dfwjUVBBgQsdq+fLL7907fcHMPx0jLpXFVBbuXKlvfDCC+5+93vooYfswQcfdPuvX7++O4YdO3a49/7880/XNgVLli1b5j6vzJTHH3/covXBBx/YuHHjbMKECbZ27Vo3TKh69erB9xU8072hoIjuE/386fpp3bjoeih7xXuVKlUq6vYAAAAAQOaUboA6po8++qj7un///jZy5EjXWevSpYtbpoCDOmDqJF100UU2ZMiQ4Gf1lFydKHWW27RpE1yup7vq2GbNmjXiPvWUWvREvlixYiFt0cujp/gfffSRCy6owxYNdb719LtRo0au06/MiNPdb8GCBS1TpkwuU8T/uVPR0+o9e/ZYy5YtrVy5cm5ZpBoQ8aldu7aVLVvWZSEoKKRgxNixY23Dhg0h60V77i677DLX8fbMmzfPZRc8//zzIZ8Pfwo/fPhwl2mhzrqoTd99953rYCvYontExzhmzBj3vjJifv31VxfkiI8yaG655ZaQ+yqudjRv3tzdWzounQt566237Nprr3XXJq77Qeewbt267vtIdTZ0fm688Ub3tY5jxowZLuDQp08fd17U0X/22Wfd/VS5cmX766+/rG/fvu5nI2PGU8cT1QbdN82aNbMsWbK4DAkv4KL3FBzTv8qYEGVJqA1arvMeiX5We/XqFXKvEZAAAAAAEPPMiMTOWFCjRo3g1+pwFypUKOSprfcEXrUL5LnnnrM6deq4jr2GA0ycOPGkp+/6fFyBiPjoibc6YuqwK2Cg7etpdlyZEXfddZdbx3uJhhfoybw6w0qF/+qrr5J8v9FSEEPtUSdaT9s1pCXarIrw7Ah1TJWFoCE0elKf2GPwOuV+ulb++yDcunXrXPbB5ZdfHnK+lSmh4SGifWl4gZ8XuPD4P6trJ7pWTZs2jXrYioJeynYRnYuPP/7YZUxIixYtgtuvVq2aW6ZMH2UcaNiEggsLFiw4abv+dmofOkc6Hu+49L4/m0VDbXS+NWQmGsp0+O+//1wAR0E+BVO8YR4K2Gj4k4a3+M+PrrV3biPRcA8NtfG/AAAAACBZMyP0tFlPeL3x8+qgKRVcT181bj+uJ8uR6Emtnzpd/mVeJ0xDNNSpU4dXT7/VQdPT6CeffNIWLVoUsg09vU4MbVtDOzSUQLUqNMa/devWEQstioZN6DN+559/vm3cuNGl5etJvs6NnkiH1zc4nf169FRc9QD8lGXgpyCCgiJ60v3OO++4LBTtS1km0VJnWx1p1R1QRoA6zIk9hkjXRuvGN3TEGxahIRWqaxDeKY6WAg8er/OsfSeEzoUyMRQc0/Hq8xrSIMrGUadfvHtYAQrV6NDPhdZX4KN79+7uPCWVU90HyljQkCHdj2rDPffc435uFHDQuVUQcPHixe5fPy/ABgAAAACpIhihcfve02F1bvRS51vDJTT+PZpsgMSYP3++qyWhzpQnvqe38VFnUU+Ew7evTAJv/L86avEVQVRtg0j1DdTRVaBGL3XI1VnduXOny1RI7H6VPRD+OWWH/P33364j6nXm/R1uj4YJ6KXUegVxNLQgIcEItVtDEXR9de0jSei5Swh/YUsFAiJRRoaGhPh9//33Id8rSBJOGRmqE6GCqdHQ/afOvQI7uueVdeAFHsIDJf7r1KFDB/e6+OKL3c+IPxihdl5yySXua2UsKDDgDW3RcSnQ57/GOtcKxKmOiLd9f8aLhkwoIOanoImyY/RSMETDPZQVoftC95WCK2obAAAAAKTaYIQ6wN74cBWI1NN/FZpUtkR4qnxSUi0GpearaKDqRbzxxhuuCKS+Tii1VZ1Qpbyro6vZPLR9FYtUh00dPxUfVEZGQqiegmY8UCdPT6xVbFMZI940lYndrz43d+5cV99An1NdDc2ysX37dhs1apQLeij7QR1k76m/OqQaxqJAguoB6Om4ihJqNouEUq0I1S/QMJpIkuLcxUUdb2VeqGiltql6HKqFoU65jlWdfA27UMaMOvoqXqkOvdp8KoMGDXLZCqo3oXOrYICyGFSTIS6aVUNBmTVr1tisWbPi3b7qOmhYkYZtqPaFfl7C63Zo6JHOn5ar0KRm4NDQGFHgTTNlqLirAhS6hmqz6jV49SJUh0PHqnOv+0z79Gc56D0FHPSzqWlK33zzTRecUD0TXU9le+ie0PnTfat7SveoAjVXX331Kc8hAAAAAMSkZoQ60Jq9QNQB1jAE0dPb8Kf3Salbt25uxgRlHKhjpRkH/FkSCaGOlzI6FFRRB8wLJOjY9PRbHTvVWtCwi4R2nBUc0Lh/zYCg7AB1br2OY2L3qyEh2pY6zV4hTHVeFSBQZ1ZDYzQjiX/YiDqeq1atcsURVRNAM2noqbjOY0Kp8xpXICLaYzgdKoipAIdmcdBxK9tEwza8QJSKMiqDQDNF6FwoWBBX8UU/BXQUMFJWheo6qGOv8xgfdd41Q4YyIRRUio8yWpSRoo69sh8UJNBwIz8VbdVL7VZRTrXFm3FD+9D9ozbpfQVdNL2tV/RVtH1ljKhQqYIHmkHFK1gqClBoyly1Ve3QcA3NCuJdTw3lUTBChUVV60SfV5BP5xQAAAAAkkOGQPhg8yjoCa2e8OpprqYjVCdZ48vVyVJHXFNyAoiffm4UTNHPkAIhaZmGhmiKz0FzN1j23JFnFkHC9KsdOgUsAAAAkJb6Bspmj6/QfaKGaSiVXMMGlB2h4INX6E7j1hObqQAAAAAAAM4MiQpGqGBf+CwSojH9SBtUw0CzPEQyYcKE4HSVAAAAAACkimCEqHikOq0bNmywhQsXumJ4KrSntPNWrVolbSuR5FSHIHwaUE/RokVj3p4zkbKLEjFKCgAAAADOzAKWL7zwgqvm36JFC9u9e3ewaKUK5SkggdRPwSNNdRnppSKcAAAAAACkqgKWVatWdTMVqOq+Oq7Lli2zsmXL2m+//eZmJ/j333+Tp7UA0nSRGgAAAADpW7R9g0RlRmzcuDE4LaVftmzZ7MCBA4nZJAAAAAAAOEMkKhihuhBLly49afmMGTOsSpUqSdEuAAAAAACQTiWqgKXqRXTv3t0OHTrkCvD98MMP9vbbb9uIESPs5ZdfTvpWAgAAAACAMzsYceedd1qOHDns0UcftYMHD9qtt95qJUqUsKeeespuueWWpG8lAAAAAAA4c4MRx44ds7feesuaN29ut912mwtG7N+/384666zkaSGANGPssh2WPfeRlG5GqtGvduGUbgIAAACQPmpGZM6c2e666y43RENy5sxJIAIAAAAAACRvAct69erZkiVLEvNRAAAAAABwhktUzYh77rnHHnzwQfvjjz+sTp06litXrpD3a9SokVTtAwAAAAAA6UyighFekcoePXoEl2XIkMHNrKF/jx8/nnQtBAAAAAAA6UqighEbN25M+pYAAAAAAIAzQqKCEaVLl076lgAAAAAAgDNCooIRr7/+erzvt2/fPrHtQRoze/Zsu/TSS23Xrl2WP39+SwuaNGlitWrVsvHjx1t6tGnTJjv33HNdkVkdJwAAAACki2DE/fffH/L90aNH7eDBg5Y1a1Y31SfBiPQpUie+QYMGtnXrVsuXL19M2zJ9+nR78skn7eeff3Y1SqpVq2bdu3e3jh07pulASVIoVaqUuyaFCxdO6aYAAAAAQNJN7anOnf+1f/9+W716tTVq1MjefvvtxGwSaZQCUMWKFXOFS2PlmWeesVatWlnDhg1t0aJF9ssvv7iiqnfddZf17t3bUsKRI0dish8F/k4lU6ZM7ppkzpyoWCMAAAAApM5gRCQVKlSwkSNHnpQ1gf85fPiwm33krLPOsuzZs7vAzY8//hh8f/ny5dayZUvLmzev5cmTxy6++GJbv3598P1XX33VPf3Pli2bFS9e3O69995gSr4CAUuXLg2uu3v3brdMmQGif/X9Z5995qZd1f4vuugi++2334Kf2bFjh7Vt29ZKlizpsluqV68eElhSxsGcOXPsqaeectvSS/v2tq19ej744INgW8uUKWNjxowJORdaNnz4cOvcubM71nPOOccmTpwY1XncsmWLm1a2Z8+ebhtVq1a18uXLu2XKlNC+FKBQ25QVIQUKFHBt9GdNnDhxwvr06WMFCxZ0HffBgweH7EfHc+edd1qRIkXcNbnsssts2bJlwfe1vrJEXn75ZTckQuc0kvfff9+dyxw5clihQoWsWbNmduDAgeD7+nyVKlXc5ytXrmzPP/988D3v2r7zzjvWuHFjt84LL7zgtvXFF1+E7Oejjz5y51IZSpHuiVPdX/G1AwAAAABSbTBC9CT2r7/+SspNphvq+KqT/tprr7mhBepAN2/e3Hbu3Gl//vmnXXLJJa7z/u2339rixYtdR/3YsWPus+qAaghC165d7ddff7VPPvnEfT6hHnroIddZVxBEnexrrrkm+KT90KFDVqdOHRewUJBC+2rXrp398MMP7n0FIerXr29dunRxQwD00nCAcGp7mzZtXKaC2qpO+4ABA2zy5Mkh66kddevWdXUN7rnnHrv77rtdds2pqHOvNkfKgOjWrZvlzp3bBVHUNp1v0XbVXh2DR9chV65cLnAxatQoe+yxx+zrr78Ovn/TTTfZtm3bXKdfx3T++edb06ZN3fXyrFu3zu3jww8/DOn4e7RPBXh0LVeuXOkCNzfccIObAlemTJliAwcOtGHDhrn3FVzRuVLb/Pr16+eCfFpH7VJQ4a233gpZR9u67rrrXCAp3Knur2jbER5c27t3b8gLAAAAAKKVqDxudYb91LlSx+vZZ591qfMIpSfhCiioQ96iRQu37KWXXnKd31deecUNdVHNhalTp1qWLFnc+xUrVgx+/vHHH3dP/v1ZJxdccEGC2zFo0CC7/PLL3dfqaJ599tnuibqCB8qI8Hfw77vvPvvyyy/t3XfftXr16rn2eTVBlEkQl7Fjx7pOuzqz3nGsWLHCZS34MxOuuuoqF4SQvn372rhx42zWrFlWqVKleI9hzZo1ri3KDgmn9pUtW9ato6EKynoQZaOE14xQhojOh5fVo3t35syZ7vx89913LgijYIQ68DJ69GibNm2aC4YoUOMNzVAxVwV2ItHPhDr8CkB4M9AoS8J/PRSU0fuiDAudqwkTJliHDh2C6ykLxFtHbrvtNhcoUhaErocCAQoi6VpG8txzz8V7f0XbDr8RI0bYkCFDIr4HAAAAAMkSjNATWD+lhKtDplT28JR8mEuH19N8f6BGnUJ18vUk+u+//3Zp815H0U8dYmWbqIN/upTZ4FFHXR1/7V9UBFJPxBV80JN0dbT19DvSk/b4aHuq5+Cn41bRS+1DQQIvGOC/fxTg0LHGin//ouCGt38Nx1AdFA2r8Pvvv/9ChjYowOAFIubNmxcMNIk68soO0XVTAEJZMFdccYW1bt3aDRtRgErbuuOOO1y2iUfBi/BioMog8VMgR/eKgoLah7IzNPxCQ0AiUdZGXPdXQtrh179/f+vVq1fwewVEImXKAAAAAECSBSM03h5JRzUAEvOeZMz4v5E2Xup/tEUOwylzQcMYFDRQ51lDGPREPrkKM4Z3jBWQiOa+0hP9PXv2uABNiRIlQt5TW9Wx9mpFJHb/CkQoOOHV3PDzZ1joHPkDBv6hGkWLFnWBF2W/LFiwwL766itXePORRx5xQ0O8II8yZC688MKQfXgBm0j78TJAFNTQUA0FI/TvzTffHGfByvjuIR1rtO3wU8aIlzUCAAAAADGpGaHx9UoRD6cnx3oPocqVK+c6kPPnzw8JGKh2gwow6im9nqxHCiKo2KAKPmoIQSTek3kNCfBEql8g33//ffBrDQ3RcAYVLRS1TRkNt99+u9WsWTM43MFPx6Dshvhoe/7j9LatIEJ8ndto3XjjjS6QECkD58UXX3RP+lWnwWuvnKrN4VQfQtkq6tyrNof/Fdd0merw+9fTdfOCHMoM0ZAG1cdQmzScQsEKBVM2bNhw0j40TOJUNFRjxowZrjCl6kDo+7jEd3+dbjsAAAAAIGaZEepYaRrF8BR+BSj0norhIfTJtgo0qoCkhkdo9ggVTdT5Unq8nsjrqbmeciv9XenxChxoGIeGUqgIpM63ah9oKMC+fftcB191HdQJ1swYmslEnUcNNXj00UcjtkOBIg09UAdUT+jVsfaG3Khuguoh6Cm+hhGo9sM///zjgiUeBUW8mSpUKNKryeCn2haqZzF06FD3tH7hwoWuHkNSzc7gnTvtRzM/qHaCghMff/yxPfzww26594RfwygUDJg+fbob2qBzpXafioY7aEiLzo32pUCKMjFUl+H6668/adhEXHSuFETS8AxdO32/ffv2YABIPyuaYUXX+8orr3TDYn766ScXKPIPgYhEBSk1tEVBCF338KwGP828Et/9dTrtAAAAAICYZUZoSIA6eeE01j5SBxXmggV6qq/Os568ayYGFYhUx18BAj3dVsq8pnDUrBZKm/eGEqiIoIZPqEOvKTM1m8LatWtDpv3UGH99TkMrVPAyrjaoCKbW05P/Tz/9NJg9oACG2qXaBk2aNHEd3fDaICpwqewGBSiUkbF58+aT9qFtqO6EiiWed955LjClIIi/eOXp0jEqu0BP+xUY0H40VEFFQlVo0qOinOpoazYKBWC86VBPRff2559/7jr8nTp1csEIdeR///13t51oqY7D3LlzXSBE29A5VkaHV1tCU4dqSs1Jkya5oTG69ipyGk1GgtqoDBD9zMWXFSGnur9Opx0AAAAAkBgZAv5iA6egjrM6QRqzr46WPyChVHh1dvQEX9X7kXqo9oHqKOhJd/isEkBSUAFLZVYMmrvBsuf+3xAVmPWrHXlYDwAAAJDe+wZe3CBJhmno6bxiF507d3ZPnP3V9vWEXWn8/hkbAAAAAAAATisYoeECovTtBg0aRJwqEDgdml5Ur0g0PeUXX3wR8zYBAAAAAFJwmEYkhw4dOmn6x/hSMYD47Ny5070iUQFK1YFA6sMwjcgYpgEAAIAzzd7kGKbh0SwQffr0cYUKd+zYcdL7CZ1KEfCoACpFUAEAAAAgfUtUMEJTVM6aNcvNXqDZIVSw8s8//7QJEya4GRsAnJl61SxEZhQAAACA5AlGaErI119/3U0BqakPNZa/fPnyVrp0aZsyZcoppxoEAAAAAABnroyJ+ZDG9JctW9Z9raeg3hj/Ro0a2dy5c5O2hQAAAAAAIF1JVDBCgYiNGze6rytXruxqR3gZE/nz50/aFgIAAAAAgHQlUcEIDc1YtmyZ+7pfv36uZkT27NntgQcecPUkAAAAAAAAkm1qT/n9999t8eLFrm5EjRo1TndzANLp9D0AAAAA0rdkndrT79ChQ65wpV4Azmxjl+2w7LmP2JmqX+3CKd0EAAAAIP0O0zh+/LgNHTrUSpYsablz57YNGza45QMGDLBXXnklqdsIAAAAAADO9GDEsGHDbPLkyTZq1CjLmjVrcPl5551nL7/8clK2DwAAAAAApDOJCka8/vrrNnHiRLvtttssU6ZMweU1a9a0VatWJWX7AAAAAABAOpOoYMSff/7pilWGO3HihB09ejQp2gUAAAAAANKpRAUjqlatavPmzTtp+fvvv2+1a9dOinYBAAAAAIB0KlHBiIEDB9q9995rTzzxhMuG+PDDD61Lly6uloTeQ6gyZcrY+PHjU812kH41adLEevbseVrbUD2Y/PnzJ1mbAAAAAOC0ghGaNSMQCFirVq3s008/tW+++cZy5crlAhArV650yy6//PKEbPKM8OOPP1rXrl2D32fIkMGmTZsW83Ykdr+xDoKoM6y2XnnllSHLd+/e7ZbPnj07Zm1JaxQY1Ew3AAAAAJCaZU7IyhUqVLCtW7faWWedZRdffLEVLFjQfv31VytatGjytTANO3LkiJttpEiRIindlDQnc+bMLtg1a9Ysu/TSSy01XteUorosWbJkidgm/UwCAAAAQLrKjFBWhN8XX3xhBw4csLSazn7fffe5lPYCBQq4gMpLL73kjqdTp06WJ08eV6RTxyjHjx+3O+64w84991zLkSOHVapUyZ566qmQbXbs2NGuu+46N1ylRIkSbp3wzAJ9Lddff717yu99v379epdxonbkzp3bLrjgAtcZTwh1SDV8pnjx4pY9e3YrXbq0jRgx4rT2q/P0+++/2wMPPOA+p5cMHjzYatWqFbJ/HaO3XVEGQ7169Vz2jNL+GzZs6LYVDX2mc+fO1q9fv3jX27Jli7Vp08ZtXx1xHcumTZvce1999ZU7D8qo8Lv//vvtsssuC37/3XffueCarmupUqWsR48eIfe1jknZBu3bt7e8efOGZLl4tE+dm3fffTe4LZ3LNWvWuMyYunXruvPbokUL2759e/Bzek/ZRIULF7Z8+fJZ48aN7eeffw7Ztrb7wgsv2LXXXuvOi+4v7/xrKl3dkzrOSMM0Dh8+bL1797aSJUu6z1544YUnZZYoE+Wcc86xnDlzuvtjx44dp7w+AAAAABDzmhFxBSfSmtdee811An/44QcXmLj77rvtpptusgYNGrgO4RVXXGHt2rWzgwcPutoYZ599tr333nu2YsUKNzTl4Ycfdp1Pv5kzZ9rq1avt66+/tunTp5+0T3U+ZdKkSS7LxPt+//79dtVVV7nPL1myxA1RuOaaa2zz5s1RH8/TTz9tn3zyiWuT2jBlypRgcCCx+1Xav477sccec5/TKxrHjh1zgRl1rn/55RdbuHCh68R7wYxoqMOtzBsVRo0rQ6B58+YucKSCqvPnz3cdfh2DAjNNmzZ1QYoPPvgg+BkFld555x03La0XjNH6N954o2un3lNwQkEdv9GjR7upa3WOBgwYEGebBw0aZI8++qi7f5Tdceutt1qfPn1c4EptXLduXUhdlX379lmHDh3cPr///nuXfaTroeXh50KBAp0PBWlE29Kx6RotXbo0Ynt0HDr3U6dOdcen+1vHu3btWvf+okWLXJBN62kbykJ5/PHHT3ltFOTYu3dvyAsAAAAAkmWYhv/JuH9ZWqXOpTqO0r9/fxs5cqQLTqgYp6jTqCfS6sRddNFFNmTIkOBn9TRanTx1/PVk3qOnz3paHVcavzdkQ53kYsWKhbRFL4+exH/00UcuuBDeMY6LAgjqzDZq1MhdF2VGnO5+lW2QKVMm1+H3f+5U1Dnds2ePtWzZ0sqVK+eWValSxRJC2SXKYnjkkUdcYCOcAgcKEul8e/ehgi06Rj39VzDplltusbfeest1uEVBF2VKKPggyhxRYMLLJtD5U1BHQRRdey/jQJkUDz744CnbrCwEBUhEbW/btq3bp7JCRO1QJoLHn6EhEydOdO2fM2eOO3ceBTWUseOngMvrr78e5zAg3Q86H/pX59Jr34wZM9zy4cOHuyCJghMKmEjFihVtwYIFbp346Lz5fx4AAAAAIFmHaWgowg033OBehw4dsrvuuiv4vfdKK2rUqBH8Wh3uQoUKWfXq1YPLvFoY27Ztc/8+99xzVqdOHdf50xN4dRzDMxf0+cTUE1CGgjqK6rCrM6rtqyhoXJkROu9ax3uJro2ebmt4iIYaaJhCUu83WgpiqD3qmCvTQp3eaLMq/Pr27euGNbz66qsnvbds2TKXHaBAiXcetF/dl8p4EAUaFJj466+/3PfKFrn66quDs0VoGwoO+M+l2qwgx8aNG4P70jCL+M59pHvKu3/C7ynvfpJ//vnHBb8UBNEwDQ0D0TUJP//+/XsUbIqvHomyKJQJogCDv70KdHjnR9daQzf86tevb6ei4J2CTd5Lw2UAAAAAIFkyI5RO7nf77bdbWhZeBFBP1/3LvKft6pgqzV2d9jFjxrjOmjrATz75pEtz91NmRGJo2xraoeEAqlWhmgOtW7d2T78j0bAJfcbv/PPPdx1o1blQ3QdlbDRr1izOYQ6J2a8nY8aMJw3T0bAJPz19V1BET9mVxaAsFO1LWSbRUtBAHV89hfdnCog67QoOKcAQzuukq26DMjN0/TQMR1kf/swEbaNbt26uneFURyHSdY107j2R7p/wZbqf/D9TqtGgYI2CC9myZXP3V/j5j3Rfnepe07EpyLZ48WL3r194ECWh1E69AAAAACDZgxHqXJ6pVI9AtSTuueee4DLv6XJCqXOqJ9bh21cmgeoCeB1JrxBjJJrRRK9werJ+8803u5eCCkrB37lzp8sYSOx+lekR/jl19v/++28XkPA63ZHqFtSuXdu9FFBQJ1tDJhISjBDV89DQifCCoQq+KMih86DjjouyIxSwUO0LBVGUGeHfhmqAKBATrbjOfWLo/D///POuToQow+Dff/9Nkm3rvOu6KRNDRTUjUUZMeEBNtSsAAAAAINUWsDyTKI3+p59+si+//NLNkKAihl4RyIRSUUnVEVBnfteuXcHte4UINXRANQL8T9CjMXbsWHv77bdt1apVro0qtqk6D96QhMTuV5+bO3eu/fnnn8GOsmZt0PCJUaNGuaCMhrB4M4+IMjQUgFBdDc2goSEjKpqY0LoRoroNyoxQQCI8yKAaH5pBQ8UhtU8NyVCWwx9//BGyngpKahYKBWj8T/Q1DEQ1ErwCjmrjxx9/HHWdjtOl8//GG2+44RIKCqityk5JChqeoe1pFhBdY50fFWtVvYfPPvvMreNlrigzRsf+7LPPnrJeBAAAAACcLoIRUVIqv+phKONAY+yVWu/PkkgIDfXQcAVNI6mn114gQVOMKvtCNRZUt0BP7RNCQ0cUHFB9AQ1PUIbD559/7rIBTme/GpagbWm4gzf8QUEFPdFXEEIFMNXJ9Q9d0DSRCoqoUKQ6xZpJo3v37u48JoaGM5QtWzZkmfahIImGU+jaqE0qEKmaEf5MCWU9aIpRFSL1ZtHw13hQDQUFb5Q9oPOiwqVewcfk9sorr7jAkM65Zm5RcCCpsi68bCYFI1R8U7VEVAhUQTRvCIqyVDSlrbJOdB0VNPKKugIAAABAcskQSOvzcwJIcZo9RQU4B83dYNlz57EzVb/ahVO6CQAAAECq6Buo0H18w+nJjAAAAAAAADFFMAIxV61atZCpJv2vSDNjAAAAAADO4Nk0gKSgOhbh04B6ihYtGvP2AAAAAABii2AEYq506dIp3QQAAAAAQAoiGAEgyfSqWSjeIjUAAAAAINSMAAAAAAAAMUUwAgAAAAAAxBTBCAAAAAAAEFMEIwAAAAAAQEwRjAAAAAAAADHFbBoAkszYZTsse+4jlt71q104pZsAAAAApGlkRgAAAAAAgJgiGAEAAAAAAGKKYAQAAAAAAIgpghEAAAAAACCmCEYAAAAAAICYIhgBAAAAAABiimBEOjd79mzLkCGD7d69O6WbghRSpkwZGz9+fNTrT5482fLnz5+sbQIAAABwZiMYkY40adLEevbsGbKsQYMGtnXrVsuXL1/M2qHgR6TX1KlTY9YG/H8//vijde3aNaWbAQAAAABBmf//l0iPsmbNasWKFYv5fidNmmRXXnllyDKetsfWkSNH3PUvUqRISjcFAAAAAEKQGfF/Dh8+bD169LCzzjrLsmfPbo0aNXJPlD3Lly+3li1bWt68eS1Pnjx28cUX2/r164Pvv/rqq1atWjXLli2bFS9e3O699163fNOmTS4rYOnSpcF1NWRCyzSEwj+U4rPPPrMaNWq4/V900UX222+/BT+zY8cOa9u2rZUsWdJy5sxp1atXt7fffjv4fseOHW3OnDn21FNPBTMRtO9IwzQ++OCDYFuVwj9mzJiQc6Flw4cPt86dO7tjPeecc2zixIkJOp8KPCgI4n/puETb1XHqnHud5tq1a1v79u2Dn//444/t/PPPd58pW7asDRkyxI4dOxZyDrt162ZFixZ165x33nk2ffr0ONszePBgq1WrlrtOOp7cuXPbPffcY8ePH7dRo0a59unaDxs2LORzY8eOdec6V65cVqpUKfeZ/fv3nzSk4csvv7QqVaq47SoIo2wUj+6jyy+/3AoXLuwyVBo3bmw///xzyH5WrVrl7jkdS9WqVe2bb75x123atGnBdbZs2WJt2rRx+ytYsKC1atXKXWP/PXDddde5YyhRooRVqlQp4jCNUx1TNHTt9u7dG/ICAAAAgGgRjPg/ffr0cZ301157zXUUy5cvb82bN7edO3fan3/+aZdcconrvH/77be2ePFi16H2OscvvPCCde/e3aXC//rrr/bJJ5+4zyfUQw895AID6rzqafY111xjR48ede8dOnTI6tSp4wIWClJoX+3atbMffvjBva8gRP369a1Lly6uI6yXOprh1HZ1aG+55RbXVnXSBwwY4DrVfmpH3bp1bcmSJa6zevfdd9vq1astKTz99NN24MAB69evn/v+kUceccGFZ5991n0/b948F5i4//77bcWKFTZhwgTXPi9QcOLECWvRooXNnz/f3nzzTbfOyJEjLVOmTPHuV8GjL774wmbMmOECOa+88opdffXV9scff7hAzhNPPGGPPvqoLVq0KPiZjBkzuvYqGKV7Q9df94rfwYMHbfTo0fbGG2/Y3LlzbfPmzda7d+/g+/v27bMOHTrYd999Z99//71VqFDBrrrqKrdcFBBREEFBJu1bgR+dEz/dB7ofFRzS+dGxe4EPBXM8M2fOdNfp66+/jjM4E80xncqIESNcYMV7RbrXAAAAACAuGQKBQMDOcOoYFyhQwHV4b7311mDnT0+UVYNh165drt6BOnlZsmQ56fPKVujUqZM9/vjjJ72nJ9fnnnuu69Trybyo4639zZo1y9V5UPbCpZde6vZx8803u3UUBDn77LNdmxQ8iESZGpUrV3YdYdG2tA//U3Bv2zoGPVG/7bbbbPv27fbVV18F11FHVEEOdU5Fx63MD3WuRbeIMgeUnXDXXXed8nzqib6e8IcHBxQ0UFaCLFy40GUIKCChjq3OhTIDpFmzZta0aVPr379/8LMKOqidf/31l2u7ghErV660ihUrWjQUdHnyySft77//dh16UUde11RBCnXQRedTGQZeoCTc+++/787Bv//+677X9dG1X7dunZUrV84te/755+2xxx5z+4pEwRRdi7feestdQwVHFHhS5oM3pEaZEcqm+Oijj1ygQsev+0vHrPMrCkJoO8qeuOKKK1y7tS0FQzQ8w+Pdx+H1ROI7Jq0bX9FTZUZ4mS2izAgFJAbN3WDZc//v/KZn/WoXTukmAAAAAKmS+gZ6YLlnzx43siAu1Iz4vyfmCj40bNgwuExBh3r16rnOnzqV6pxHCkRs27bNdZDVeT5dymzwKA1fafbav/f0XEMn3n33XZepoY6oOoN6mp4Q2p7S+/103ApgaB9eAEHDKDzq/KqTrGON1rhx41xQwU9DB/zHquyBoUOHWt++fYOBCFm2bJl78u8fMqG2KTtEWQga8qJATVyBCGUMeG6//XZ78cUXg51yLxAhGuKh4/UCEd4y/3EqKKBgiYZR6IdK2TBeO7xzr3+9QIRomI5/G//884/LuFBgSMt1LPq8ggaigIg68v7aHrr3/HROFPDwt1/UFv9wIQ2/8AciIonmmE5FWUJ6AQAAAEBiEIyIQo4cORL1nngdXX8Cijf0IiH0VF9DMRQ08Mb76+m1P0U/KYUHXhSQ0BP9aKljHd9QFW1LAQcFA9TJ9lP9AmVh3HDDDSd9ThkXpzrn/voc/khcpGOK7ziV1aLMBQ1RUWBEASINtbjjjjvcefc67pG24b/eGqKhmh+6fqVLl3adeAVjEnLtdE40TGfKlCknvecvUKn7Ij7RHhMAAAAAJCeCEWbuqbaeJqtzrM6iFzBQ7QZ1+DWMQ2PrtSy846kn1XrirrH6Gg4RV0dRNRxUpDG8s+ynegLeMAYNq1izZo0riihqmzIa9KRf1GHW+yp26NEx6Kl7fLQ9bctP3yvL4FQ1F5KSgit6Mq9aDaqFoNk3NNxBVLhS2QJxBTOUtaE6Dzr+SNkRianXEYnqa+g8q36GF1RSZkpC6fxq6IbqRIiGY3hDIkQZMFqmDAplZoi/eKp3Tt555x1XZDO+VKdYHRMAAAAAnA4KWP7f02Q9KVYBSY25V20DFYJU2rqeGGtmDKWzq+jjTz/9ZGvXrnX1FLyCjqpHoM6digLqPRXAfOaZZ9x7eoqvmTFUYFFDJNT5Vsp+JKozoKCGClRq/L9mX1C9AFHRQxUlXLBggduOZpJQ59VPQREVQNTTb3V2I2UyPPjgg24fGh6hzryCLCoc6S+4mBRUb0DDW/wvBXVE9TMGDhxoL7/8shsiotkdVKxyw4YN7n299/rrr7vsCNWx0PGqnoZ33lRrQgVFb7zxRndONm7cGCxMmZQU1FAAStdSbdM194Z8JISunT6r49D1Ud0Of3aHakMoIKYMil9++cUFL7xj9epD6DO6HxSQUgFLHbOGfWgGGAVmYn1MAAAAAHA6CEb8HwUL1LnVDBV6Cq2hA5quUYUmCxUq5GYcUKq8OsJKl3/ppZeCWRLqRGr4hJ5+a8pMpcErKOHRdJIal6/PKdMiUqFLrw3qlGs9dd4//fTT4Ph/dU7VLmURqFClhkF4gQqPAgrKblC2hDIyvJoEftqGnoSrc6/pMNXxVxBEwY+kpCwH1U7wv9QBVm0CZXdofyraKJoZRFklOvfK7NAxaiYIFaq84IILXDBHNSi8rBXRzCd6T9Od6nhV3PJUWSEJVbNmTRco0SwbOlcaIqFaCwmlWTuU6aJzr2P0ppD16JqpCKXuLx3TnXfeGZxNw5sOVcMnNFOHMmc0fEUZLgqU6XwmJFMiqY4JAAAAAE4Hs2mkAuEzXgDKjlBRT/8sHWmhYi6zaQAAAABntr3MpgGkHZrCU7OAaEiHAhDKkNEQlrQQiAAAAACAhGKYBhJE04uq0xzp1aJFi5RuXpq1b98+6969u1WuXNkNYdFwjY8//jilmwUAAAAAyYJhGkiQnTt3ulckKspYsmTJmLcJKY9hGgAAAACEYRpIFgULFnQvAAAAAAASi2AEgCTTq2ahBM3uAQAAAODMRM0IAAAAAAAQUwQjAAAAAABATBGMAAAAAAAAMUUwAgAAAAAAxBTBCAAAAAAAEFPMpgEgyYxdtsOy5z5i6VW/2oVTugkAAABAukBmBAAAAAAAiCmCEQAAAAAAIKYIRgAAAAAAgJgiGAEAAAAAAGKKYAQAAAAAAIgpghGwTZs2WYYMGWzp0qWJ+vzkyZMtf/78lh5EOpaJEydaqVKlLGPGjDZ+/HhL7Tp27GjXXXddSjcDAAAAAOJEMALpxsGDB61///5Wrlw5y549uxUpUsQaN25sH3/8caK3uXfvXrv33nutb9++9ueff1rXrl0ttXvqqadcUAUAAAAAUqvMKd0ApKwjR45YenHXXXfZokWL7JlnnrGqVavajh07bMGCBe7fxNq8ebMdPXrUrr76aitevLilBfny5UvpJgAAAABAvMiMSOWmT5/uhg0cP37cfa+hFBpS0a9fv+A6d955p91+++3u6w8++MCqVatm2bJlszJlytiYMWNCtqdlQ4cOtfbt21vevHkjPunXvjp37myVK1d2nXHZvXu3devWzYoWLeqyDs477zzXtkjWr19vrVq1cuvmzp3bLrjgAvvmm29C1nn++eetQoUKbltar3Xr1sH33n//fatevbrlyJHDChUqZM2aNbMDBw6c8lx98skn9vDDD9tVV13ljrNOnTp23333uWPxHD582Hr37m0lS5a0XLly2YUXXmizZ8+OuD1lF6gdUrZsWXfeNaQlku+++84uvvhi12YN6ejRo0dIm9Wexx9/3J13nZPSpUu79m7fvt2dKy2rUaOG/fTTTyH717WfNm1a8Fw1b97ctmzZkqBhGk2aNHHt6dOnjxUsWNCKFStmgwcPDvlMQq4vAAAAAJwughGpnDq4+/btsyVLlrjv58yZY4ULFw7pQGuZOpyLFy+2Nm3a2C233GK//vqr63AOGDDgpJT90aNHW82aNd029b6fOus33XSTC3rMmzfPzjnnHDtx4oS1aNHC5s+fb2+++aatWLHCRo4caZkyZYrY5v3797uAwMyZM90+rrzySrvmmmuCgQ11uNU5fuyxx2z16tU2Y8YMu+SSS9x7W7dutbZt27oAwsqVK91x3nDDDRYIBE55rtTJ/vzzz935iouGXCxcuNCmTp1qv/zyiztWtW/t2rUnrXvzzTcHgyg//PCDa5sCDZGCL9rGjTfe6Lb5zjvvuOCE9uU3btw4a9iwoTsnyrRo166dC04okPTzzz+74SX63n+sGnoybNgwe/311935V9BA1zehXnvtNRd8UebIqFGj3Ln/+uuv3XsJvb7efaIhLP4XAAAAAEQrQyCaXh5SlJ7wq4OuJ/rXX3+9yzQYMmSIG36wZ88eO/vss23NmjUu+KAn7V999VXws3oa/tlnn9ny5cuDT+hr165tH330UXAdPe0/99xzXfBB21BHU0/FvXR/bU+dVQUHKlaseFL7FOzo2bOn6yjHRU/aNYxCHfQPP/zQOnXqZH/88YflyZMnZD11ynW8apOyBxJi7ty5dtttt9k///zjgi2NGjVyGRcKAIiCIcpw0L8lSpQIfk6ZF/Xq1bPhw4efdCwKyuh8bdy40Z27SJSZoo77hAkTgssUjFC9CmVHKNNAn1Vg6Y033nDv//33327Yh4JBCgzI999/b/Xr13dBDwVW1BadJy1XBoesWrXKqlSp4oIKanNcmRFqvzIqRIEqZbvo+nr02csuu8wFHU51fSPRfaJ7MNyguRsse+7Qa5qe9KtdOKWbAAAAAKRqelCpvqT6qsrGjwuZEWmAOrXKEFDcSB1KZQqoQ6oOr7Ii1LFWGr86k17H26Pv9dTfG+YhdevWjbgfBTzUeVbn1F93QB1yBTyi7agqM0KBE7VRwww0BEFt8zIjLr/8chdoUGBA2QFTpkxxGQCiIELTpk3d8AhlLbz00ku2a9euqPar7IoNGza4jAwFIRSAUQBAw1JE2SI6DzoOtcl76RwquyEaGgLjfU4deFm2bJkLHPi3qeEUyjhQEMOjYRgeDYcQbxiIf9m2bduCyzJnzuyCTx4NndE59c6nf58KpsTFv29RIMTbT0Kvr6hQqH65eK9TDR0BAAAAAD8KWKYBerL96quvuk5vlixZXIdUyxSgUEddwYqEULp+JBpaoTR9DWPQU3OP6iAkhAIRGgKg4SDly5d3n1dwwCuWqWwIZUCo/Qp8DBw40D1p//HHH11HW59V4Um9p2KUjzzyiMsEUPbGqej8KAChl2bAUJ0GZR7oawVJlMGg4SzhQxDUmY+GhoGooKX/vGi7qregoSfhNMzF3zaP6k/EtUxBjGgoCOWfjlX1IOLi34+3L28/Cb2+opokegEAAABAYhCMSEN1I1RzwAs8KBihFHsFIx588EG3TJkIGvfvp+/1xDu+8f+eu+++2w2nuPbaa93QDm9feqquIRUaChLN03PtU0MFNKTE66yHF37UE38Nj9Br0KBBLgjx7bffuqwPdZSV0aGXAhXKotCwkl69ellCaVaNY8eO2aFDh9xwC2VGKCNA5zQxIg0dOf/8812dBQVekprarhob3pAM1djQEAxda53DpNhnQq8vAAAAAJwughFpQIECBVyHUcMZnn322eCQBBWr1FN6L2igoIRS+jUsQcUXleGg9TVzRbQ0+4Q67C1btrQvvvjC1V3Q9rU/FWgcO3as6wCrdoGCBircGE5DRlQXQkUrtY7qIvif9qsehYZTaJs6NmUb6P1KlSq5DAgNs7jiiivsrLPOct+rDoY636eiAI2GmmgYimbhUIBAs2tceumlbqySXqopoSKRmmVEwQltW/vT+VVRycRQ1sVFF13k6mGofoQyT7RvZXh41yuxlNGga/L000+74IP2oX3FVS8iMRJ6fQEAAADgdFEzIo1Qh1FBAnW4vZR8PfVXoUN14r0n9O+++66bKUIZDsoq0BAFZSkkhAo4qjihhm1ouIQ3ZagCHersa78qjOmvQ+GnDq2CDA0aNHABCdVPUNs8yoJQsEJDQRRkePHFF+3tt9929RgUMFAhSu1bT+kfffRRFzjw6jPER/vRrBEKZGi76sRrmc6JZ9KkSS4YocCNzpumwNTwEP9wioRSIEN1J5RZoIwLBTl07v1FMhMrZ86cLthx6623ukwRDSfRbB1JLSHXFwAAAABOF7NpAKlUNLOUpLaKucymAQAAAJzZ9jKbBgAAAAAASI0IRiDN8E9jGf7SlKcAAAAAgLSBApZIM/zTWIYrWbKkpTeq9ZHQeh8AAAAAkBYQjECakRxTZwIAAAAAYo9gBIAk06tmoXiL1AAAAACAUDMCAAAAAADEFMEIAAAAAAAQUwQjAAAAAABATBGMAAAAAAAAMUUwAgAAAAAAxBSzaQBIMmOX7bDsuY9YWtevduGUbgIAAACQrpEZAQAAAAAAYopgBAAAAAAAiCmCEQAAAAAAIKYIRgAAAAAAgJgiGAEAAAAAAGKKYARwmjp27GjXXXddqtnOqUyePNny58+f7PsBAAAAgLgwtSdwmp566ikLBALB75s0aWK1atWy8ePHW2p0880321VXXZXSzQAAAABwBiMYAZymfPnyWVqSI0cO9wIAAACAlMIwDaR7J06csFGjRln58uUtW7Zsds4559iwYcPce3379rWKFStazpw5rWzZsjZgwAA7evRo8LODBw92WQ4TJkywUqVKufXatGlje/bsiTi8Ql/PmTPHZUtkyJDBvTZt2mTHjx+3O+64w84991wXCKhUqZJbJ6G2bt1qV199tduGtvXWW29ZmTJlQrIwxo4da9WrV7dcuXK5Nt9zzz22f//+OIdpeMf4xhtvuG0puHLLLbfYvn37EnG2AQAAAODUyIxAute/f3976aWXbNy4cdaoUSPXoV+1apV7L0+ePK5zXqJECfv111+tS5cublmfPn2Cn1+3bp29++679umnn9revXtdUEEd/ClTppy0LwUY1qxZY+edd5499thjblmRIkVcQOTss8+29957zwoVKmQLFiywrl27WvHixV1wI1rt27e3f//912bPnm1ZsmSxXr162bZt20LWyZgxoz399NMuWLFhwwbXVh3P888/H+d2169fb9OmTbPp06fbrl27XJtGjhwZDNqEO3z4sHt5dF4AAAAAIFoEI5Cu6em+AgTPPvusdejQwS0rV66cC0rIo48+GlxXWQG9e/e2qVOnhgQjDh06ZK+//rqVLFnSff/MM8+47IQxY8ZYsWLFQvanrIKsWbO6DAr/e5kyZbIhQ4YEv1egYOHChS7IEW0wQgGUb775xn788UerW7euW/byyy9bhQoVQtbr2bNnyDE9/vjjdtddd8UbjFCwREEZBWKkXbt2NnPmzDiDESNGjAg5HgAAAABICIZpIF1buXKle4LftGnTiO+/88471rBhQxc4yJ07twtObN68OWQdDevwAhFSv35913lfvXp1gtry3HPPWZ06dVymhPY1ceLEk/blUdaF1vFe8+bNc/vLnDmznX/++cH1NPSkQIECIZ9VwELHqzYruKDAwo4dO+zgwYNxtk1BCy8QIcrYCM+4CM820VAV77Vly5YEnQsAAAAAZzaCEUjX4ivUqMyE2267zc0soeEJS5YssUceecSOHDmS5O1QtoWyLjTE46uvvrKlS5dap06d4tzXtdde69bxXl4mxKmoPkXLli2tRo0a9sEHH9jixYtdEETiOy4N+fBTrQsFXOKi2ht58+YNeQEAAABAtBimgXRNQxgUkNCQgzvvvDPkPdVtKF26tAtAeH7//feTtqHshb/++svVlZDvv//e1WVQEcpINExDBSv95s+fbw0aNHD1G/x1GuKiLAV/poJof8eOHXNBE2VYePUsVOPBo+CDgggaQqI2ioaCAAAAAEBqQjAC6Vr27NndjBmqAaEggYZkbN++3ZYvX+4CFQo0KGvhggsusM8++8w++uijiNtQvYnRo0e7Qo09evRwdR7C60X4hzwsWrTIZSloiEXBggXdvlR34ssvv3T1IjRzhWo/6OtoVa5c2Zo1a+YKX77wwgsum+HBBx90wRZlMnjDNjQbiOpaXHPNNS4I8uKLL57GGQQAAACApMcwDaR7mq5TnfaBAwdalSpV7Oabb3b1EDQU4oEHHrB7773XTW2pTAmtG04d/BtuuMEN57jiiivcEIj4ikFqOIYKVlatWtXVh1DAo1u3bm4b2veFF17oajj4sySipYBG0aJF7ZJLLrHrr78+OPuHAiZSs2ZNN7XnE0884Wb0UO0JFZsEAAAAgNQkQyAQCKR0I4DUavDgwW7KS9VtSI3++OMPK1WqVLBoZUpRxohmEhk0d4Nlzx06vCQt6le7cEo3AQAAAEiTvL6BCt3HV1uOYRpAGvLtt9/a/v37rXr16rZ161Y3/ETDQpQpAQAAAABpBcEIIA1RPYiHH37YNmzY4IZnqCimhmKEz4YBAAAAAKkZwzQAnDaGaQAAAABIyDANClgCAAAAAICYIhgBAAAAAABiipoRAJJMr5qF4k3FAgAAAAAhMwIAAAAAAMQUwQgAAAAAABBTBCMAAAAAAEBMEYwAAAAAAAAxRQFLAElm7LIdlj33EUtr+tUunNJNAAAAAM4oZEYAAAAAAICYIhgBAAAAAABiimAEAAAAAACIKYIRAAAAAAAgpghGAAAAAACAmCIYAQAAAAAAYopgRCrUpEkT69mzZ6rZTrQGDx5stWrVSlVtSg5JdQzz58+36tWrW5YsWey6666z5LJp0ybLkCGDLV261H0/e/Zs9/3u3buTbZ8AAAAAEJ/M8b6LNEGdy0svvdR27dpl+fPnDy7/8MMPXUc3NUnqNpUpU8Z+//13W7hwoV100UXB5QoWqPOtcxNth/3cc8+1JUuWnDKgklTH0KtXL7evL774wnLnzm3JpVSpUrZ161YrXLhwsu0DAAAAABKCzIgYO3LkSMz2VbBgQcuTJ4+lJsnRpuzZs1vfvn0trR3D+vXr7bLLLrOzzz47JIiU1DJlymTFihWzzJmJPQIAAABIHQhGxCCl/95773VP6vVkunnz5vbbb79ZixYt3NPwokWLWrt27ezff/+NcxtvvPGG1a1b13WA1am89dZbbdu2bcEn+sqKkAIFCrj0+44dO0YcTqDMifbt27v1cubM6dqwdu3a4PuTJ092neIvv/zSqlSp4tp35ZVXuqfqHmUa1KtXz3LlyuXWbdiwoctMCG+vMhby5ctnt9xyi+3bty/kfPjbpPWGDh1qbdu2ddssWbKkPffccwk6x127drXvv//ePv/88zjXOXHihD322GOu458tWzaXkTBjxozg+8qKkNq1a7tzqHbGJdIxDB8+3Dp37uyu0TnnnGMTJ0485bCJHTt2uM/oa53748eP2x133OHakiNHDqtUqZI99dRTIZ/VtdWQDu1P946ugY7r2LFj9tBDD7lAiY5x0qRJcQ7T8Dtw4IDlzZvX3n///ZDl06ZNc9fDf+0AAAAAIKkQjIiB1157zbJmzepqBIwcOdI9DVen96effnId4n/++cfatGkT5+ePHj3qOuzLli1znUR1Lr2Ag1LwP/jgA/f16tWrXeAgvAPr0We0z08++cQNawgEAnbVVVe57XsOHjxoo0ePdgGFuXPn2ubNm613797uPXV41RFu3Lix/fLLL24bCgSoo+t/2q82Tp8+3b3mzJnjjjk+Tz75pNWsWdMNkejXr5/df//99vXXX0d9ftV5v+uuu6x///4u6BCJzsmYMWPcsantCgpde+21wWDMDz/84P795ptv3DnUUIyE0LYVMNIx3HPPPXb33Xe76xHfsAkFAcaPH+++vvnmm13bFUh47733bMWKFTZw4EB7+OGH7d133w35/Lfffmt//fWXuz5jx461QYMGWcuWLV2QadGiRe5cdOvWzf74449TtlsBBwWM/MEL0fetW7eOMwPk8OHDtnfv3pAXAAAAAESLvO0YqFChgo0aNcp9/fjjj7tAhJ5se1599VXXQV2zZo1VrFjxpM/r6bmnbNmy9vTTT9sFF1xg+/fvd9kLehouZ511Vpzp/up0KwihgEiDBg3csilTprj9Knhw0003uWUKTLz44otWrlw5972yOvTkXdTh3LNnj+v4eu8rg8JPHWo95fc6scr6mDlzpg0bNizO86PsCgUhRMevNo4bN84uv/xyi9ajjz7qOtA6Ju0znIIQGsqhjrc88cQTNmvWLBcMUCZGkSJF3PJChQq57JOEUlBHQQjRftR+bV/ZDXENm1AQR9kj/v0NGTIkJMiigI+CEf5gla637oGMGTO67eveUhBJgQtRUEYBoO+++y54vPG588473T2hoEjx4sVd1o2yTBSYicuIESNC2goAAAAACUFmRAzUqVMn+LWyG9RJVRDBe1WuXDmYVRDJ4sWL7ZprrnHp/+rkKzNBlLUQrZUrV7qaARdeeGFwmTre6szqPY+Gb3iBBvE6p14nWNkVyipQe5Rt4B/C4Q1Z8D9N938+LvXr1z/pe3+boqFggjI4lE0QXpdDQRRlEijo4afv49vPvHnzQq6TAh1xqVGjRvBrBRkUYPCO2xuSo1e1atXiPQ4FRnS/6Hi0voZ7hF9nbUOBCI+Ga2hWDn+wQ9f2VOfdo2E32qYyeOTNN9+00qVL2yWXXBLnZxTwUGDKe23ZsiWqfQEAAACAEIyIAaXCe5TNoI68xu/7X8pciNT505h+df6V0q/O8I8//mgfffRRshXDDJ8lQh1rDefwKPtAT+v1JP2dd95xmQyq1xDf5+MaOpHUNDvFf//9Z88//3ySbE/DLvzXSMM64hLfcb/88svBbcRX12Lq1KkuoKK6EV999ZVbv1OnTidd50j7Ot3zruwIZbR411j79Q+/Cae6G7on/S8AAAAAiBbDNGLs/PPPdzUelEEQzewGq1atcoUOlXavIRWiug9+qkchKoAYFw2nUM0H1RTwhmlou6prULVq1QQdg4aZ6KWn48pieOutt0Km1UwofzDD+z58+Ec0lEkwYMAAGzx4cEjgQB3lEiVKuOEfXlaJ6HtlBcR1DlVEsnz58na6VJQzGt4QGm+4R3zZMknt9ttvtz59+rjhH6pX0aFDh5jsFwAAAMCZicyIGOvevbvt3LnTzR6hLAd1NjV7hZ5ERwomaGiGOsrPPPOMbdiwwdV9UDFLP6XU6ym2CkZu377dZV9EqlvRqlUr69Kli6sloOEi6oCqo6zl0di4caMLQCgzQjNo6Om9MjoSEzgI74Sr7oFqZmiYggo4qohlYqigpuowKEDip5kmVCdC2RwKwKhGhTIPvP2o3oaCD15BUQ09iDVdIwWadD/oXCiwonskFlT88oYbbnDn6YorrnCFNAEAAAAguRCMiDHvCb0CD+r0aay/polU4Ul/HQCPagcofV4ddGUwKENCxRj9FFBQMUF1sFU/QEUnI1H6veoRqAClMho0/ELDBsJT/OOiehLK1Ljxxhvd8Ax1/BVc0cwNp+PBBx90nXBlW6jAp2aI0NCUxNCxKFhz6NChkOU9evRwwzi0L51zBR0U2FEAQJSloqyACRMmuGsUbYAmKek8KiCgmTVU20OZK/4sieSm4SEaEuIvmAoAAAAAySFDwF8QAIgxDVdRMEYvpCxN5/rAAw+4Yp/esJVoqUioMlIGzd1g2XNHng40NetXu3BKNwEAAABIF7y+gbLN46stR80I4AynaUE1K4qybpSdkdBABAAAAAAkFMM0kGpp9hD/1Jr+16mmyET0VK9D08tqOlLVBAEAAACA5MYwDaRa+/btc8Uk46oNocKdSB0YpgEAAABAGKaBNC9PnjzuBQAAAABIXwhGAEgyvWoWijf6CQAAAABCzQgAAAAAABBTBCMAAAAAAEBMEYwAAAAAAAAxRTACAAAAAADEFMEIAAAAAAAQU8ymASDJjF22w7LnPmJpSb/ahVO6CQAAAMAZh8wIAAAAAAAQUwQjAAAAAABATBGMAAAAAAAAMUUwAgAAAAAAxBTBCAAAAAAAEFMEI3BaZs+ebRkyZLDdu3dbWvf333/b5Zdfbrly5bL8+fNbatSkSRPr2bPnGXl9AAAAAKQfBCNwWh3hBg0a2NatWy1fvnwxa4c6195L+23YsKF9++23p73dcePGuWNZunSprVmzxlKjDz/80IYOHZroz0+ePDnVBloAAAAAnDkIRuC0ZM2a1YoVK+YCA7E0adIkFziYP3++FS5c2Fq2bGkbNmyIuO7Ro0ej2ub69eutTp06VqFCBTvrrLMsNSpYsKDlyZMnpZsBAAAAAKeFYESMHD582Hr06OE6udmzZ7dGjRrZjz/+GHx/+fLlrkOdN29e19m8+OKLXefY8+qrr1q1atUsW7ZsVrx4cbv33nvd8k2bNrlAgJ7me5SSr2VK0fen6n/22WdWo0YNt/+LLrrIfvvtt+BnduzYYW3btrWSJUtazpw5rXr16vb2228H3+/YsaPNmTPHnnrqqWBWgvYdaRjABx98EGxrmTJlbMyYMSHnQsuGDx9unTt3dsd6zjnn2MSJExN0PvV0X0GQ8847z1544QX777//7Ouvv3bvqT1adu2117ohF8OGDXPLtaxcuXIugFKpUiV74403Qtqkdr/++uvu8zreSJ5//nkXrNA5LFq0qLVu3Tokc0TXRS9lbChIMmDAAAsEAiH3Qe/evd15VtsuvPDC4HXyKMCibek6FChQwJo3b267du2KmJ2iY6hbt647jzoft956q23bti1i27WfTp062Z49e4LXcPDgwfbYY4+58xiuVq1arv0AAAAAkNQIRsRInz59XGf3tddes59//tnKly/vOpk7d+60P//80y655BLXeddwg8WLF7uO+rFjx4Kd6O7du1vXrl3t119/tU8++cR9PqEeeughFxhQEKRIkSJ2zTXXBLMGDh065LICFLBQkEL7ateunf3www/ufQUh6tevb126dHEZCXqVKlXqpH2o7W3atLFbbrnFtVWdXXVoNTzAT+1QJ3rJkiV2zz332N13322rV69O1LnNkSOH+/fIkSPBZdrv9ddf79qgc/nRRx/Z/fffbw8++KA7vm7durmO+axZs9z6OidXXnmla7uOTccb7qeffnIBJXXe1dYZM2a46+an65s5c2Z33rSNsWPH2ssvvxx8X4GKhQsX2tSpU+2XX36xm266ye137dq17n0FlZo2bWpVq1Z163333XfuOh0/fjzisev6adjGsmXLbNq0aS5AFFcgRUNqxo8f7wJe3jVUYETnZ+XKlSHBMV0XtU/nKBIFVfbu3RvyAgAAAIBoZY56TSTagQMHXEBBHfIWLVq4ZS+99JJ7kv/KK6+4p956kq4OapYsWdz7FStWDH7+8ccfd51odaY9F1xwQYLbMWjQIFeg0es0n3322a6Trg64ntSrY+q577777Msvv7R3333X6tWr59qnjAI9rdcT+Lio863OtPdEXcexYsUKe/LJJ0M6yVdddZULQkjfvn1dvQYFBpSxkBAHDx60Rx991DJlymSNGzcOLleGgL8jrawP7d/bZ69evez777+30aNH26WXXuqCMwoGKbAR1/Ft3rzZZTMog0WZCKVLl7batWuHrKMAjY5FWQc6FgVD9L2COPq8hpfo3xIlSrj1dc4V1NByZYuMGjXKBWmUgeFRlklcFEjwlC1b1p5++ml3b+zfv99y584dsq6un66j2uY/Rq2nwJja4N1X+lrnU9uMZMSIETZkyJA42wUAAAAA8SEzIgY03EJPsFVo0aOggzr5eiKtp+EaluEFIvyUcv/XX3+5Dv7pUmaDv/aAOsvav+jJu56wa3iG3lMHVcEIdZwTQtvzH6foez359z/d13ARj9c5jmt4QSQKLqiNCgoo40RBHf821aGPpl3e8YebMmWK2773mjdvngvkKAChDrqyRrSOgiF+Gv7ir5+hc+4duwIT+lcBGv+2NfzFG5LjZUZES5koypzQUBedCy8gk9DrpmCJhuUoQ0YZJm+99VZIoCNc//793XAP77Vly5YE7Q8AAADAmY3MiFTAG2aQ0PckY8b/xZP8dQmiLdjop8wFDStQGr8CEsoAUG0C/9CHpBQeeFEH/sSJE1F/XtkGzZo1c0/6ldUQTu0/Hao3oXoOHmWO6FpoiI1qL3z11Vc2cOBANxxEwxuimaFC2QrK4FAAQf/6eVkMp7re4Rk3ymjQS4ERnQcFIfR9Qq+bAhrKDFGmjDIodA/562GE07p6AQAAAEBikBkRA17RRBUm9Kizp06sagPoib6evEcKIuhpt4orzpw5M+K2vY64xv97/MUs/TQswaOhIZq+skqVKu57ta1Vq1Z2++23W82aNd3T//DpLXUMcdUu8Gh7/uP0tq1sgPAO+OlQJoXqZkQKRCSkXTr/kei8a/veywsSqB6EgiAaTqGaCqrR4J9WdNGiRSedcxW81LFrSIfOnzJA/NvWyxs2oXshrmsdbtWqVa7w6MiRI11mTeXKlU+ZXRLXNdRxdejQwQ3P0Es1PxISGAEAAACAhCAzIgb0lF4FGlVAUkMglFKvzqxS/O+44w6XEfDMM8+4DqDS3/W0X51YDePQUAo9fb/rrrvcTByqObFv3z7XkVZdB3UYNTRAHdJzzz3XdUZVQyESFV4sVKiQmwXikUcecbM9XHfdde49dZjff/99W7BggZvBQbUf/vnnn5DOuoIi6myrA64n+TqWcKptoboDGvJx8803uyKMzz77bEgNhJSgc6/aGAoIKJjw6aef2ocffmjffPNN1NuYPn26mz5URSt1jj7//HN37fx1LpSZoHoUKpCpLApdV282EQVkbrvtNmvfvr1bprZs377dBR8UhLj66qvd9Vdmimpb6JoreKBaGip0qevlp/tI72sfWleFOXXe46NrqAwN7VNBJ9UA0UvuvPPOkOAUAAAAACQXMiNiRMGCG2+80dUaOP/8823dunWuJoM6tQoQ6Om6Ooka869ZLVTg0hvKoCfWGj6hDr2KGaqAojf7gjftp2be0Oc0tEIFL+Nqg4pgar2///7bdcjVmRUFMNQupfhr+kg9qfcCFR4VW9QTfgUovCEB4bQNFb1UMU5NF6mhDAqCxDXDQ6zoWDQMRQUrdQ4nTJjgMgB0rNHSUAwFMC677DLXaX/xxRddnQV/gUkFGjTNqAJJmgFF51szk3i0T62joI2CGGqXMmQUWPACFhoCotkxtA3VnPj4449d5kI4XQMVRX3vvffcNdH11fHFRzNqKHChQJE+r6CYRwEpva8MC/8QFQAAAABIahkC/mIDSJdU40AzRmhoRjS1DZA4CmzUqlXLBY7SIv0qUEBCWRnK7kgITe2pjJ5BczdY9tx5LC3pVzs04wQAAABA4nl9AxW6z5s3b5zrMUwDgBsuomwWZcz4p0QFAAAAgOTAMA2kKsOHDw+Z9tL/Ur0MJA/VI9FwmokTJ7qhQwAAAACQnBimgVRl586d7hWJinVqik2kPgzTAAAAACAM00CapBk6Is3SAQAAAABIPwhGAEgyvWoWijf6CQAAAABCzQgAAAAAABBTBCMAAAAAAEBMEYwAAAAAAAAxRTACAAAAAADEFMEIAAAAAAAQU8ymASDJjF22w7LnPmJpRb/ahVO6CQAAAMAZicwIAAAAAAAQUwQjAAAAAABATBGMAAAAAAAAMUUwAgAAAAAAxBTBCAAAAAAAEFMEIwAAAAAAQEwRjIAze/Zsy5Ahg+3evdvONE2aNLGePXvGZF/Tpk2z8uXLW6ZMmdw+J0+ebPnz54/qs9Gs27FjR7vuuuuSqLUAAAAAkDwyJ9N2kco737Vq1bLx48cHlzVo0MC2bt1q+fLli1k7FPwI17BhQ/vuu+8slj788EPLkiVLTPbVrVs369Spk/Xo0cPy5MljmTNntquuuirJtv/UU09ZIBBIsu0BAAAAQHIgGAEna9asVqxYsZjvd9KkSXbllVeGtCMxjhw5kujPFixY0GJh//79tm3bNmvevLmVKFEiuDxHjhxJto9YBpMAAAAAILEYppFAhw8fdk+1zzrrLMuePbs1atTIfvzxx+D7y5cvt5YtW1revHndk++LL77Y1q9fH3z/1VdftWrVqlm2bNmsePHidu+997rlmzZtcpkCS5cuDa6rIRNapiEU/qEUn332mdWoUcPt/6KLLrLffvst+JkdO3ZY27ZtrWTJkpYzZ06rXr26vf322yFp/HPmzHFP0LUtvbTvSMM0Pvjgg2Bby5QpY2PGjAk5F1o2fPhw69y5szvWc845xyZOnJig86lhBwqCeC8FBk51DF52h86dhjoULlzYdfC9Y/jyyy+tdu3arpN/2WWXuQDAF198YVWqVHHX5dZbb7WDBw/GOUwjmuNasGCByy7RNahbt64bfhF+/fzUNm1L1CbvuoYPvVi2bJldeumlbl21tU6dOvbTTz+FbEvHp2PJnTu3C+QooyWuYRo6Nt2vffr0cedW53jw4MEh21u1apW7j3UsVatWtW+++ca1T8cU38/B3r17Q14AAAAAEC2CEQmkTp066a+99pr9/PPPbvy/OsI7d+60P//80y655BLXef/2229t8eLFrkN77Ngx99kXXnjBunfvbl27drVff/3VPvnkE/f5hHrooYdcYEBBkCJFitg111xjR48ede8dOnTIdWAVsFCQQvtq166d/fDDD+59BSHq169vXbp0cZ1YvUqVKnXSPtT2Nm3a2C233OLaqg7sgAEDXOfZT+1QZ3zJkiV2zz332N13322rV6+203GqY/DoGigbYv78+fbiiy8Gl6utzz77rAsYbNmyxR2HhqS89dZbbptfffWVPfPMM/G2Ib7jUsdb51xBEt0DQ4cOtb59+8a7PQ2D8T6v+0fnXcvC3XbbbXb22We7a6tr0K9fv5AhJAqijB492t544w2bO3eubd682Xr37h3vvnWecuXKZYsWLbJRo0bZY489Zl9//bV77/jx4y54oaCP3lfQ5ZFHHrFTGTFihMvC8F6R7iEAAAAAiAvDNBLgwIEDLqCgDnmLFi3cspdeesl17F555RXbtWuX65hNnTo12IGsWLFi8POPP/64Pfjgg3b//fcHl11wwQUJbsegQYPs8ssvD3Y01Xn96KOPXKdb2QT+zul9993nnqS/++67Vq9ePdc+deDV+YxvWMbYsWOtadOmLgDhHceKFSvsySefdE/fPap3oM66qEM+btw4mzVrllWqVCmqY1EGhIo5et58803XOY7vGDwVKlRwnWuPlyGg86zaE3LHHXdY//79XXZK2bJl3bLWrVu7NsYXQIjvuBTUUOaArr2XTaBAlAI8cdE5VzaNeBkKkSi4oGBT5cqVg8fop6CTAi/lypVz3ys7RMGF+CiLRveMtz0FambOnOnuId27OjfK0vDaNGzYsOD9FRed0169egW/V4CGgAQAAACAaBGMSAB12tQZ9Dq6oqCDOsgrV660v//+2w3LiFQMUUMF/vrrL9fBP13KbPCoY6sOsvbvPenWEAN13NVBVi0FpdQr+JAQ2l6rVq1Clum4lWGgfXgBBHV0Peqgq0OrY42WOvnNmjULfq+hK9Eeg7InIvG3qWjRou5zXiDCWxaeZRHfNsKPSxkO3jAZjz9IIhre8vvvv7uvdU9omEg01MG/8847XeaDzstNN90UDDyIjsX/vc7Xqc63/1jCP6NjURDBHxwJP5ZIlP2jFwAAAAAkBsGIJBRfIcJTFSnMmPF/I2b8MyF4Qy8SQpkLGoqhoIGGESg9X/UQ1KFPDuGBF3XcT5w4EfXn1QkOH6oycuTIqI5By0/VJrUnMW083eP6/PPPg9cvIQUqNcRENS00nEQBDGU0KNPm+uuvj7Ndp5o943SPBQAAAACSGjUjEkBPpL0aBR51ODW+X6n6egI9b968iEEEFSRUYUSlx0ei2g/iL0YYVzHE77//Pvi1hoasWbPGFTQUtU0ZDbfffrvVrFnTZQTofT8dg7IP4qPt+Y/T27aGa/iHVSSHaI4hJSkTRXU0lK3h8RcxldKlS7sgi14aOpMQOscPPPCAq21xww03uBlHkvNYVFfjn3/+ifNYAAAAACCpEYxIAD2JVyFDjemfMWOGq6GgOgEqKqjaBBq/r7HzKvqoGRDWrl3r0u29woV66q3CiE8//bR7T8UPvUKKenqumTGUFaAhEprx4tFHH43YDtUIUFBDxR1Vv0GzSXgzKKgmgOoAqHijttOtW7eQjqYoKKJihZpF499//434lFy1LbQPFWdUIEC1KVRr4FTFEpNCNMeQkpS5oHOmwppqn+pZqKikl3WQWP/995+7h1S/QUM8FJRRYMALNCUH1YZQkK1Dhw72yy+/uH16993pHAsAAAAAxIdgRAIpWHDjjTe62R3OP/98W7duneuMFihQwAoVKuRm0di/f781btzY1TRQkUMvTV4dPg09eP75511NAU0BqqCEf9pPzbyhz2lYggoxxtUGFcHUeqpT8emnn7psB1FHUu3SDB+a1lHDIPxTPYoCCspuUDaHMjJUNDGctqGaDRoicN5559nAgQNdEMRfvDK5RHMMKUlTbuqcK3NF03tq9gmdH/HXkUgoXRNNa9q+fXuXHaGCpCqUOmTIkCRs/cn71BSeumdVTFX1KrzZNE7nWAAAAAAgPhkCpxpwjlRDT8wvvfRSNzQjf/78Kd0c+EyZMsU6depke/bsSVCNiNRI2RGNGjVygTZ/scz4KCNIM7UMmrvBsufOY2lFv9qFU7oJAAAAQLri9Q3UN9KD3LhQwBJIhNdff93VslA9iGXLlrnpP5XJkBYDEZoWNnfu3G54jAIQyrrRzCnRBiIAAAAAIKEYpoFkoak51cGN9NLQg7ROw2NUYFP1HFRsUlNwTpw40dKiffv2Wffu3a1y5cpuGI6Ga3z88ccp3SwAAAAA6RjDNJAsdu7c6V6RKHsgoTNMIHVjmAYAAAAAYZgGUlTBggXdCwAAAACAcAQjACSZXjULxRv9BAAAAAChZgQAAAAAAIgpghEAAAAAACCmCEYAAAAAAICYIhgBAAAAAABiimAEAAAAAACIKYIRAAAAAAAgpghGAAAAAACAmCIYAQAAAAAAYopgBAAAAAAAiCmCEQAAAAAAIKYIRgAAAAAAgJgiGAEAAAAAAGKKYAQAAAAAAIgpghEAAAAAACCmCEYAAAAAAICYIhgBAAAAAABiimAEAAAAAACIKYIRAAAAAAAgpghGAAAAAACAmMoc290BSI8CgYD7d+/evSndFAAAAAApyOsTeH2EuBCMAHDaduzY4f4tVapUSjcFAAAAQCqwb98+y5cvX5zvE4wAcNoKFizo/t28eXO8v3Bw5kTDFZjasmWL5c2bN6WbgxTG/QA/7gf4cT/Aj/sh/VBGhAIRJUqUiHc9ghEATlvGjP8rP6NABP/zgEf3AvcDPNwP8ON+gB/3A/y4H9KHaB5QUsASAAAAAADEFMEIAAAAAAAQUwQjAJy2bNmy2aBBg9y/APcD/Lgf4Mf9AD/uB/hxP5x5MgRONd8GAAAAAABAEiIzAgAAAAAAxBTBCAAAAAAAEFMEIwAAAAAAQEwRjAAAAAAAADFFMALASZ577jkrU6aMZc+e3S688EL74Ycf4l3/vffes8qVK7v1q1evbp9//nnI+6qTO3DgQCtevLjlyJHDmjVrZmvXrk3mo0BqvR86duxoGTJkCHldeeWVyXwUSIn7Yfny5XbjjTe69XWdx48ff9rbRPq+HwYPHnzS7wf9PkH6vCdeeuklu/jii61AgQLupb8Pwtfnb4i0LanvB/6GSF8IRgAI8c4771ivXr3c1Eo///yz1axZ05o3b27btm2LuP6CBQusbdu2dscdd9iSJUvsuuuuc6/ffvstuM6oUaPs6aefthdffNEWLVpkuXLlcts8dOhQDI8MqeV+EP3hsHXr1uDr7bffjtERIZb3w8GDB61s2bI2cuRIK1asWJJsE+n7fpBq1aqF/H747rvvkvEokJL3xOzZs93/M2bNmmULFy60UqVK2RVXXGF//vlncB3+hki7kuN+EP6GSEc0tScAeOrVqxfo3r178Pvjx48HSpQoERgxYkTE9du0aRO4+uqrQ5ZdeOGFgW7durmvT5w4EShWrFjgySefDL6/e/fuQLZs2QJvv/12sh0HUuf9IB06dAi0atUqGVuN1HI/+JUuXTowbty4JN0m0t/9MGjQoEDNmjWTvK2IjdP9eT527FggT548gddee819z98QaVtS3w/C3xDpC5kRAIKOHDliixcvdmlxnowZM7rvFaGORMv964ui3t76GzdutL///jtknXz58rlUvbi2ifR7P/iffpx11llWqVIlu/vuu23Hjh3JdBRIyfshJbaJ2EjOa6cU/BIlSrgsittuu802b96cBC1GWrgnlD1z9OhRK1iwoPuevyHSruS4Hzz8DZF+EIwAEPTvv//a8ePHrWjRoiHL9b3+GIhEy+Nb3/s3IdtE+r0fvPTK119/3WbOnGlPPPGEzZkzx1q0aOH2hfR1P6TENhEbyXXt1MmcPHmyzZgxw1544QXXGdUY8n379iVBq5Ha74m+ffu6QJTXgeVviLQrOe4H4W+I9CVzSjcAAHBmueWWW4Jfq8BljRo1rFy5cu5JR9OmTVO0bQBSljoVHv1uUHCidOnS9u6777paNEi/VEtk6tSp7v8FKnaIM1tc9wN/Q6QvZEYACCpcuLBlypTJ/vnnn5Dl+j6uYmNaHt/63r8J2SbS7/0QiVKxta9169YlUcuRWu6HlNgmYiNW1y5//vxWsWJFfj+k83ti9OjRrvP51Vdfuc6lh78h0q7kuB8i4W+ItI1gBICgrFmzWp06dVzqm+fEiRPu+/r160f8jJb715evv/46uP65557r/qfjX2fv3r2uInZc20T6vR8i+eOPP9x4T03bhvR1P6TENhEbsbp2+/fvt/Xr1/P7IR3fE5otY+jQoW5oTt26dUPe42+ItCs57odI+BsijUvpCpoAUpepU6e6KtWTJ08OrFixItC1a9dA/vz5A3///bd7v127doF+/foF158/f34gc+bMgdGjRwdWrlzpKqFnyZIl8OuvvwbXGTlypNvGxx9/HPjll19cFeRzzz038N9//6XIMSLl7od9+/YFevfuHVi4cGFg48aNgW+++SZw/vnnBypUqBA4dOhQih0nkud+OHz4cGDJkiXuVbx4cXft9fXatWuj3ibOrPvhwQcfDMyePdv9ftDvk2bNmgUKFy4c2LZtW4ocI5L3ntDfB1mzZg28//77ga1btwZf+n+Ffx3+hkibkvp+4G+I9IdgBICTPPPMM4FzzjnH/Q9B0zJ9//33wfcaN27splXye/fddwMVK1Z061erVi3w2WefhbyvqbkGDBgQKFq0qPufUtOmTQOrV6+O2fEg9dwPBw8eDFxxxRWBIkWKuCCFpvfr0qULHc90ej/oj0U99wh/ab1ot4kz6364+eabXaBC2ytZsqT7ft26dTE/LsTmntD/AyLdEwpke/gbIm1LyvuBvyHSnwz6T0pnZwAAAAAAgDMHNSMAAAAAAEBMEYwAAAAAAAAxRTACAAAAAADEFMEIAAAAAAAQUwQjAAAAAABATBGMAAAAAAAAMUUwAgAAAAAAxBTBCAAAAAAAEFMEIwAAABCvJk2aWM+ePVO6GQCAdIRgBAAAwGno2LGjZciQ4aTXunXrkmT7kydPtvz581tK+vDDD23o0KGWWs2ePdud8927d6d0UwAAUcoc7YoAAACI7Morr7RJkyaFLCtSpIilNkePHrUsWbIk+HMFCxa01ErHBABIe8iMAAAAOE3ZsmWzYsWKhbwyZcrk3vv444/t/PPPt+zZs1vZsmVtyJAhduzYseBnx44da9WrV7dcuXJZqVKl7J577rH9+/cHn/h36tTJ9uzZE8y4GDx4sHtPX0+bNi2kHcqgUCaFbNq0ya3zzjvvWOPGjd3+p0yZ4t57+eWXrUqVKm5Z5cqV7fnnn0/QMI0yZcrY448/bu3bt7fcuXNb6dKl7ZNPPrHt27dbq1at3LIaNWrYTz/9dFKGh9pcoUIFt+/mzZvbli1bQvb1wgsvWLly5Sxr1qxWqVIle+ONN0Le1zFpnWuvvdadsy5dutill17q3itQoIB7X9kqMmPGDGvUqJHbb6FChaxly5a2fv364La8c6TMD20jZ86cVrNmTVu4cGHIPufPn+/Ogd7XPtTuXbt2ufdOnDhhI0aMsHPPPddy5MjhPv/+++/Hez4BAAQjAAAAks28efNch/3++++3FStW2IQJE1ynfNiwYcF1MmbMaE8//bQtX77cXnvtNfv222+tT58+7r0GDRrY+PHjLW/evLZ161b36t27d4La0K9fP7f/lStXuk60AhIDBw50bdCy4cOH24ABA9y+E2LcuHHWsGFDW7JkiV199dXWrl07d6y33367/fzzzy6goO8DgUDwMwcPHnT7ff31110HX8MqbrnlluD7H330kWvrgw8+aL/99pt169bNBWNmzZoVsm8FZK6//nr79ddfXXDngw8+cMtXr17tztFTTz3lvj9w4ID16tXLBUVmzpzpzrU+pwCC3yOPPOLO69KlS61ixYrWtm3bYMBIy5o2bWpVq1Z1QYrvvvvOrrnmGjt+/Lh7X4EIHc+LL77oruEDDzzgzsGcOXMSdD4B4IwTAAAAQKJ16NAhkClTpkCuXLmCr9atW7v3mjZtGhg+fHjI+m+88UagePHicW7vvffeCxQqVCj4/aRJkwL58uU7aT39GffRRx+FLNN6Wl82btzo1hk/fnzIOuXKlQu89dZbIcuGDh0aqF+/fpxtaty4ceD+++8Pfl+6dOnA7bffHvx+69atbl8DBgwILlu4cKFbpve849D333//fXCdlStXumWLFi1y3zdo0CDQpUuXkH3fdNNNgauuuirkuHv27BmyzqxZs9zyXbt2BeKzfft2t96vv/4aco5efvnl4DrLly93y9Q2adu2baBhw4YRt3fo0KFAzpw5AwsWLAhZfscdd7jPAQDiRs0IAACA06QUfw0d8Gj4gCxbtsxlAPgzIfRE/dChQy5LQGn/33zzjXu6vmrVKtu7d697Iu9//3TVrVs3+LUyBTRM4Y477nDDGzzaZ758+RK0XQ3D8BQtWtT9q+Em4cu2bdvmhq1I5syZ7YILLgiuoyEiGkKhDI169eq5f7t27RqyH2VfeJkOkY4pPmvXrnVZIIsWLbJ///03mBGxefNmO++88yIeS/HixYPtVvuUGXHTTTdF3L6KlOo6XX755SHLjxw5YrVr146qjQBwpiIYAQAAcJoUfChfvvxJy1X7QcMIbrjhhpPeU80E1SxQHYO7777bBSxUKFLDABQsUIc2vmCEah34h0DEVczRC4x47ZGXXnrJLrzwwpD1vBoX0fIXwlRb4loWPiQiKfiPKT4aTqF6FjreEiVKuLYoCKFz6xdfu1UHIi7e+fzss8+sZMmSJ9URAQDEjWAEAABAMlHhStUxiBSokMWLF7tO75gxY1w9A3n33XdD1lEhR68+QfhsHaqP4M8C0FP6+ChbQZ3yDRs22G233WaxpgwM1W9QFoTo3KhuhIppiv5VJkmHDh2Cn9H3qtcQH50j8Z+nHTt2uO0rEHHxxRe7ZQr0JJSyJlRvQkGlcGqXgg7KtFCRUABA9AhGAAAAJBMNEVDmwznnnGOtW7d2AQcN3VBxRs1GoSCFshmeeeYZ9xRfHW8VQvTTzBV6Aq8OsWZqULaEXpdddpk9++yzVr9+fdcJ79u3b1TTdqpT3aNHDzcsQ1OSHj582AUINDuEij0mJ7XvvvvucwU7NWTj3nvvtYsuuigYnHjooYesTZs2bohDs2bN7NNPP3UzXWgoS3yU/aCMhunTp9tVV13lshk064Vm0Jg4caIbeqGAgYp5JlT//v3d8BPNcnLXXXe5wIcKamroRuHChV3hSxWtVFBJM3do5hNdRxUd9QdVAAChmE0DAAAgmWj2CnWQv/rqK1crQR1vzUKhzrMouKCpPZ944gk3fEAzXah+hJ9m1FAn+Oabb3bZEKNGjXLLlU2hqUD11P/WW291neJoakzceeedbmrPSZMmuU62nuhrhg9NTZnc1D4FTdRe1YLQFKCaetRz3XXXufoQo0ePtmrVqrnZR9ROTasZHw2RUJBFwQZlfyjIocDP1KlTXfaJzq0CBk8++WSC26zZNXT9FERS0ETBH03XqmCKDB061M1GouumzA4FeDRsIxbnEwDSsgyqYpnSjQAAAED6poBHz5493bAMAADIjAAAAAAAADFFMAIAAAAAAMQUwzQAAAAAAEBMkRkBAAAAAABiimAEAAAAAACIKYIRAAAAAAAgpghGAAAAAACAmCIYAQAAAAAAYopgBAAAAAAAiCmCEQAAAAAAIKYIRgAAAAAAAIul/wf9rpg+XaX54QAAAABJRU5ErkJggg==",
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#Extract feature importance from the tuned RandomForest model\n",
        "feature_importance = best_rf_model.feature_importances_\n",
        "\n",
        "#Create a DataFrame for better visualization\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': X_train.columns,\n",
        "    'Importance': feature_importance\n",
        "})\n",
        "\n",
        "#Sort features by importance (descending order)\n",
        "feature_importance_df = feature_importance_df.sort_values(by=\"Importance\", ascending=False)\n",
        "\n",
        "#Display the DataFrame\n",
        "print(\"Feature importance - tuned Random Forest:\")\n",
        "print(feature_importance_df.head(15))\n",
        "\n",
        "#Plot feature importance\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(feature_importance_df[\"Feature\"][:15], feature_importance_df[\"Importance\"][:15], color='skyblue')\n",
        "plt.xlabel(\"Feature importance\")\n",
        "plt.ylabel(\"Features\")\n",
        "plt.title(\"Feature importance for tuned Random Forest model\")\n",
        "plt.gca().invert_yaxis() \n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This approach helps in selecting the optimal number of features for training the tuned Random Forest model based on feature importance. First, it calculates the cumulative importance of each feature and retains only those contributing to at least 95% of the total importance. The selected features are then used to train a new Random Forest model and its performance is evaluated using standard metrics such as MAE, MSE, RMSE and RÂ² score. The better performance of the new model suggests that eliminating less important features has no negative impact on the predictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of selected features: 28\n",
            "Selected features: ['age', 'fnlwgt', 'education-num', 'sex', 'income', 'relationship_Own-child', 'marital-status_Married-civ-spouse', 'occupation_Other-service', 'marital-status_Never-married', 'occupation_Exec-managerial', 'workclass_Self-emp-inc', 'capital-gain', 'relationship_Not-in-family', 'occupation_Prof-specialty', 'occupation_Farming-fishing', 'workclass_Self-emp-not-inc', 'marital-status_Widowed', 'relationship_Wife', 'workclass_Private', 'capital-loss', 'occupation_Sales', 'occupation_Transport-moving', 'race_White', 'native-country_United-States', 'occupation_Craft-repair', 'relationship_Unmarried', 'workclass_State-gov', 'race_Black']\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>MAE</th>\n",
              "      <th>MSE</th>\n",
              "      <th>RMSE</th>\n",
              "      <th>R2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>LinearRegression</th>\n",
              "      <td>0.635098</td>\n",
              "      <td>0.811954</td>\n",
              "      <td>0.901085</td>\n",
              "      <td>0.180752</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>SGDRegressor</th>\n",
              "      <td>0.637655</td>\n",
              "      <td>0.813540</td>\n",
              "      <td>0.901965</td>\n",
              "      <td>0.179151</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>HuberRegressor</th>\n",
              "      <td>0.614613</td>\n",
              "      <td>0.828097</td>\n",
              "      <td>0.909998</td>\n",
              "      <td>0.164464</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>DecisionTreeRegressor</th>\n",
              "      <td>0.813775</td>\n",
              "      <td>1.427130</td>\n",
              "      <td>1.194626</td>\n",
              "      <td>-0.439951</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>RandomForestRegressor</th>\n",
              "      <td>0.627742</td>\n",
              "      <td>0.792422</td>\n",
              "      <td>0.890181</td>\n",
              "      <td>0.200459</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>RidgeRegression</th>\n",
              "      <td>0.634998</td>\n",
              "      <td>0.811766</td>\n",
              "      <td>0.900981</td>\n",
              "      <td>0.180941</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>LassoRegression</th>\n",
              "      <td>0.613671</td>\n",
              "      <td>0.971518</td>\n",
              "      <td>0.985656</td>\n",
              "      <td>0.019755</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Tuned RandomForestRegressor</th>\n",
              "      <td>0.590542</td>\n",
              "      <td>0.736844</td>\n",
              "      <td>0.858396</td>\n",
              "      <td>0.256536</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Tuned RandomForestRegressor_SelectedFeatures(28)</th>\n",
              "      <td>0.592218</td>\n",
              "      <td>0.735400</td>\n",
              "      <td>0.857554</td>\n",
              "      <td>0.257994</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                       MAE       MSE  \\\n",
              "LinearRegression                                  0.635098  0.811954   \n",
              "SGDRegressor                                      0.637655  0.813540   \n",
              "HuberRegressor                                    0.614613  0.828097   \n",
              "DecisionTreeRegressor                             0.813775  1.427130   \n",
              "RandomForestRegressor                             0.627742  0.792422   \n",
              "RidgeRegression                                   0.634998  0.811766   \n",
              "LassoRegression                                   0.613671  0.971518   \n",
              "Tuned RandomForestRegressor                       0.590542  0.736844   \n",
              "Tuned RandomForestRegressor_SelectedFeatures(28)  0.592218  0.735400   \n",
              "\n",
              "                                                      RMSE        R2  \n",
              "LinearRegression                                  0.901085  0.180752  \n",
              "SGDRegressor                                      0.901965  0.179151  \n",
              "HuberRegressor                                    0.909998  0.164464  \n",
              "DecisionTreeRegressor                             1.194626 -0.439951  \n",
              "RandomForestRegressor                             0.890181  0.200459  \n",
              "RidgeRegression                                   0.900981  0.180941  \n",
              "LassoRegression                                   0.985656  0.019755  \n",
              "Tuned RandomForestRegressor                       0.858396  0.256536  \n",
              "Tuned RandomForestRegressor_SelectedFeatures(28)  0.857554  0.257994  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#Compute cumulative importance\n",
        "feature_importance_df[\"Cumulative_Importance\"] = feature_importance_df[\"Importance\"].cumsum()\n",
        "\n",
        "#Select features that contribute to 95% of the total importance\n",
        "selected_features = feature_importance_df[feature_importance_df[\"Cumulative_Importance\"] <= 0.95][\"Feature\"].tolist()\n",
        "\n",
        "#Print selected features and their count\n",
        "print(f\"Number of selected features: {len(selected_features)}\")\n",
        "print(f\"Selected features: {selected_features}\")\n",
        "\n",
        "# Train a new RandomForest model using only the selected features\n",
        "X_train_selected = X_train[selected_features]\n",
        "X_val_selected = X_val[selected_features]\n",
        "X_test_selected = X_test[selected_features]\n",
        "\n",
        "best_rf_model_selected = RandomForestRegressor(**best_params, random_state=42)\n",
        "best_rf_model_selected.fit(X_train_selected, y_train)\n",
        "\n",
        "#Predict on validation set\n",
        "y_val_pred_selected = best_rf_model_selected.predict(X_val_selected)\n",
        "\n",
        "#Compute evaluation metrics\n",
        "best_rf_mae_selected = mean_absolute_error(y_val, y_val_pred_selected)\n",
        "best_rf_mse_selected = mean_squared_error(y_val, y_val_pred_selected)\n",
        "best_rf_rmse_selected = np.sqrt(best_rf_mse_selected)\n",
        "best_rf_r2_selected = r2_score(y_val, y_val_pred_selected)\n",
        "\n",
        "\n",
        "#Save results in the same dictionary for consistency\n",
        "results[\"Tuned RandomForestRegressor_SelectedFeatures(28)\"] = {\n",
        "    \"MAE\": best_rf_mae_selected,\n",
        "    \"MSE\": best_rf_mse_selected,\n",
        "    \"RMSE\": best_rf_rmse_selected,\n",
        "    \"R2\": best_rf_r2_selected\n",
        "}\n",
        "\n",
        "# Convert results to DataFrame\n",
        "results_df = pd.DataFrame(results).T\n",
        "\n",
        "#Display results\n",
        "display(results_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Final report**\n",
        "\n",
        "The objective of this project was to build regression models to predict census-related variables. Below are the key findings:\n",
        "\n",
        "\n",
        "### **1. Key findings from model performance**\n",
        "\n",
        "1.1 **Linear models (Linear Regression, Ridge, Lasso) provide a baseline but have limited predictive power**\n",
        "   - Linear regression achieved an **RÂ² score of 0.1808**, explaining only ~18% of the variance.\n",
        "   - Ridge Regression performed similarly, indicating that multicollinearity wasnât a major issue.\n",
        "   - Lasso Regression underperformed (**RÂ² = 0.0198**).\n",
        "\n",
        "1.2 **Gradient descent-based models (SGDRegressor, HuberRegressor) show marginal differences**\n",
        "   - **SGDRegressor** performed similarly to Linear Regression but scales better for large datasets.\n",
        "   - **HuberRegressor**, designed for robustness against outliers, showed a slight improvement in MAE but higher MSE.\n",
        "\n",
        "1.3 **Tree-based models perform significantly better**\n",
        "   - **Decision Tree Regressor overfits significantly**, with **negative RÂ² (-0.4399)**, meaning it fails to generalize.\n",
        "   - **Random Forest Regression improves generalization**, achieving **RÂ² = 0.2005**.\n",
        "\n",
        "1.4 **Hyperparameter tuning enhances model performance**\n",
        "   - Fine-tuning the Random Forest model led to **RÂ² improvement from 0.2005 to 0.2565**.\n",
        "   - **Feature selection further improved performance**, lowering MSE from **0.7368 to 0.7354** and increasing RÂ² to **0.2580**.\n",
        "\n",
        "1.5 **Feature selection impact**\n",
        "\n",
        "   - By selecting the top 28 most important features, the tuned modelâs RÂ² slightly improved to 0.2580 and MSE decreased to 0.7354.\n",
        "   - This suggests that removing irrelevant features improved performance marginally while reducing complexity.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Model Performance Comparison**\n",
        "\n",
        "| Model                                      | MAE      | MSE      | RMSE     | RÂ²       |\n",
        "|--------------------------------------------|---------|---------|---------|---------|\n",
        "| **LinearRegression**                       | 0.635098 | 0.811954 | 0.901085 | 0.180752 |\n",
        "| **SGDRegressor**                           | 0.637655 | 0.813540 | 0.901965 | 0.179151 |\n",
        "| **HuberRegressor**                         | 0.614613 | 0.828097 | 0.909998 | 0.164464 |\n",
        "| **DecisionTreeRegressor**                  | 0.813775 | 1.427130 | 1.194626 | -0.439951 |\n",
        "| **RandomForestRegressor**                  | 0.627742 | 0.792422 | 0.890181 | 0.200459 |\n",
        "| **RidgeRegression**                        | 0.634998 | 0.811766 | 0.900981 | 0.180941 |\n",
        "| **LassoRegression**                        | 0.613671 | 0.971518 | 0.985656 | 0.019755 |\n",
        "| **Tuned RandomForestRegressor**            | 0.590542 | 0.736844 | 0.858396 | 0.256536 |\n",
        "| **Tuned RandomForestRegressor_Selected(28)** | 0.592218 | 0.735400 | 0.857554 | 0.257994 |\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Loss function and model selection**  \n",
        "\n",
        "The choice of loss function was essential in optimizing model performance. **Mean Squared Error (MSE)** was primarily used due to its smooth optimization properties, despite its sensitivity to outliers. **Root Mean Squared Error (RMSE)** was preferred for its interpretability, as it maintains the same unit as the target variable, making error magnitude easier to understand. **Mean Absolute Error (MAE)** was also considered, as it is more robust to outliers than MSE.  \n",
        "\n",
        "For model selection, **Linear Regression and Ridge/Lasso Regression** served as baselines but struggled with capturing complex relationships. **SGDRegressor** was included for its computational efficiency in large datasets. **Decision Trees captured non-linearity but overfitted easily**, whereas **Random Forest mitigated overfitting and generalized better**. The **Tuned Random Forest Regressor with feature selection** emerged as the best-performing model.\n",
        "\n",
        "---\n",
        "\n",
        "## **4. Conclusion**\n",
        "\n",
        "### **Best Model Choice**\n",
        " The **Tuned Random Forest Regressor with feature selection (28 features)** had:\n",
        "\n",
        "  - Highest RÂ² (0.2580)\n",
        "  - Lowest MSE (0.7354)\n",
        "  - Reduced complexity compared to using all features\n",
        "\n",
        "\n",
        "   The results show that **tree-based models outperform linear models** in predicting census-related variables. While **Linear Regression, Ridge, and Lasso** provided a baseline, they failed to capture complex relationships. **Random Forest Regression significantly improved performance**, demonstrating the power of ensemble learning. **Hyperparameter tuning further enhanced accuracy**, reducing errors and increasing RÂ². Feature selection **slightly improved efficiency but had minimal impact on accuracy**. Overall, the **Tuned Random Forest Regressor with selected features performed best**, balancing accuracy and complexity. Notably, **polynomial features, while beneficial for linear models (e.g., Linear Regression, Ridge, Lasso), are ineffective for Random Forest**, as decision trees inherently capture non-linear patterns through recursive splits without requiring feature transformations. Future improvements could focus on boosting models (XGBoost, LightGBM), deep learning approaches, and interpretability techniques (e.g., SHAP, LIME) to further optimize predictive performance.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "env_ml",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
