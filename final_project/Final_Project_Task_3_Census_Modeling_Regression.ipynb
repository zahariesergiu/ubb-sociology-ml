{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydHb_ZL5yy6f"
      },
      "source": [
        "# **Final Project Task 3 - Census Modeling Regression**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnzXS8Oo9jwY"
      },
      "source": [
        "Requirements\n",
        "- Create a regression model on the Census dataset, with 'hours-per-week' target\n",
        "\n",
        "- You can use models (estmators) from sklearn, but feel free to use any library for traditional ML. \n",
        "    - Note: in sklearn, the LinearRegression estimator is based on OLS, a statistical method. Please use the SGDRegressor estimator, since this is based on gradient descent. \n",
        "    - You can use LinearRegression estimator, but only as comparison with the SGDRegressor - Optional.\n",
        "\n",
        "- Model Selection and Setup **2p**:\n",
        "    - Implement multiple models, to solve a regression problem using traditional ML: \n",
        "        - Linear Regression\n",
        "        - Decision Tree Regression\n",
        "        - Random Forest Regression - Optional\n",
        "        - Ridge Regression - Optional\n",
        "        - Lasso Regression - Optional\n",
        "    - Choose a loss (or experiment with different losses) for the model and justify the choice. *1p*\n",
        "        - MSE, MAE, RMSE, Huber Loss or others\n",
        "    - Justify model choices based on dataset characteristics and task requirements; specify model pros and cons. *1p*\n",
        "\n",
        "\n",
        "- Data Preparation\n",
        "    - Use the preprocessed datasets from Task 1.\n",
        "    - From the train set, create an extra validation set, if necesarry. So in total there will be: train, validation and test datasets.\n",
        "    - Be sure all models have their data preprocessed as needed. Some models require different, or no encoding for some features.\n",
        "\n",
        "\n",
        "- Model Training and Experimentation **8p**\n",
        "    - Establish a Baseline Model *2p*\n",
        "        - For each model type, train a simple model with default settings as a baseline.\n",
        "        - Evaluate its performance to establish a benchmark for comparison.\n",
        "    - Make plots with train, validation loss and metric on epochs (or on steps), if applicable. - Optional\n",
        "    - Feature Selection: - Optional\n",
        "        - Use insights from EDA in Task 2 to identify candidate features by analyzing patterns, relationships, and distributions.\n",
        "    - Experimentation: *6p*\n",
        "        - For each baseline model type, iteratively experiment with different combinations of features and transformations.\n",
        "        - Experiment with feature engineering techniques such as interaction terms, polynomial features, or scaling transformations.\n",
        "        - Identify the best model which have the best performance metrics on test set.\n",
        "        - You may need multiple preprocessed datasets preprocessed\n",
        "- Hyperparameter Tuning **2p**\n",
        "  - Perform hyperparameter tuning only on the best-performing model after evaluating all model types and experiments. *2p*\n",
        "  - Consider using techniques like Grid Search for exhaustive tuning, Random Search for quicker exploration, or Bayesian Optimization for an intelligent, efficient search of hyperparameters.\n",
        "  - Avoid tuning models that do not show strong baseline performance or are unlikely to outperform others based on experimentation.\n",
        "  - Ensure that hyperparameter tuning is done after completing feature selection, baseline modeling, and experimentation, ensuring that the model is stable and representative of the dataset.\n",
        "\n",
        "\n",
        "- Model Evaluation **3p**\n",
        "    - Evaluate models on the test dataset using regression metrics: *1p*\n",
        "        - Mean Absolute Error (MAE)\n",
        "        - Mean Squared Error (MSE)\n",
        "        - Root Mean Squared Error (RMSE)\n",
        "        - RÂ² Score\n",
        "    - Choose one metric for model comparison and explain your choice *1p*\n",
        "    - Compare the results across different models. Save all experiment results  into a table. *1p*\n",
        "\n",
        "Feature Importance - Optional\n",
        "- For applicable models (e.g., Decision Tree Regression), analyze feature importance and discuss its relevance to the problem.\n",
        "\n",
        "\n",
        "\n",
        "Deliverables\n",
        "\n",
        "- Notebook code with no errors.\n",
        "- Code and results from experiments. Create a table with all experiments results, include experiment name, metrics results.\n",
        "- Explain findings, choices, results.\n",
        "- Potential areas for improvement or further exploration.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Baseline Model Results:\n",
            "                             MAE        MSE      RMSE        R2\n",
            "SGDRegressor           4.290110  29.492624  5.430711  0.224885\n",
            "LinearRegression       4.306563  29.451021  5.426880  0.225978\n",
            "DecisionTreeRegressor  5.406934  57.357400  7.573467 -0.507448\n",
            "RandomForestRegressor  4.325599  30.827392  5.552242  0.189805\n",
            "RidgeRegression        4.306604  29.450548  5.426836  0.225991\n",
            "LassoRegression        4.758433  37.796275  6.147868  0.006651\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import SGDRegressor, LinearRegression, Ridge, Lasso\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "# --- Load Preprocessed Data ---\n",
        "train_data_path = 'E:/Master/ADC/14.Machine_Learning/ubb-sociology-ml/final_project/Train_Preprocessed.csv'\n",
        "test_data_path = 'E:/Master/ADC/14.Machine_Learning/ubb-sociology-ml/final_project/Test_Preprocessed.csv'\n",
        "\n",
        "data_train = pd.read_csv(train_data_path)\n",
        "data_test = pd.read_csv(test_data_path)\n",
        "\n",
        "# Define target variable and extract features\n",
        "target = 'hours-per-week'\n",
        "X_train = data_train.drop(columns=[target])\n",
        "y_train = data_train[target]\n",
        "X_test = data_test.drop(columns=[target])\n",
        "y_test = data_test[target]\n",
        "\n",
        "# Split train data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# --- Model Evaluation Function ---\n",
        "def evaluate_model(y_true, y_pred):\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    return {\"MAE\": mae, \"MSE\": mse, \"RMSE\": rmse, \"R2\": r2}\n",
        "\n",
        "# --- Baseline Model Selection ---\n",
        "models = {\n",
        "    \"SGDRegressor\": SGDRegressor(random_state=42),\n",
        "    \"LinearRegression\": LinearRegression(),\n",
        "    \"DecisionTreeRegressor\": DecisionTreeRegressor(random_state=42),\n",
        "    \"RandomForestRegressor\": RandomForestRegressor(random_state=42),\n",
        "    \"RidgeRegression\": Ridge(random_state=42),\n",
        "    \"LassoRegression\": Lasso(random_state=42)\n",
        "}\n",
        "\n",
        "results = {}\n",
        "for name, model in models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    y_val_pred = model.predict(X_val)\n",
        "    results[name] = evaluate_model(y_val, y_val_pred)\n",
        "\n",
        "results_df = pd.DataFrame(results).T\n",
        "print(\"Baseline Model Results:\\n\", results_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Out of all these models, the SGDRegressor seems the best based on the low MAE, MSE and relatively high R squared."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment 1 - Polynomial Features Results:\n",
            "Train Metrics: {'MAE': 4.3126048444028084, 'MSE': 28.902747425642865, 'RMSE': 5.376127549234939, 'R2': 0.2437855077469584}\n",
            "Validation Metrics: {'MAE': 4.353720005686385, 'MSE': 29.57915307194361, 'RMSE': 5.438671995252482, 'R2': 0.22261063917999835}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# --- Feature Selection Based on Correlation ---\n",
        "correlation_with_target = X_train.corrwith(y_train)\n",
        "selected_features = correlation_with_target[abs(correlation_with_target) > 0.1].index\n",
        "\n",
        "X_train_ftrs = X_train[selected_features]\n",
        "X_val_ftrs = X_val[selected_features]\n",
        "X_test_ftrs = X_test[selected_features]\n",
        "\n",
        "# --- Experiment 1: Polynomial Features ---\n",
        "# Add polynomial features without re-scaling\n",
        "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
        "\n",
        "# Apply transformations\n",
        "X_train_poly = poly.fit_transform(X_train_ftrs)\n",
        "X_val_poly = poly.transform(X_val_ftrs)\n",
        "X_test_poly = poly.transform(X_test_ftrs)\n",
        "\n",
        "# Fit the SGDRegressor model\n",
        "sgd_poly = SGDRegressor(random_state=42)\n",
        "sgd_poly.fit(X_train_poly, y_train)\n",
        "\n",
        "# Evaluate the model with polynomial features\n",
        "poly_train_metrics = evaluate_model(y_train, sgd_poly.predict(X_train_poly))\n",
        "poly_val_metrics = evaluate_model(y_val, sgd_poly.predict(X_val_poly))\n",
        "print(\"Experiment 1 - Polynomial Features Results:\")\n",
        "print(\"Train Metrics:\", poly_train_metrics)\n",
        "print(\"Validation Metrics:\", poly_val_metrics)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment 2 - Hyperparameter Tuning Results:\n",
            "Train Metrics: {'MAE': 4.31715475203357, 'MSE': 29.512022705616648, 'RMSE': 5.432496912619155, 'R2': 0.22784436589969825}\n",
            "Validation Metrics: {'MAE': 4.349385580180138, 'MSE': 29.964969570441184, 'RMSE': 5.474026814917989, 'R2': 0.21247073962197727}\n",
            "Best Parameters: {'alpha': 0.0001, 'learning_rate': 'adaptive', 'max_iter': 1000, 'penalty': 'l1'}\n"
          ]
        }
      ],
      "source": [
        "# --- Experiment 2: Hyperparameter Tuning ---\n",
        "# Define hyperparameter grid\n",
        "param_grid = {\n",
        "    \"alpha\": [0.0001, 0.001, 0.01],\n",
        "    \"penalty\": [\"l2\", \"l1\", \"elasticnet\"],\n",
        "    \"learning_rate\": [\"constant\", \"adaptive\"],\n",
        "    \"max_iter\": [1000, 2000]\n",
        "}\n",
        "\n",
        "# Perform GridSearchCV\n",
        "grid_search = GridSearchCV(SGDRegressor(random_state=42), param_grid, cv=3, scoring=\"neg_mean_squared_error\")\n",
        "grid_search.fit(X_train_ftrs, y_train)\n",
        "\n",
        "# Best model after hyperparameter tuning\n",
        "best_sgd = grid_search.best_estimator_\n",
        "best_sgd.fit(X_train_ftrs, y_train)\n",
        "\n",
        "# Evaluate the tuned model\n",
        "tuned_train_metrics = evaluate_model(y_train, best_sgd.predict(X_train_ftrs))\n",
        "tuned_val_metrics = evaluate_model(y_val, best_sgd.predict(X_val_ftrs))\n",
        "print(\"Experiment 2 - Hyperparameter Tuning Results:\")\n",
        "print(\"Train Metrics:\", tuned_train_metrics)\n",
        "print(\"Validation Metrics:\", tuned_val_metrics)\n",
        "print(\"Best Parameters:\", grid_search.best_params_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Comparison of Experiments:\n",
            "                    Model  Train_MAE   Val_MAE  Train_R2    Val_R2\n",
            "0                   Base   4.290110  4.290110  0.224885  0.224885\n",
            "1    Polynomial Features   4.312605  4.353720  0.243786  0.222611\n",
            "2  Hyperparameter Tuning   4.317155  4.349386  0.227844  0.212471\n"
          ]
        }
      ],
      "source": [
        "# --- Compare Results ---\n",
        "comparison_results = {\n",
        "    \"Model\": [\"Base\", \"Polynomial Features\", \"Hyperparameter Tuning\"],\n",
        "    \"Train_MAE\": [results[\"SGDRegressor\"][\"MAE\"], poly_train_metrics[\"MAE\"], tuned_train_metrics[\"MAE\"]],\n",
        "    \"Val_MAE\": [results[\"SGDRegressor\"][\"MAE\"], poly_val_metrics[\"MAE\"], tuned_val_metrics[\"MAE\"]],\n",
        "    \"Train_R2\": [results[\"SGDRegressor\"][\"R2\"], poly_train_metrics[\"R2\"], tuned_train_metrics[\"R2\"]],\n",
        "    \"Val_R2\": [results[\"SGDRegressor\"][\"R2\"], poly_val_metrics[\"R2\"], tuned_val_metrics[\"R2\"]],\n",
        "}\n",
        "\n",
        "comparison_df = pd.DataFrame(comparison_results)\n",
        "print(\"\\nComparison of Experiments:\\n\", comparison_df)\n",
        "\n",
        "output_path = \"E:/Master/ADC/14.Machine_Learning/ubb-sociology-ml/final_project/Experiment_Results.csv\"\n",
        "comparison_df.to_csv(output_path, index=False) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Test Set Comparison:\n",
            "                    Model  Test_MAE   Test_R2\n",
            "0                   Base  4.337558  0.232241\n",
            "1    Polynomial Features  4.345263  0.244144\n",
            "2  Hyperparameter Tuning  4.347847  0.232171\n"
          ]
        }
      ],
      "source": [
        "# --- Evaluate Base Model ---\n",
        "base_model = SGDRegressor(random_state=42)\n",
        "base_model.fit(X_train_ftrs, y_train) \n",
        "y_test_base = base_model.predict(X_test_ftrs)  \n",
        "base_test_metrics = evaluate_model(y_test, y_test_base)\n",
        "\n",
        "# --- Evaluate Polynomial Features Model ---\n",
        "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
        "X_test_poly = poly.fit_transform(X_test_ftrs)\n",
        "sgd_poly.fit(poly.fit_transform(X_train_ftrs), y_train) \n",
        "y_test_poly = sgd_poly.predict(X_test_poly) \n",
        "poly_test_metrics = evaluate_model(y_test, y_test_poly)\n",
        "\n",
        "# --- Evaluate Hyperparameter-Tuned Model ---\n",
        "best_sgd = SGDRegressor(alpha=0.0001, learning_rate='adaptive', max_iter=1000, penalty='l1', random_state=42)\n",
        "best_sgd.fit(X_train_ftrs, y_train) \n",
        "y_test_tuned = best_sgd.predict(X_test_ftrs)\n",
        "tuned_test_metrics = evaluate_model(y_test, y_test_tuned)\n",
        "\n",
        "\n",
        "test_comparison_results = {\n",
        "    \"Model\": [\"Base\", \"Polynomial Features\", \"Hyperparameter Tuning\"],\n",
        "    \"Test_MAE\": [base_test_metrics[\"MAE\"], poly_test_metrics[\"MAE\"], tuned_test_metrics[\"MAE\"]],\n",
        "    \"Test_R2\": [base_test_metrics[\"R2\"], poly_test_metrics[\"R2\"], tuned_test_metrics[\"R2\"]],\n",
        "}\n",
        "\n",
        "test_comparison_df = pd.DataFrame(test_comparison_results)\n",
        "print(\"\\nTest Set Comparison:\\n\", test_comparison_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Based on the Experimental results and the Test results, the more complex models don't show a significant improvement so the Base Model would work just as well."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
