{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydHb_ZL5yy6f"
      },
      "source": [
        "# **Final Project Task 3 - Census Modeling Regression**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnzXS8Oo9jwY"
      },
      "source": [
        "Requirements\n",
        "- Create a regression model on the Census dataset, with 'hours-per-week' target\n",
        "\n",
        "- You can use models (estmators) from sklearn, but feel free to use any library for traditional ML. \n",
        "    - Note: in sklearn, the LinearRegression estimator is based on OLS, a statistical method. Please use the SGDRegressor estimator, since this is based on gradient descent. \n",
        "    - You can use LinearRegression estimator, but only as comparison with the SGDRegressor - Optional.\n",
        "\n",
        "- Model Selection and Setup **2p**:\n",
        "    - Implement multiple models, to solve a regression problem using traditional ML: \n",
        "        - Linear Regression\n",
        "        - Decision Tree Regression\n",
        "        - Random Forest Regression - Optional\n",
        "        - Ridge Regression - Optional\n",
        "        - Lasso Regression - Optional\n",
        "    - Choose a loss (or experiment with different losses) for the model and justify the choice. *1p*\n",
        "        - MSE, MAE, RMSE, Huber Loss or others\n",
        "    - Justify model choices based on dataset characteristics and task requirements; specify model pros and cons. *1p*\n",
        "\n",
        "\n",
        "- Data Preparation\n",
        "    - Use the preprocessed datasets from Task 1.\n",
        "    - From the train set, create an extra validation set, if necesarry. So in total there will be: train, validation and test datasets.\n",
        "    - Be sure all models have their data preprocessed as needed. Some models require different, or no encoding for some features.\n",
        "\n",
        "\n",
        "- Model Training and Experimentation **10p**\n",
        "    - Establish a Baseline Model *2p*\n",
        "        - For each model type, train a simple model with default settings as a baseline.\n",
        "        - Evaluate its performance to establish a benchmark for comparison.\n",
        "    - Make plots with train, validation loss and metric on epochs (or on steps), if applicable. - Optional\n",
        "    - Feature Selection: - Optional\n",
        "        - Use insights from EDA in Task 2 to identify candidate features by analyzing patterns, relationships, and distributions.\n",
        "    - Experimentation: *8p*\n",
        "        - For each baseline model type, iteratively experiment with different combinations of features and transformations.\n",
        "        - Experiment with feature engineering techniques such as interaction terms, polynomial features, or scaling transformations.\n",
        "        - Identify the best model which have the best performance metrics on test set.\n",
        "        - You may need multiple preprocessed datasets preprocessed\n",
        "- Hyperparameter Tuning - Optional\n",
        "  - Perform hyperparameter tuning only on the best-performing model after evaluating all model types and experiments. \n",
        "  - Consider using techniques like Grid Search for exhaustive tuning, Random Search for quicker exploration, or Bayesian Optimization for an intelligent, efficient search of hyperparameters.\n",
        "  - Avoid tuning models that do not show strong baseline performance or are unlikely to outperform others based on experimentation.\n",
        "  - Ensure that hyperparameter tuning is done after completing feature selection, baseline modeling, and experimentation, ensuring that the model is stable and representative of the dataset.\n",
        "\n",
        "\n",
        "- Model Evaluation **3p**\n",
        "    - Evaluate models on the test dataset using regression metrics: *1p*\n",
        "        - Mean Absolute Error (MAE)\n",
        "        - Mean Squared Error (MSE)\n",
        "        - Root Mean Squared Error (RMSE)\n",
        "        - R² Score\n",
        "    - Choose one metric for model comparison and explain your choice *1p*\n",
        "    - Compare the results across different models. Save all experiment results  into a table. *1p*\n",
        "\n",
        "Feature Importance - Optional\n",
        "- For applicable models (e.g., Decision Tree Regression), analyze feature importance and discuss its relevance to the problem.\n",
        "\n",
        "\n",
        "\n",
        "Deliverables\n",
        "\n",
        "- Notebook code with no errors.\n",
        "- Code and results from experiments. Create a table with all experiments results, include experiment name, metrics results.\n",
        "- Explain findings, choices, results.\n",
        "- Potential areas for improvement or further exploration.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 24 rows with NaN in target 'hours-per-week' — dropping them.\n",
            "Post-clean target check — y NaNs: 0\n",
            "Shapes — Train: (20823, 105), Val: (5206, 105), Test: (6508, 105)\n",
            "[Train] NaNs: 1470, Infs: 0\n",
            "Columns with NaNs (105): ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', 'capital-net', 'is_married', 'workclass_Federal-gov', 'workclass_Local-gov', 'workclass_Never-worked'] ...\n",
            "[Val] NaNs: 840, Infs: 0\n",
            "Columns with NaNs (105): ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', 'capital-net', 'is_married', 'workclass_Federal-gov', 'workclass_Local-gov', 'workclass_Never-worked'] ...\n",
            "[Test] NaNs: 210, Infs: 0\n",
            "Columns with NaNs (105): ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', 'capital-net', 'is_married', 'workclass_Federal-gov', 'workclass_Local-gov', 'workclass_Never-worked'] ...\n",
            "[Train_i] No NaNs or Infs detected.\n",
            "[Val_i] No NaNs or Infs detected.\n",
            "[Test_i] No NaNs or Infs detected.\n",
            "\n",
            "Top 10 experiments by validation MAE:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>experiment</th>\n",
              "      <th>train_MAE</th>\n",
              "      <th>train_MSE</th>\n",
              "      <th>train_RMSE</th>\n",
              "      <th>train_R2</th>\n",
              "      <th>val_MAE</th>\n",
              "      <th>val_MSE</th>\n",
              "      <th>val_RMSE</th>\n",
              "      <th>val_R2</th>\n",
              "      <th>test_MAE</th>\n",
              "      <th>test_MSE</th>\n",
              "      <th>test_RMSE</th>\n",
              "      <th>test_R2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>SGD_interactions_huber</td>\n",
              "      <td>7.485635</td>\n",
              "      <td>152.513053</td>\n",
              "      <td>12.349618</td>\n",
              "      <td>0.002120</td>\n",
              "      <td>7.421685</td>\n",
              "      <td>149.923194</td>\n",
              "      <td>12.244313</td>\n",
              "      <td>0.001536</td>\n",
              "      <td>7.544989</td>\n",
              "      <td>153.057285</td>\n",
              "      <td>12.371632</td>\n",
              "      <td>-0.000640</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>SGD_baseline_huber</td>\n",
              "      <td>7.494254</td>\n",
              "      <td>152.614605</td>\n",
              "      <td>12.353728</td>\n",
              "      <td>0.001455</td>\n",
              "      <td>7.429802</td>\n",
              "      <td>150.016083</td>\n",
              "      <td>12.248105</td>\n",
              "      <td>0.000917</td>\n",
              "      <td>7.551661</td>\n",
              "      <td>153.196184</td>\n",
              "      <td>12.377245</td>\n",
              "      <td>-0.001548</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>SGD_poly2_huber</td>\n",
              "      <td>7.488265</td>\n",
              "      <td>152.547157</td>\n",
              "      <td>12.350998</td>\n",
              "      <td>0.001897</td>\n",
              "      <td>7.430721</td>\n",
              "      <td>149.990826</td>\n",
              "      <td>12.247074</td>\n",
              "      <td>0.001085</td>\n",
              "      <td>7.550120</td>\n",
              "      <td>153.200700</td>\n",
              "      <td>12.377427</td>\n",
              "      <td>-0.001578</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Lasso_alpha0.01</td>\n",
              "      <td>7.739182</td>\n",
              "      <td>151.396846</td>\n",
              "      <td>12.304343</td>\n",
              "      <td>0.009423</td>\n",
              "      <td>7.677394</td>\n",
              "      <td>149.199258</td>\n",
              "      <td>12.214715</td>\n",
              "      <td>0.006357</td>\n",
              "      <td>7.810460</td>\n",
              "      <td>153.222153</td>\n",
              "      <td>12.378294</td>\n",
              "      <td>-0.001718</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Lasso_alpha0.001</td>\n",
              "      <td>7.761449</td>\n",
              "      <td>151.372543</td>\n",
              "      <td>12.303355</td>\n",
              "      <td>0.009582</td>\n",
              "      <td>7.703332</td>\n",
              "      <td>149.319716</td>\n",
              "      <td>12.219645</td>\n",
              "      <td>0.005555</td>\n",
              "      <td>7.836077</td>\n",
              "      <td>153.367269</td>\n",
              "      <td>12.384154</td>\n",
              "      <td>-0.002667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Lasso_baseline</td>\n",
              "      <td>7.761449</td>\n",
              "      <td>151.372543</td>\n",
              "      <td>12.303355</td>\n",
              "      <td>0.009582</td>\n",
              "      <td>7.703332</td>\n",
              "      <td>149.319716</td>\n",
              "      <td>12.219645</td>\n",
              "      <td>0.005555</td>\n",
              "      <td>7.836077</td>\n",
              "      <td>153.367269</td>\n",
              "      <td>12.384154</td>\n",
              "      <td>-0.002667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Lasso_alpha0.0005</td>\n",
              "      <td>7.762870</td>\n",
              "      <td>151.372264</td>\n",
              "      <td>12.303344</td>\n",
              "      <td>0.009584</td>\n",
              "      <td>7.704952</td>\n",
              "      <td>149.326581</td>\n",
              "      <td>12.219926</td>\n",
              "      <td>0.005509</td>\n",
              "      <td>7.837587</td>\n",
              "      <td>153.374009</td>\n",
              "      <td>12.384426</td>\n",
              "      <td>-0.002711</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Ridge_alpha10.0</td>\n",
              "      <td>7.763872</td>\n",
              "      <td>151.372181</td>\n",
              "      <td>12.303340</td>\n",
              "      <td>0.009585</td>\n",
              "      <td>7.706066</td>\n",
              "      <td>149.330448</td>\n",
              "      <td>12.220084</td>\n",
              "      <td>0.005483</td>\n",
              "      <td>7.838653</td>\n",
              "      <td>153.378972</td>\n",
              "      <td>12.384626</td>\n",
              "      <td>-0.002743</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Ridge_baseline</td>\n",
              "      <td>7.764269</td>\n",
              "      <td>151.372172</td>\n",
              "      <td>12.303340</td>\n",
              "      <td>0.009585</td>\n",
              "      <td>7.706534</td>\n",
              "      <td>149.333422</td>\n",
              "      <td>12.220205</td>\n",
              "      <td>0.005463</td>\n",
              "      <td>7.839071</td>\n",
              "      <td>153.380826</td>\n",
              "      <td>12.384701</td>\n",
              "      <td>-0.002756</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Ridge_alpha1.0</td>\n",
              "      <td>7.764269</td>\n",
              "      <td>151.372172</td>\n",
              "      <td>12.303340</td>\n",
              "      <td>0.009585</td>\n",
              "      <td>7.706534</td>\n",
              "      <td>149.333422</td>\n",
              "      <td>12.220205</td>\n",
              "      <td>0.005463</td>\n",
              "      <td>7.839071</td>\n",
              "      <td>153.380826</td>\n",
              "      <td>12.384701</td>\n",
              "      <td>-0.002756</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               experiment  train_MAE   train_MSE  train_RMSE  train_R2  \\\n",
              "0  SGD_interactions_huber   7.485635  152.513053   12.349618  0.002120   \n",
              "1      SGD_baseline_huber   7.494254  152.614605   12.353728  0.001455   \n",
              "2         SGD_poly2_huber   7.488265  152.547157   12.350998  0.001897   \n",
              "3         Lasso_alpha0.01   7.739182  151.396846   12.304343  0.009423   \n",
              "4        Lasso_alpha0.001   7.761449  151.372543   12.303355  0.009582   \n",
              "5          Lasso_baseline   7.761449  151.372543   12.303355  0.009582   \n",
              "6       Lasso_alpha0.0005   7.762870  151.372264   12.303344  0.009584   \n",
              "7         Ridge_alpha10.0   7.763872  151.372181   12.303340  0.009585   \n",
              "8          Ridge_baseline   7.764269  151.372172   12.303340  0.009585   \n",
              "9          Ridge_alpha1.0   7.764269  151.372172   12.303340  0.009585   \n",
              "\n",
              "    val_MAE     val_MSE   val_RMSE    val_R2  test_MAE    test_MSE  test_RMSE  \\\n",
              "0  7.421685  149.923194  12.244313  0.001536  7.544989  153.057285  12.371632   \n",
              "1  7.429802  150.016083  12.248105  0.000917  7.551661  153.196184  12.377245   \n",
              "2  7.430721  149.990826  12.247074  0.001085  7.550120  153.200700  12.377427   \n",
              "3  7.677394  149.199258  12.214715  0.006357  7.810460  153.222153  12.378294   \n",
              "4  7.703332  149.319716  12.219645  0.005555  7.836077  153.367269  12.384154   \n",
              "5  7.703332  149.319716  12.219645  0.005555  7.836077  153.367269  12.384154   \n",
              "6  7.704952  149.326581  12.219926  0.005509  7.837587  153.374009  12.384426   \n",
              "7  7.706066  149.330448  12.220084  0.005483  7.838653  153.378972  12.384626   \n",
              "8  7.706534  149.333422  12.220205  0.005463  7.839071  153.380826  12.384701   \n",
              "9  7.706534  149.333422  12.220205  0.005463  7.839071  153.380826  12.384701   \n",
              "\n",
              "    test_R2  \n",
              "0 -0.000640  \n",
              "1 -0.001548  \n",
              "2 -0.001578  \n",
              "3 -0.001718  \n",
              "4 -0.002667  \n",
              "5 -0.002667  \n",
              "6 -0.002711  \n",
              "7 -0.002743  \n",
              "8 -0.002756  \n",
              "9 -0.002756  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Best experiment by validation MAE: SGD_interactions_huber\n",
            "Best model saved -> best_model.pkl\n",
            "\n",
            "Feature importance / coefficients for: SGD_interactions_huber\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>coefficient</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>relationship_Not-in-family</th>\n",
              "      <td>0.298265</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>relationship_Own-child</th>\n",
              "      <td>0.226161</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>relationship_Unmarried</th>\n",
              "      <td>0.225592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sex_Male</th>\n",
              "      <td>0.149724</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>is_married*sex_Male</th>\n",
              "      <td>0.147071</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>native-country_Cambodia</th>\n",
              "      <td>-0.140131</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_group_mid-age</th>\n",
              "      <td>0.121540</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>relationship_Wife</th>\n",
              "      <td>0.114165</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>marital-status_Married-civ-spouse</th>\n",
              "      <td>0.107625</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>is_married</th>\n",
              "      <td>0.103090</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>relationship_Other-relative</th>\n",
              "      <td>0.097754</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_group_adult</th>\n",
              "      <td>0.091616</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age</th>\n",
              "      <td>-0.069564</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>education-num</th>\n",
              "      <td>0.068900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>native-country_Hungary</th>\n",
              "      <td>0.062478</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>education_HS-grad</th>\n",
              "      <td>0.062399</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>age_group_senior</th>\n",
              "      <td>0.060328</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>native-country_Laos</th>\n",
              "      <td>0.050347</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>workclass_Self-emp-inc</th>\n",
              "      <td>0.046898</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>occupation_Farming-fishing</th>\n",
              "      <td>0.045676</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>education_Bachelors</th>\n",
              "      <td>0.045399</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>native-country_Vietnam</th>\n",
              "      <td>-0.043727</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>race_Other</th>\n",
              "      <td>-0.039544</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>native-country_Japan</th>\n",
              "      <td>0.036892</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>income_&gt;50K</th>\n",
              "      <td>0.036169</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                   coefficient\n",
              "relationship_Not-in-family            0.298265\n",
              "relationship_Own-child                0.226161\n",
              "relationship_Unmarried                0.225592\n",
              "sex_Male                              0.149724\n",
              "is_married*sex_Male                   0.147071\n",
              "native-country_Cambodia              -0.140131\n",
              "age_group_mid-age                     0.121540\n",
              "relationship_Wife                     0.114165\n",
              "marital-status_Married-civ-spouse     0.107625\n",
              "is_married                            0.103090\n",
              "relationship_Other-relative           0.097754\n",
              "age_group_adult                       0.091616\n",
              "age                                  -0.069564\n",
              "education-num                         0.068900\n",
              "native-country_Hungary                0.062478\n",
              "education_HS-grad                     0.062399\n",
              "age_group_senior                      0.060328\n",
              "native-country_Laos                   0.050347\n",
              "workclass_Self-emp-inc                0.046898\n",
              "occupation_Farming-fishing            0.045676\n",
              "education_Bachelors                   0.045399\n",
              "native-country_Vietnam               -0.043727\n",
              "race_Other                           -0.039544\n",
              "native-country_Japan                  0.036892\n",
              "income_>50K                           0.036169"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# %% [markdown]\n",
        "# # **Final Project Task 3 — Modeling: Predicting hours-per-week**\n",
        "# - Load preprocessed dataset from Task 1\n",
        "# - Clean/validate target (drop missing)\n",
        "# - Split into train/val/test (with consistent random_state)\n",
        "# - Impute residual NaNs/Infs in X AFTER split (fill=0 keeps scaled features centered)\n",
        "# - Train baselines for multiple estimators\n",
        "# - Run experiments (loss functions, interactions, polynomial terms)\n",
        "# - Evaluate with MAE (primary), MSE, RMSE, R²\n",
        "# - Save experiment table + persist best model\n",
        "\n",
        "# %% Imports\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "from sklearn.linear_model import SGDRegressor, LinearRegression, Ridge, Lasso\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.impute import SimpleImputer\n",
        "import joblib\n",
        "\n",
        "# Reproducibility\n",
        "RANDOM_STATE = 42\n",
        "np.random.seed(RANDOM_STATE)\n",
        "\n",
        "DATA_PATH = \"adult_preprocessed.csv\"\n",
        "assert os.path.exists(DATA_PATH), \"adult_preprocessed.csv not found. Please run Task 1 first.\"\n",
        "\n",
        "# %% Load preprocessed data (NOTE: scaling was fit on full data in Task 1 — potential leakage)\n",
        "pre = pd.read_csv(DATA_PATH)\n",
        "assert \"hours-per-week\" in pre.columns, \"Target 'hours-per-week' not found.\"\n",
        "\n",
        "# -----------------------------\n",
        "# Clean & validate target (y)\n",
        "# -----------------------------\n",
        "X = pre.drop(columns=[\"hours-per-week\"]).copy()\n",
        "\n",
        "# Force y to numeric and drop rows with missing/invalid target\n",
        "y = pd.to_numeric(pre[\"hours-per-week\"], errors=\"coerce\")\n",
        "\n",
        "n_bad_y = int(y.isna().sum())\n",
        "if n_bad_y > 0:\n",
        "    print(f\"Found {n_bad_y} rows with NaN in target 'hours-per-week' — dropping them.\")\n",
        "    mask = y.notna()\n",
        "    X = X.loc[mask].reset_index(drop=True)\n",
        "    y = y.loc[mask].reset_index(drop=True)\n",
        "\n",
        "# Guard against inf in y (shouldn't happen)\n",
        "if np.isinf(y.to_numpy()).any():\n",
        "    print(\"Found inf in y — replacing with NaN and dropping.\")\n",
        "    y = y.replace([np.inf, -np.inf], np.nan)\n",
        "    mask = y.notna()\n",
        "    X = X.loc[mask].reset_index(drop=True)\n",
        "    y = y.loc[mask].reset_index(drop=True)\n",
        "\n",
        "print(f\"Post-clean target check — y NaNs: {int(y.isna().sum())}\")\n",
        "\n",
        "# -----------------------------\n",
        "# Split: 80% train+val, 20% test; then from train, take 20% as val (64/16/20)\n",
        "# -----------------------------\n",
        "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
        "    X, y, test_size=0.20, random_state=RANDOM_STATE\n",
        ")\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train_full, y_train_full, test_size=0.20, random_state=RANDOM_STATE\n",
        ")\n",
        "\n",
        "print(f\"Shapes — Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")\n",
        "\n",
        "# Optional: save raw splits (pre-imputation)\n",
        "Path(\"splits\").mkdir(exist_ok=True)\n",
        "pd.concat([X_train, y_train], axis=1).to_csv(\"splits/train.csv\", index=False)\n",
        "pd.concat([X_val, y_val], axis=1).to_csv(\"splits/val.csv\", index=False)\n",
        "pd.concat([X_test, y_test], axis=1).to_csv(\"splits/test.csv\", index=False)\n",
        "\n",
        "# -----------------------------\n",
        "# Diagnose NaNs/Infs in X AFTER split and impute\n",
        "# -----------------------------\n",
        "def inspect_nan_inf(name, Xdf):\n",
        "    arr = Xdf.to_numpy()\n",
        "    n_nan = int(np.isnan(arr).sum())\n",
        "    n_inf = int(np.isinf(arr).sum())\n",
        "    if n_nan or n_inf:\n",
        "        nan_cols = Xdf.columns[Xdf.isna().any()].tolist()\n",
        "        print(f\"[{name}] NaNs: {n_nan}, Infs: {n_inf}\")\n",
        "        if nan_cols:\n",
        "            print(f\"Columns with NaNs ({len(nan_cols)}): {nan_cols[:10]}{' ...' if len(nan_cols) > 10 else ''}\")\n",
        "    else:\n",
        "        print(f\"[{name}] No NaNs or Infs detected.\")\n",
        "\n",
        "# Replace infs with NaNs, then impute with 0.0\n",
        "for part_name, X_part in [(\"Train\", X_train), (\"Val\", X_val), (\"Test\", X_test)]:\n",
        "    X_part.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "    inspect_nan_inf(part_name, X_part)\n",
        "\n",
        "imputer = SimpleImputer(strategy=\"constant\", fill_value=0.0)  # 0 keeps scaled features centered; dummies: absence\n",
        "X_train_i = pd.DataFrame(imputer.fit_transform(X_train), columns=X_train.columns, index=X_train.index)\n",
        "X_val_i   = pd.DataFrame(imputer.transform(X_val), columns=X_val.columns, index=X_val.index)\n",
        "X_test_i  = pd.DataFrame(imputer.transform(X_test), columns=X_test.columns, index=X_test.index)\n",
        "\n",
        "for part_name, X_part in [(\"Train_i\", X_train_i), (\"Val_i\", X_val_i), (\"Test_i\", X_test_i)]:\n",
        "    inspect_nan_inf(part_name, X_part)\n",
        "\n",
        "# Sanity checks for y\n",
        "assert not y_train.isna().any(), \"y_train has NaN\"\n",
        "assert not y_val.isna().any(),   \"y_val has NaN\"\n",
        "assert not y_test.isna().any(),  \"y_test has NaN\"\n",
        "\n",
        "# -----------------------------\n",
        "# Utilities\n",
        "# -----------------------------\n",
        "def evaluate(y_true, y_pred, prefix=\"\"):\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    return {\n",
        "        f\"{prefix}MAE\": mae,\n",
        "        f\"{prefix}MSE\": mse,\n",
        "        f\"{prefix}RMSE\": rmse,\n",
        "        f\"{prefix}R2\": r2,\n",
        "    }\n",
        "\n",
        "def run_model(name, estimator, X_tr, y_tr, X_v, y_v, X_te, y_te):\n",
        "    \"\"\"Train on train, eval on val and test; return metrics dict and fitted estimator.\"\"\"\n",
        "    model = estimator\n",
        "    model.fit(X_tr, y_tr)\n",
        "    y_tr_pred = model.predict(X_tr)\n",
        "    y_v_pred = model.predict(X_v)\n",
        "    y_te_pred = model.predict(X_te)\n",
        "\n",
        "    res = {\"experiment\": name}\n",
        "    res.update(evaluate(y_tr, y_tr_pred, prefix=\"train_\"))\n",
        "    res.update(evaluate(y_v, y_v_pred, prefix=\"val_\"))\n",
        "    res.update(evaluate(y_te, y_te_pred, prefix=\"test_\"))\n",
        "    return res, model\n",
        "\n",
        "results = []\n",
        "fitted_models = {}\n",
        "experiment_features = {}  # map experiment -> feature columns used\n",
        "\n",
        "# -----------------------------\n",
        "# Baselines\n",
        "# -----------------------------\n",
        "# 1) SGDRegressor (squared_error)\n",
        "sgd_baseline = SGDRegressor(\n",
        "    loss=\"squared_error\",  # OLS via GD\n",
        "    penalty=\"l2\",\n",
        "    alpha=1e-4,\n",
        "    max_iter=3000,\n",
        "    tol=1e-3,\n",
        "    random_state=RANDOM_STATE\n",
        ")\n",
        "res, mdl = run_model(\"SGD_baseline_squared_error\", sgd_baseline,\n",
        "                     X_train_i, y_train, X_val_i, y_val, X_test_i, y_test)\n",
        "results.append(res); fitted_models[\"SGD_baseline_squared_error\"] = mdl\n",
        "experiment_features[\"SGD_baseline_squared_error\"] = X_train_i.columns\n",
        "\n",
        "# 2) SGDRegressor (huber)\n",
        "sgd_huber = SGDRegressor(\n",
        "    loss=\"huber\",\n",
        "    epsilon=1.35,\n",
        "    penalty=\"l2\",\n",
        "    alpha=1e-4,\n",
        "    max_iter=4000,\n",
        "    tol=1e-3,\n",
        "    random_state=RANDOM_STATE\n",
        ")\n",
        "res, mdl = run_model(\"SGD_baseline_huber\", sgd_huber,\n",
        "                     X_train_i, y_train, X_val_i, y_val, X_test_i, y_test)\n",
        "results.append(res); fitted_models[\"SGD_baseline_huber\"] = mdl\n",
        "experiment_features[\"SGD_baseline_huber\"] = X_train_i.columns\n",
        "\n",
        "# 3) LinearRegression (comparison-only)\n",
        "linreg = LinearRegression()\n",
        "res, mdl = run_model(\"LinearRegression_baseline\", linreg,\n",
        "                     X_train_i, y_train, X_val_i, y_val, X_test_i, y_test)\n",
        "results.append(res); fitted_models[\"LinearRegression_baseline\"] = mdl\n",
        "experiment_features[\"LinearRegression_baseline\"] = X_train_i.columns\n",
        "\n",
        "# 4) Decision Tree (defaults)\n",
        "dtr = DecisionTreeRegressor(random_state=RANDOM_STATE)\n",
        "res, mdl = run_model(\"DecisionTree_baseline\", dtr,\n",
        "                     X_train_i, y_train, X_val_i, y_val, X_test_i, y_test)\n",
        "results.append(res); fitted_models[\"DecisionTree_baseline\"] = mdl\n",
        "experiment_features[\"DecisionTree_baseline\"] = X_train_i.columns\n",
        "\n",
        "# 5) Random Forest (optional but useful)\n",
        "rfr = RandomForestRegressor(\n",
        "    n_estimators=200,\n",
        "    random_state=RANDOM_STATE,\n",
        "    n_jobs=-1\n",
        ")\n",
        "res, mdl = run_model(\"RandomForest_baseline\", rfr,\n",
        "                     X_train_i, y_train, X_val_i, y_val, X_test_i, y_test)\n",
        "results.append(res); fitted_models[\"RandomForest_baseline\"] = mdl\n",
        "experiment_features[\"RandomForest_baseline\"] = X_train_i.columns\n",
        "\n",
        "# 6) Ridge (note: no random_state)\n",
        "ridge = Ridge(alpha=1.0)\n",
        "res, mdl = run_model(\"Ridge_baseline\", ridge,\n",
        "                     X_train_i, y_train, X_val_i, y_val, X_test_i, y_test)\n",
        "results.append(res); fitted_models[\"Ridge_baseline\"] = mdl\n",
        "experiment_features[\"Ridge_baseline\"] = X_train_i.columns\n",
        "\n",
        "# 7) Lasso\n",
        "lasso = Lasso(alpha=0.001, max_iter=5000, random_state=RANDOM_STATE)\n",
        "res, mdl = run_model(\"Lasso_baseline\", lasso,\n",
        "                     X_train_i, y_train, X_val_i, y_val, X_test_i, y_test)\n",
        "results.append(res); fitted_models[\"Lasso_baseline\"] = mdl\n",
        "experiment_features[\"Lasso_baseline\"] = X_train_i.columns\n",
        "\n",
        "# -----------------------------\n",
        "# Experiments — Simple interactions (built on imputed frames)\n",
        "# -----------------------------\n",
        "X_train_exp = X_train_i.copy()\n",
        "X_val_exp   = X_val_i.copy()\n",
        "X_test_exp  = X_test_i.copy()\n",
        "\n",
        "def safe_add_interaction(df, a, b, new_name=None):\n",
        "    if a in df.columns and b in df.columns:\n",
        "        name = new_name or f\"{a}*{b}\"\n",
        "        df[name] = df[a] * df[b]\n",
        "\n",
        "# Guided by EDA:\n",
        "for a, b in [\n",
        "    (\"age\", \"education-num\"),\n",
        "    (\"age\", \"is_married\"),\n",
        "    (\"education-num\", \"capital-net\"),\n",
        "    (\"is_married\", \"sex_Male\"),  # only if 'sex_Male' exists\n",
        "]:\n",
        "    safe_add_interaction(X_train_exp, a, b)\n",
        "    safe_add_interaction(X_val_exp, a, b)\n",
        "    safe_add_interaction(X_test_exp, a, b)\n",
        "\n",
        "# SGD with interactions (squared_error)\n",
        "sgd_inter_sq = SGDRegressor(\n",
        "    loss=\"squared_error\",\n",
        "    penalty=\"l2\",\n",
        "    alpha=3e-4,\n",
        "    max_iter=4000,\n",
        "    tol=1e-3,\n",
        "    random_state=RANDOM_STATE\n",
        ")\n",
        "res, mdl = run_model(\"SGD_interactions_squared_error\", sgd_inter_sq,\n",
        "                     X_train_exp, y_train, X_val_exp, y_val, X_test_exp, y_test)\n",
        "results.append(res); fitted_models[\"SGD_interactions_squared_error\"] = mdl\n",
        "experiment_features[\"SGD_interactions_squared_error\"] = X_train_exp.columns\n",
        "\n",
        "# SGD with interactions (huber)\n",
        "sgd_inter_huber = SGDRegressor(\n",
        "    loss=\"huber\",\n",
        "    epsilon=1.35,\n",
        "    penalty=\"l2\",\n",
        "    alpha=3e-4,\n",
        "    max_iter=5000,\n",
        "    tol=1e-3,\n",
        "    random_state=RANDOM_STATE\n",
        ")\n",
        "res, mdl = run_model(\"SGD_interactions_huber\", sgd_inter_huber,\n",
        "                     X_train_exp, y_train, X_val_exp, y_val, X_test_exp, y_test)\n",
        "results.append(res); fitted_models[\"SGD_interactions_huber\"] = mdl\n",
        "experiment_features[\"SGD_interactions_huber\"] = X_train_exp.columns\n",
        "\n",
        "# Decision Tree with depth control (overfitting guard)\n",
        "dtr_tuned = DecisionTreeRegressor(\n",
        "    max_depth=10, min_samples_leaf=20, random_state=RANDOM_STATE\n",
        ")\n",
        "res, mdl = run_model(\"DecisionTree_depth10_leaf20\", dtr_tuned,\n",
        "                     X_train_i, y_train, X_val_i, y_val, X_test_i, y_test)\n",
        "results.append(res); fitted_models[\"DecisionTree_depth10_leaf20\"] = mdl\n",
        "experiment_features[\"DecisionTree_depth10_leaf20\"] = X_train_i.columns\n",
        "\n",
        "# Random Forest tuned (light)\n",
        "rfr_tuned = RandomForestRegressor(\n",
        "    n_estimators=400,\n",
        "    max_depth=18,\n",
        "    min_samples_leaf=5,\n",
        "    random_state=RANDOM_STATE,\n",
        "    n_jobs=-1\n",
        ")\n",
        "res, mdl = run_model(\"RandomForest_tuned\", rfr_tuned,\n",
        "                     X_train_i, y_train, X_val_i, y_val, X_test_i, y_test)\n",
        "results.append(res); fitted_models[\"RandomForest_tuned\"] = mdl\n",
        "experiment_features[\"RandomForest_tuned\"] = X_train_i.columns\n",
        "\n",
        "# Ridge/Lasso alpha sweeps\n",
        "for alpha in [0.1, 1.0, 10.0]:\n",
        "    r = Ridge(alpha=alpha)\n",
        "    res, mdl = run_model(f\"Ridge_alpha{alpha}\", r,\n",
        "                         X_train_i, y_train, X_val_i, y_val, X_test_i, y_test)\n",
        "    results.append(res); fitted_models[f\"Ridge_alpha{alpha}\"] = mdl\n",
        "    experiment_features[f\"Ridge_alpha{alpha}\"] = X_train_i.columns\n",
        "\n",
        "for alpha in [0.0005, 0.001, 0.01]:\n",
        "    l = Lasso(alpha=alpha, max_iter=6000, random_state=RANDOM_STATE)\n",
        "    res, mdl = run_model(f\"Lasso_alpha{alpha}\", l,\n",
        "                         X_train_i, y_train, X_val_i, y_val, X_test_i, y_test)\n",
        "    results.append(res); fitted_models[f\"Lasso_alpha{alpha}\"] = mdl\n",
        "    experiment_features[f\"Lasso_alpha{alpha}\"] = X_train_i.columns\n",
        "\n",
        "# -----------------------------\n",
        "# PolynomialFeatures (degree=2) on selected numeric features (built on imputed frames)\n",
        "# -----------------------------\n",
        "poly_feats = [\"age\", \"education-num\", \"capital-net\"]\n",
        "available_poly_feats = [c for c in poly_feats if c in X_train_i.columns]\n",
        "\n",
        "if available_poly_feats:\n",
        "    def add_poly_block(X_tr, X_v, X_te, feat_names):\n",
        "        poly = PolynomialFeatures(degree=2, include_bias=False)\n",
        "        Xtr_poly = pd.DataFrame(poly.fit_transform(X_tr[feat_names]),\n",
        "                                columns=poly.get_feature_names_out(feat_names),\n",
        "                                index=X_tr.index)\n",
        "        Xv_poly = pd.DataFrame(poly.transform(X_v[feat_names]),\n",
        "                               columns=poly.get_feature_names_out(feat_names),\n",
        "                               index=X_v.index)\n",
        "        Xte_poly = pd.DataFrame(poly.transform(X_te[feat_names]),\n",
        "                                columns=poly.get_feature_names_out(feat_names),\n",
        "                                index=X_te.index)\n",
        "        # Merge with original (avoid duplicates)\n",
        "        Xtr_all = pd.concat([X_tr, Xtr_poly], axis=1)\n",
        "        Xv_all = pd.concat([X_v, Xv_poly], axis=1)\n",
        "        Xte_all = pd.concat([X_te, Xte_poly], axis=1)\n",
        "        return Xtr_all, Xv_all, Xte_all\n",
        "\n",
        "    Xtr_poly, Xv_poly, Xte_poly = add_poly_block(X_train_i, X_val_i, X_test_i, available_poly_feats)\n",
        "\n",
        "    sgd_poly = SGDRegressor(\n",
        "        loss=\"huber\", epsilon=1.35, alpha=5e-4, penalty=\"l2\",\n",
        "        max_iter=5000, tol=1e-3, random_state=RANDOM_STATE\n",
        "    )\n",
        "    res, mdl = run_model(\"SGD_poly2_huber\", sgd_poly,\n",
        "                         Xtr_poly, y_train, Xv_poly, y_val, Xte_poly, y_test)\n",
        "    results.append(res); fitted_models[\"SGD_poly2_huber\"] = mdl\n",
        "    experiment_features[\"SGD_poly2_huber\"] = Xtr_poly.columns\n",
        "\n",
        "    ridge_poly = Ridge(alpha=1.0)\n",
        "    res, mdl = run_model(\"Ridge_poly2\", ridge_poly,\n",
        "                         Xtr_poly, y_train, Xv_poly, y_val, Xte_poly, y_test)\n",
        "    results.append(res); fitted_models[\"Ridge_poly2\"] = mdl\n",
        "    experiment_features[\"Ridge_poly2\"] = Xtr_poly.columns\n",
        "else:\n",
        "    print(\"Polynomial experiment skipped — required columns not in preprocessed data.\")\n",
        "\n",
        "# -----------------------------\n",
        "# Collect and save results\n",
        "# -----------------------------\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df = results_df.sort_values(by=\"val_MAE\").reset_index(drop=True)\n",
        "results_df.to_csv(\"experiments_results.csv\", index=False)\n",
        "\n",
        "print(\"\\nTop 10 experiments by validation MAE:\")\n",
        "try:\n",
        "    from IPython.display import display\n",
        "    display(results_df.head(10))\n",
        "except Exception:\n",
        "    print(results_df.head(10))\n",
        "\n",
        "best_name = results_df.loc[0, \"experiment\"]\n",
        "print(f\"\\nBest experiment by validation MAE: {best_name}\")\n",
        "\n",
        "# Save the best fitted model\n",
        "best_model = fitted_models[best_name]\n",
        "joblib.dump(best_model, \"best_model.pkl\")\n",
        "print(\"Best model saved -> best_model.pkl\")\n",
        "\n",
        "# -----------------------------\n",
        "# (Optional) Feature importance / coefficients\n",
        "# -----------------------------\n",
        "def show_feature_importance(name, model, feature_names):\n",
        "    print(f\"\\nFeature importance / coefficients for: {name}\")\n",
        "    if hasattr(model, \"feature_importances_\"):\n",
        "        imp = pd.Series(model.feature_importances_, index=feature_names).sort_values(ascending=False)\n",
        "        try:\n",
        "            display(imp.head(25).to_frame(\"importance\"))\n",
        "        except Exception:\n",
        "            print(imp.head(25))\n",
        "    elif hasattr(model, \"coef_\"):\n",
        "        coef = pd.Series(model.coef_, index=feature_names).sort_values(key=np.abs, ascending=False)\n",
        "        try:\n",
        "            display(coef.head(25).to_frame(\"coefficient\"))\n",
        "        except Exception:\n",
        "            print(coef.head(25))\n",
        "    else:\n",
        "        print(\"This model does not expose importances/coeffs.\")\n",
        "\n",
        "feat_cols = experiment_features.get(best_name, X_train_i.columns)\n",
        "show_feature_importance(best_name, best_model, feat_cols)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Updated Top 10 experiments by validation MAE:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>experiment</th>\n",
              "      <th>train_MAE</th>\n",
              "      <th>train_MSE</th>\n",
              "      <th>train_RMSE</th>\n",
              "      <th>train_R2</th>\n",
              "      <th>val_MAE</th>\n",
              "      <th>val_MSE</th>\n",
              "      <th>val_RMSE</th>\n",
              "      <th>val_R2</th>\n",
              "      <th>test_MAE</th>\n",
              "      <th>test_MSE</th>\n",
              "      <th>test_RMSE</th>\n",
              "      <th>test_R2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Dummy_median</td>\n",
              "      <td>7.403928</td>\n",
              "      <td>152.986505</td>\n",
              "      <td>12.368771</td>\n",
              "      <td>-0.000978</td>\n",
              "      <td>7.336919</td>\n",
              "      <td>150.332693</td>\n",
              "      <td>12.261023</td>\n",
              "      <td>-0.001191</td>\n",
              "      <td>7.453749</td>\n",
              "      <td>153.351414</td>\n",
              "      <td>12.383514</td>\n",
              "      <td>-0.002563</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>SGD_interactions_huber</td>\n",
              "      <td>7.485635</td>\n",
              "      <td>152.513053</td>\n",
              "      <td>12.349618</td>\n",
              "      <td>0.002120</td>\n",
              "      <td>7.421685</td>\n",
              "      <td>149.923194</td>\n",
              "      <td>12.244313</td>\n",
              "      <td>0.001536</td>\n",
              "      <td>7.544989</td>\n",
              "      <td>153.057285</td>\n",
              "      <td>12.371632</td>\n",
              "      <td>-0.000640</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>SGD_baseline_huber</td>\n",
              "      <td>7.494254</td>\n",
              "      <td>152.614605</td>\n",
              "      <td>12.353728</td>\n",
              "      <td>0.001455</td>\n",
              "      <td>7.429802</td>\n",
              "      <td>150.016083</td>\n",
              "      <td>12.248105</td>\n",
              "      <td>0.000917</td>\n",
              "      <td>7.551661</td>\n",
              "      <td>153.196184</td>\n",
              "      <td>12.377245</td>\n",
              "      <td>-0.001548</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>SGD_poly2_huber</td>\n",
              "      <td>7.488265</td>\n",
              "      <td>152.547157</td>\n",
              "      <td>12.350998</td>\n",
              "      <td>0.001897</td>\n",
              "      <td>7.430721</td>\n",
              "      <td>149.990826</td>\n",
              "      <td>12.247074</td>\n",
              "      <td>0.001085</td>\n",
              "      <td>7.550120</td>\n",
              "      <td>153.200700</td>\n",
              "      <td>12.377427</td>\n",
              "      <td>-0.001578</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Dummy_mean</td>\n",
              "      <td>7.565096</td>\n",
              "      <td>152.837052</td>\n",
              "      <td>12.362728</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>7.495684</td>\n",
              "      <td>150.155110</td>\n",
              "      <td>12.253779</td>\n",
              "      <td>-0.000009</td>\n",
              "      <td>7.606176</td>\n",
              "      <td>153.016736</td>\n",
              "      <td>12.369993</td>\n",
              "      <td>-0.000375</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Lasso_alpha0.01</td>\n",
              "      <td>7.739182</td>\n",
              "      <td>151.396846</td>\n",
              "      <td>12.304343</td>\n",
              "      <td>0.009423</td>\n",
              "      <td>7.677394</td>\n",
              "      <td>149.199258</td>\n",
              "      <td>12.214715</td>\n",
              "      <td>0.006357</td>\n",
              "      <td>7.810460</td>\n",
              "      <td>153.222153</td>\n",
              "      <td>12.378294</td>\n",
              "      <td>-0.001718</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Lasso_alpha0.001</td>\n",
              "      <td>7.761449</td>\n",
              "      <td>151.372543</td>\n",
              "      <td>12.303355</td>\n",
              "      <td>0.009582</td>\n",
              "      <td>7.703332</td>\n",
              "      <td>149.319716</td>\n",
              "      <td>12.219645</td>\n",
              "      <td>0.005555</td>\n",
              "      <td>7.836077</td>\n",
              "      <td>153.367269</td>\n",
              "      <td>12.384154</td>\n",
              "      <td>-0.002667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Lasso_baseline</td>\n",
              "      <td>7.761449</td>\n",
              "      <td>151.372543</td>\n",
              "      <td>12.303355</td>\n",
              "      <td>0.009582</td>\n",
              "      <td>7.703332</td>\n",
              "      <td>149.319716</td>\n",
              "      <td>12.219645</td>\n",
              "      <td>0.005555</td>\n",
              "      <td>7.836077</td>\n",
              "      <td>153.367269</td>\n",
              "      <td>12.384154</td>\n",
              "      <td>-0.002667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Lasso_alpha0.0005</td>\n",
              "      <td>7.762870</td>\n",
              "      <td>151.372264</td>\n",
              "      <td>12.303344</td>\n",
              "      <td>0.009584</td>\n",
              "      <td>7.704952</td>\n",
              "      <td>149.326581</td>\n",
              "      <td>12.219926</td>\n",
              "      <td>0.005509</td>\n",
              "      <td>7.837587</td>\n",
              "      <td>153.374009</td>\n",
              "      <td>12.384426</td>\n",
              "      <td>-0.002711</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Ridge_alpha10.0</td>\n",
              "      <td>7.763872</td>\n",
              "      <td>151.372181</td>\n",
              "      <td>12.303340</td>\n",
              "      <td>0.009585</td>\n",
              "      <td>7.706066</td>\n",
              "      <td>149.330448</td>\n",
              "      <td>12.220084</td>\n",
              "      <td>0.005483</td>\n",
              "      <td>7.838653</td>\n",
              "      <td>153.378972</td>\n",
              "      <td>12.384626</td>\n",
              "      <td>-0.002743</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               experiment  train_MAE   train_MSE  train_RMSE  train_R2  \\\n",
              "0            Dummy_median   7.403928  152.986505   12.368771 -0.000978   \n",
              "1  SGD_interactions_huber   7.485635  152.513053   12.349618  0.002120   \n",
              "2      SGD_baseline_huber   7.494254  152.614605   12.353728  0.001455   \n",
              "3         SGD_poly2_huber   7.488265  152.547157   12.350998  0.001897   \n",
              "4              Dummy_mean   7.565096  152.837052   12.362728  0.000000   \n",
              "5         Lasso_alpha0.01   7.739182  151.396846   12.304343  0.009423   \n",
              "6        Lasso_alpha0.001   7.761449  151.372543   12.303355  0.009582   \n",
              "7          Lasso_baseline   7.761449  151.372543   12.303355  0.009582   \n",
              "8       Lasso_alpha0.0005   7.762870  151.372264   12.303344  0.009584   \n",
              "9         Ridge_alpha10.0   7.763872  151.372181   12.303340  0.009585   \n",
              "\n",
              "    val_MAE     val_MSE   val_RMSE    val_R2  test_MAE    test_MSE  test_RMSE  \\\n",
              "0  7.336919  150.332693  12.261023 -0.001191  7.453749  153.351414  12.383514   \n",
              "1  7.421685  149.923194  12.244313  0.001536  7.544989  153.057285  12.371632   \n",
              "2  7.429802  150.016083  12.248105  0.000917  7.551661  153.196184  12.377245   \n",
              "3  7.430721  149.990826  12.247074  0.001085  7.550120  153.200700  12.377427   \n",
              "4  7.495684  150.155110  12.253779 -0.000009  7.606176  153.016736  12.369993   \n",
              "5  7.677394  149.199258  12.214715  0.006357  7.810460  153.222153  12.378294   \n",
              "6  7.703332  149.319716  12.219645  0.005555  7.836077  153.367269  12.384154   \n",
              "7  7.703332  149.319716  12.219645  0.005555  7.836077  153.367269  12.384154   \n",
              "8  7.704952  149.326581  12.219926  0.005509  7.837587  153.374009  12.384426   \n",
              "9  7.706066  149.330448  12.220084  0.005483  7.838653  153.378972  12.384626   \n",
              "\n",
              "    test_R2  \n",
              "0 -0.002563  \n",
              "1 -0.000640  \n",
              "2 -0.001548  \n",
              "3 -0.001578  \n",
              "4 -0.000375  \n",
              "5 -0.001718  \n",
              "6 -0.002667  \n",
              "7 -0.002667  \n",
              "8 -0.002711  \n",
              "9 -0.002743  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best (updated) by val MAE: Dummy_median\n",
            "Best model re-saved -> best_model.pkl\n"
          ]
        }
      ],
      "source": [
        "# %% Naïve baselines — DummyRegressor (mean and median)\n",
        "from sklearn.dummy import DummyRegressor\n",
        "\n",
        "def run_and_log(name, est, X_tr, y_tr, X_v, y_v, X_te, y_te, results, fitted, exp_feats, cols):\n",
        "    res, mdl = run_model(name, est, X_tr, y_tr, X_v, y_v, X_te, y_te)\n",
        "    results.append(res); fitted[name] = mdl; exp_feats[name] = cols\n",
        "\n",
        "dummy_mean = DummyRegressor(strategy=\"mean\")\n",
        "run_and_log(\"Dummy_mean\", dummy_mean, X_train_i, y_train, X_val_i, y_val, X_test_i, y_test,\n",
        "            results, fitted_models, experiment_features, X_train_i.columns)\n",
        "\n",
        "dummy_median = DummyRegressor(strategy=\"median\")\n",
        "run_and_log(\"Dummy_median\", dummy_median, X_train_i, y_train, X_val_i, y_val, X_test_i, y_test,\n",
        "            results, fitted_models, experiment_features, X_train_i.columns)\n",
        "\n",
        "# Rebuild and persist the updated results table\n",
        "results_df = pd.DataFrame(results).sort_values(by=\"val_MAE\").reset_index(drop=True)\n",
        "results_df.to_csv(\"experiments_results.csv\", index=False)\n",
        "\n",
        "print(\"\\nUpdated Top 10 experiments by validation MAE:\")\n",
        "try:\n",
        "    from IPython.display import display\n",
        "    display(results_df.head(10))\n",
        "except Exception:\n",
        "    print(results_df.head(10))\n",
        "\n",
        "best_name = results_df.loc[0, \"experiment\"]\n",
        "print(f\"Best (updated) by val MAE: {best_name}\")\n",
        "best_model = fitted_models[best_name]\n",
        "joblib.dump(best_model, \"best_model.pkl\")\n",
        "print(\"Best model re-saved -> best_model.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 600x400 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAk4AAAGGCAYAAACNCg6xAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjgsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvwVt1zgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAV89JREFUeJzt3Qd4FNXaB/B3N8lueiMkBAglhCoCAoqgdAQr2PsVFEURpVkAC8i14MWuV2kqlk+v9Vq5qIgiqFgRASkRAoJAQijpybbM9/yPzjK72cCSANlk/r/nWcics2X6vHPaWDRN04SIiIiIDst6+LcQEREREQMnIiIioiPAEiciIiKiIDFwIiIiIgoSAyciIiKiIDFwIiIiIgoSAyciIiKiIDFwIiIiIgoSAyciIiKiIDFwoqPuvvvuE4vFEtR78T68/1gaMGCAetGRr6tt27apbfTSSy8dtdXXqlUrGTVqFDcHhZxly5ap/R3/67CvYp+tKz/88IPYbDb5448/JFSceuqpcuedd4pZMXBqwHCxw0lAf4WHh0uzZs3UiWDnzp11PXtUzUlbf0VEREhmZqZcc801kpOTU6/W17fffqsC4oKCgrqelZCAYFTfrlarVeLj46V9+/byj3/8Q5YsWVLXs0ch7O6775YrrrhCWrZs6U177rnnjurNTCDr169XxzBunvxNmTJFnn32WcnNzRUzCq/rGaBj75///Ke0bt1aKioq5LvvvlMH3Ndffy3r1q2TyMjIo/5799xzj0ydOvWof69ZjB8/Xk4++WRxuVyyatUqmT9/vixatEjWrl0rTZs2Pa7zgpN1eXm5CuKONHCaOXOmCtITExN98jZt2qSCB7Np3ry5zJo1S/1dWloqmzdvlv/+97/yf//3f3LppZeq/490PdOxt2DBAqmsrKyTVb169Wr5/PPP1fFkhMApJSXlmJbcInDCMYyg37/EbcSIESr4x3zg+mI2DJxM4KyzzpKePXuqv6+//np1wP3rX/+SDz/8UJ2wjzaUbOFFNdO3b1+5+OKL1d/XXnuttGvXTgVTL7/8skybNi3gZ3AhjomJOeqrHCUkRzu4ttvtYkYJCQly9dVX+6Q9/PDDatviAoSLE45LCi11GcwuXLhQWrRooarGQonValXnqFdeeUUFV8E2zWgozHfbR+rCDFu2bPFZGxs3blQHQ3JysrpYIthCcGWEUhAcKG3btlXvadSokZx++uk+1Q2B2jg5HA6ZNGmSNG7cWOLi4mT48OHy559/Vtka1bUnCPSdOKkMGjRIUlNT1cW4U6dOMmfOnKC28DPPPCMnnHCCREdHS1JSklrW119/vdr35+XlqWAQy+4PJSiYt3//+99Br6MjgWWErVu3+qwL3BFeeeWVav7x/TqUXPTo0UOioqLUtrz88stlx44dVb4XJVlt2rRR7zvllFNkxYoVVd5TXRsn7CsIurE98XlUO6FKQZ+/O+64Q/2Nkk69ikov8g/UxglVkZdccomaX2wTXChQyhaoKvOtt96SBx98UJXgYP0OHjxYld4Y/f7773LRRRdJkyZN1HvwXqyHwsJCCSVhYWHy9NNPq30X+48+f4dqW+bfLlDfH7Kzs1VghgAN2+Xee+8VTdPUttdLCLA+HnvssWrXK/ZbVOfjGMW5APODY3fixInqOIuNjVXBPNJ0/fv3l65duwZcPuwXw4YNC3p9oHSjc+fOsmbNGvW92BeysrLknXfeUflfffWV9OrVy7vPoTTGH5ohXHfddZKWlqbOCzjOX3zxxSrvw/nn/PPPVzccWDacn4zLdahz0qOPPip9+vRRxzbmBcebPo9GWK+33HKLvP/++2q59Pn55JNPglof+ByOf+O5D/Py22+/qXWhH1vGdomoHsf2ysjIUL+H9YeA3L/U7I033lDzjW2NfePEE0+Up556SuVhv8PxCAMHDvT+jrHt1xlnnKHaXaFUzGxYLGBC+gUMF1wdDsTTTjtNnTRRzYaTCU6kOLG8++67csEFF3hP0qhuQMkVLrZFRUXy008/qSolHEjVwftxQceFHiecL774Qs4555xaLQeCJJyEEIQhqPnoo4/k5ptvVieIcePGHbLoHXf5uDBMmDBBVWHiRP3999+r+QsEJ2GcyLFOZsyY4ZP35ptvqgugfqKp6Tqqjh7g4iRthN9DcPbQQw+pCyQgoMAFE0ENfj8/P18Fif369ZNffvnFW232wgsvyI033qi2BU6yCFywHhG44IR7KFhXCL5xJz5mzBh1Isc8Yv3j9y+88EJ1Ef/Pf/4jTzzxhCrhBFzMqwtKMR9lZWVqu2A5UbqG+cHFSN/3jKU0uOO9/fbb1YV99uzZctVVV6ntB06nU12scRG89dZbVbCAi+nHH3+sLioILEIJ9h20YcF2QxV6TY+Lyy67TDp27KjWD4LOBx54QG3PefPmqYsvLp6vvfaaWm+oCsY+YYR9FkEAjn8EothvsI2xrg8cOKD2a72qHwHx9OnT1efQTuuGG25QVf8IDnQ//vij2g9QdX8k8FvnnnuuCnSxj+M4x9+Yd+yrN910kzpOH3nkEXUMIzDExV/flxB06wEL9rnFixfL6NGj1XGIzwOqnxFwb9++Xe1zqAJ/9dVX1XkpGAgwsH9iv8P+hiAE84p9zH/7YZuiShbnJswnAmUE9fht/2PaCPss3tO9e3ef9CeffFLt1whi9ZsVnJ8AxxDOU/gsjm+UVqGaDyXVu3fvVp8F3MRhn8M60Es5N2zYIN988406J2LfwHrBvN51111qvwL9f0DQBfjMSSedJKaiUYO1cOFCXE21zz//XMvPz9d27NihvfPOO1rjxo01u92upnWDBw/WTjzxRK2iosKbVllZqfXp00dr27atN61r167aOeecc8jfnTFjhvpd3erVq9X0zTff7PO+K6+8UqXj/bqRI0dqLVu2POx3QllZWZX3DRs2TMvMzPRJ69+/v3rpRowYoZ1wwgnakZo3b56ah7Vr1/qkd+rUSRs0aNARraNAvvzyS/X9L774otpeu3bt0hYtWqS1atVKs1gs2o8//uizLq644gqfz2/btk0LCwvTHnzwQZ90zG94eLg33el0aqmpqVq3bt00h8Phfd/8+fPV9xrX1datW1Ua9iVdv379tLi4OO2PP/7w+R3sL7pHHnlEfQ6f94fti+2smzhxonrvihUrvGnFxcVa69at1bJ7PB6f9dOxY0ef+X7qqad8tssvv/yipt9++20tVGCdHmqfe++999Q8Y1mqW+86/2NG3x/GjBnjTXO73Vrz5s3VfvPwww970w8cOKBFRUX5rH99vXbu3FntGzrsX/j8WWed5fP7vXv39jlGCwoKtMjISG3KlCk+7xs/frwWExOjlZSUaEeynjAvr7/+ujdt48aNKs1qtWrfffedN/3TTz+tso5Gjx6tpaena3v37vX53ssvv1xLSEjwnjOefPJJ9dm33nrL+57S0lItKytLpWOdHOqc5H/uwXrD+jOeBwDfZbPZtM2bN3vTfv31V5X+zDPPHHJd4LyN93300UdV8rAvGY9T3f3336/WeXZ2tk/61KlT1blh+/btanrChAlafHy82k+qg+PHf134s9ls2tixYzWzYVWdCQwZMkTdeaEkAXdoKE1CFRyqL2D//v3qTgulFMXFxbJ371712rdvn7pzR7WH3gsPJRYonUJasP73v/+p/3EHY6Tf/dUU7o51KHnAPONuC6Unh6qSwTKgmB53xEcCJSko2UIJkw532agyw92+8fuPdB0ZoZoB2wt3wbh7RfsllMDo7dR0uPM2wl0tStuwHfVtiBdKXFAy9eWXX6r3ofRrz5496vPo5myskjhcaQxKsJYvX67mEXezRjVt54D9AyVzxupG3E2jNAulo1i/RqgqMs63XvWs9zzUl+HTTz9Vd+D1AZYXcPzVFEoYjaVY2F9w7UZpi3HfRBVXoF6a6L1pbM+DKjF8HtvaCOko5XG73d71japAlDDqJZ8ej0cdJ3pV2JGuC5Qw6TC/mG+UduC3jfMB+rLgt1E6ft5556m/jccAzmM4J6DUV9/n0tPTvW0JAdWC2OeO9NyDEjJ8N/ZD/fv9z7+oEtd16dJFVY0drqcszr/+NQOH8/bbb6v5wGeMy495wDbBsQtYnziv1LZHZ9Lfv2M2rKozAXQbRQNjHNyo68fBY2ygi2J5nGhQVYBXILjQohoPPShwksT3oVj+zDPPVEX1OBlUB/XgKO43njz0E2JtoIgY1WYrV66scoHEslYXBKArLdpG4GKN+v+hQ4eqon9UVR4KqpxQtI3quvvvv1+l4eKAYApBla4m68gIVSA4+eHih9/EBSNQY3tUlxghUMN2RJAUiH5R1MeD8X+fPvzBoegne2OVTG1hfowXRJ1eLYB84+/5B2z6hQUXMH29TJ48WR5//HFVvYN1iWoVvf1PdUpKStSrJnCx14OfmtB/V69yqgn/9YJlRfsuvarUmK5flA/3efCvukU6AnQcY3pVE4IuHAtoJ4dqHhxfqDbDfn+kcEPnH4TjNwPNh3G7I6hHVSza7uFV3XlM36dw7Pv/TrDnJFTJoSoU7XuM7aIC3Tz4r1d9n9Xn+3D0YDQYOAegKr26anF9+VFtiPMYOg7hvI5zIG64cK46Epqmma5hODBwMgEECHppBe4AcWePQAGNmnGy1xsNou1DdQ05cZIBnBTRnuWDDz6Qzz77TJ5//nnVjmXu3Lk+d7w1Vd1BiLslI8wDgpgOHTqoCyROqiiFwJ0k5udQ3YdxQcay4+SHRpq4S0WvJgQsgRp/G+FOGCUeOGF269ZNnXwwH8aLU23XERpp4g7xSO56AcuM9Yc2HQi6/NXmwh5KAi2b/wUGDaBRgqZvA5R2og0P2ujoJa3+0OD3cNu/OgjgazOQK0oujcdZsMfB4dZLMOvqcO8N5jtw3kA7G7RjxP6P/1HSGcx+fLTmQz/mESCPHDky4HuDvXk5FASHCMSxnDhvoOQKNx3orBKog8mRbAMjPSgNNsDS1wHaUVY3OCVu5gCN4XEOQ6kszhd4Yf4RAKN0O1gFBQVVAnMzYOBkMjiIcQFBTwn04kFDUL2UAQd/MCc6NDhF8IAX7pRxAsFFo7qgAGMB4YBGMGG8o0PwEuhOLNCgif6j5qIhMu70UOVovKPTq6MOB9UHqF7DC407UWKEhs1oRHmo7vcIPNHoUq+uQ+PXQEMEHOk6OhpQooeTMUpc9BNkIPpAerg71Xvs6b0B0XOvuh5SoO8r+oW+OkdyF4r5CbQvoOeecX6PFAJQvNA4GQ1kUaKI4BUlBYHgomGsLjwShyupOxQEQ7jgoqpI/329FM3/WAil0aP9zyu4GUPDcTQ2Rm8wNBivLmg4FvQeu1ifhzuPYZ/CPuxfYhJoP/SHGy2cIxB0GEvuEXgcTbgpNPamDeb4wjkA55tgzuO40US1Jl44P6MUCh0JUOsQqDTO386dO9W509hg3CzYxsmE0HUVpVDoYYEeZbj7QBoOGvS88IcicJ1/ET9KMXCQBerGq0NxMKCHhpHew8P/wEcVAIqbdZin9957z+d9+gnZeNeGzwVz8vJfBpxA0B0c34Xg4VDQNgB31yhpQk8afBbB1KG+P5h1dDQg+MN6QamJ/90spvX5QukjLjIIInDi0+Gid7iRvvE5BIGo8kWPH//f0OntWoIZOfzss89Wj5VAlasO7S9Q3YIee9g2RwK9p/T2NzoEUKguPtQ2QPCDC05NXjUNnHCRR2kYejThf7R9AfyPO3m9TYoOJRyhCtVyKB3BjQUu3v5jVh1r2PfRWw2BTaDA3ngewz63a9cunyEEUN1fXRWf/+8gqDCW/qEtHoLFowlVaChJR5tEfzi+Ah1bqG7DcYSgzh/erx8X/ucoHBt6aZx+jBzuGP7555/V/+gRazYscTIpjLOD7rO4WKKRMNpB4W4XFxjcKeJCgDYKOAjRkPrXX39Vn8NFDEEWuqKiVAUHNU4+6PpbHVRpoesrTvoIbnCgLV26tMrYO3pVGNogoQs6LiQ4maE7MkpQjA0vUSev3zHpJ2oMM4AgMFDwZ4TPohoBJRCoXsBFC6VvaIgdTBsTlFLhooDlQRDlPzJ2TdbR0YCgE6UpKAHDiRwBHZYHd6wIPNHwFdWxKFnE+7DeUOKE5cF7EHQGEwAgAMa+gm7S+E6UcOH30AVeH9NF76qM7tLYpvhNbKtADYVR6omGxQiwsc2xzlBdgHnCRfBIRxlHRwesa+zf2G9wsUBXc/3CWlew76MKC7Bf6yOHoyQW60hvN6dD6SSGFsD/CHYRRKGEM1ShSzraoqGBMkoh/LvRHw9YXyh1Rps5nMdwLKLzC84daHeFvwF5OOZRyogAANVt2EdQ6nc4OE+geQDaA6GUDe2GcP7EzZHxhu9oQFtJHLv+JWM4vnBexHGM38V5D8cyzusohcdwDqiqxvtwE4KnDuAchOMUATn2KawLfAZV1yjJxPATOFfrJUj4G8cMShCx76J0TR83D9CwHKX9phuKAOq6Wx8d++EI9G7sRuji3aZNG/XSu6Ru2bJFu+aaa7QmTZpoERERWrNmzbRzzz1XDWGge+CBB7RTTjlFS0xMVN2aO3TooLq5G7sxBxo6oLy8XHVPbtSokeoue95556nhEPy7VsNnn32muvaiq2v79u21//u//wv4nR9++KHWpUsX1RUa3db/9a9/qa78/t3g/YcjwLAC6FKPecGwDFgHd9xxh1ZYWBjUei0qKlLLjt/BvPkLZh0FoncLP1w3en1dYMiCQN59913t9NNPV+sZL/z+uHHjtE2bNvm877nnnlNd/rEOevbsqS1fvrzKuqquW/y6deu0Cy64QC0j1j+207333lulazT2IXQjN24T/+EI9H3v4osv9n4f1t/HH38c1Prxn8ecnBztuuuuU9sV35WcnKwNHDhQde+uK3o3e/0VGxurhvm4+uqr1f4eCLq8o3s9utFj+IdLL71U27NnT7XDEfjvD1jH2P6HGxqhuvVa3fnjUPvf7NmzVd5DDz10BGun+nnTYZ8JNMQHfgv7tlFeXp5Ky8jIUOcxnM8w3AqG2zDCcBrDhw/XoqOjtZSUFNVF/5NPPglqOIIXXnhBbT8cOzi+sK4CnaMCzV91x0Agq1atqjJUB+Tm5qr1gf3CfwgRDOUxbdo0NbQCzqFYNgwr8+ijj3rPQTinDx06VA1Lgve0aNFCu/HGG7Xdu3f7/M6CBQvU8C4YysC4Xjwejxr24Z577tHMyIJ/6jp4IyKi+g8DQ2IEbpRsBOpNRkcOnU/0ATpDxfvvv69K21BaitI6s2HgREREtYZ7cHQsQG+wYDtp0OFhRHwMqYHOHDXtKHG09e7dW80TRu03I7ZxIiKiGkMbGrSrQbCEtjQYAsIf2tMYOyL4Q1ua6sYeMju01zrUuqsLKw0dOcyIJU5ERFRjqJZDBwF0kkCXdgzr4Q+dJfBQ2uqgJEV/hiZRqGPgRERExxR6rh1qIEcM5nq4kfuJQgUDJyIiIqIgcQBMIiIioiCxcbgBhp3HaLIYNNCMDy4kIiIya6/Q4uJiNfTD4QbdZeBkgKDJ/wncREREZA47duyo9kHgOgZOBvrjNrDi9GdGERERUcNWVFSkCk6CeewWAycDvXoOQRMDJyIiInOxBNFMh43DiYiIiILEwImIiIgoSAyciIiIiILEwImIiIioIQZOO3fulKuvvlo9fRtD9J944ony008/+YzDMH36dElPT1f5Q4YMUU+UJiIiIjJV4ITnHOFZRhEREbJ48WJZv369PPbYY5KUlOR9z+zZs+Xpp5+WuXPnyvfffy8xMTEybNgwqaioqNN5JyIiooah3jyrburUqfLNN9/IihUrAuZjMTDi52233Sa33367SissLJS0tDR56aWX5PLLLw9qHIeEhAT1OQ5HQEREZA5FR3D9rzclTh9++KH07NlTLrnkEklNTZWTTjpJFixY4M3funWr5Obmquo5HVZCr169ZOXKlQG/0+FwqJVlfBGR+azZvl/e+XG7vPj1Fnnn5+1qmoioXg+AmZOTI3PmzJHJkyfLXXfdJT/++KOMHz9ebDabjBw5UgVNgBImI0zref5mzZolM2fOPC7zT0Sh6fMNefLcl5tl1fYCb1r3Foly88AsGdLR93xCRGStTw/g7d69uzz00EOqtGnMmDFyww03qPZMNTVt2jRVLKe/8KgVIjIPlCz5B02AaaSz5ImI6m3ghJ5ynTp18knr2LGjbN++Xf3dpEkT9X9eXp7PezCt5/mz2+3ex6vwMStE5pOdV1IlaNIhHflERPUycEKPuk2bNvmkZWdnS8uWLdXfrVu3VgHS0qVLvflos4Tedb179z7u80tEoa/I4apVPhGZT71p4zRp0iTp06ePqqq79NJL5YcffpD58+erl/5gvokTJ8oDDzwgbdu2VYHUvffeq3ranX/++XU9+0QUguLtEbXKJyLzqTeB08knnyzvvfeeapf0z3/+UwVGTz75pFx11VXe99x5551SWlqq2j8VFBTI6aefLp988olERkbW6bwTUWhqlxarGoIHqq5DOvKJiOrlOE7HA8dxIjIf9qojoqIjGMep3pQ4EREdbRVOj9jCLDL9rCzJ3uuQwjKnJETbpF2KXYpcFpUfaQvjiiciLwZORGRa+SUOcbo12esWSY6xqxfsLce/msrPSI6u69kkohBSb3rVEREdbQ53Za3yich8GDgRkWnZw61iEZEwq4jT45FSh0tcHo+atvydT0RkxKo6IjKtxrF22WItlvW7i6XM6fGmR9vCpFN6nMonIjLi7RQRmVqJ0yMuj2+VHKZLXQcDKSIiHUuciMi00Pg7MjxM2qbGSVEFqukqJSLMKvGREWILs7JxOBFVwcCJiExLb/xtC7dKSoBqOTYOJyJ/rKojItM6XONvNg4nIn8MnIjItND42xaO/nNVIZ2Nw4nIHwMnIjItjAreMT2+SvCE6U5N4zlqOBFVwTZORGRqjeMiJc4eoRqCo00TqudQ0sRHrRBRIAyciMj0ECTx0SpEFAxW1REREREFiYETERERUZBYVUdEplfh9LCNExEFhYETEZlafnGFbNhdJE635k3bGl6ietuh4TgRkRGr6ojI1CVN/kETYBrpyCciMmLgRESmhSEINLcmcXaR/aUO2ZpfLAdKHWoa6cgnIjJiVR0RmRbGbarwuOWpT7fJqu0F3vTuLRLlxr6t+Kw6IqqCJU5EZFplFU6Zt8I3aAJMIx35RERGDJyIyLSy80rkF7+gSYd05BMRGTFwIiLTKnK4xGoR8X/ML6aRjnwiIiO2cSIi04q3R0ilJmKx/HUXqf0dNOF/pCOfiMiIJU5EZFod02PlpBaJoml/BUrG/5GOfCIiIwZORGRaJzRPlpsHZqkgyQjTSEc+EZERq+qIyNSGdEyT9LgI2bC7RLVpQvUcSpoYNBFRIAyciMj0ECQxUCKiYLCqjoiIiChIDJyIiIiIgsTAiYiIiChIDJyIiIiIgsTG4URkeoWlTtm6r1RKnR6JsYVJ60YxkhBjM/16IaKqGDgRkalt3lMsX2zYI0UVbm9afGS4DOqYKlmpcXU6b0QUelhVR0SmLmnyD5oA00hHPhGREQMnIjItVM+VO9zSKCZMPdS33OmWMIuoaaQjn4jIiFV1RGRaaNNkt1nlrZ92yqa8Em96+7RYGdGtqconIjJiiRMRmZZVKuWD1bt8gibANNKRT0RkxMCJiExrT7FDdhdWBMxDOvKJiIwYOBGRae0rdUpavF31ojPCdFp8pMonIjJiGyciMq2EyAjZvr9cGsXYJDXOLpWaqEbiZU6PbN9fpvKJiIwYOBGRaXVuFi8nNI2XtTsLq+Sd2CxB5RMRGbGqjohMq316oowdkKWCJCNMjxuYpfKJiIxY4kREpja4Y5o0TbDJbzuLpcjhknh7hJzQLE46Nk2q61kjohDEwImITC2/uELyS1ySGGNXL5VW4pKU4gppHBdZ17NHRCGm3lbVPfzww2KxWGTixInetIqKChk3bpw0atRIYmNj5aKLLpK8vLw6nU8iCl0VTo9s2F0kTrfmk45ppCOfiKjeB04//vijzJs3T7p06eKTPmnSJPnoo4/k7bfflq+++kp27dolF154YZ3NJxGFtvwSh2huTVKiRPaXOmT7/hIpKHWoaaQjn4ioXlfVlZSUyFVXXSULFiyQBx54wJteWFgoL7zwgrz++usyaNAglbZw4ULp2LGjfPfdd3LqqafW4VwTUShyuCulwuOWpxZvk1XbC7zp3Vskyo19W6l8IqJ6XeKEqrhzzjlHhgwZ4pP+888/i8vl8knv0KGDtGjRQlauXFkHc0pEoa6swinzVvgGTYBppCOfiKjelji98cYbsmrVKlVV5y83N1dsNpskJvp2H05LS1N5gTgcDvXSFRUVHYO5JqJQlZ1XUiVo0iEd+V1aJB/3+SKi0FVvSpx27NghEyZMkNdee00iI49OT5dZs2ZJQkKC95WRkXFUvpeI6gcMP1CbfCIyn3oTOKEqbs+ePdK9e3cJDw9XLzQAf/rpp9XfKFlyOp1SUOB794hedU2aNAn4ndOmTVNto/QXgjMiMg+M2QRhFpFw68EXpo35RET1rqpu8ODBsnbtWp+0a6+9VrVjmjJliiotioiIkKVLl6phCGDTpk2yfft26d27d8DvtNvt6kVE5tQuLVZ6tEiUn1Fd5zsigUpHPhFRvQyc4uLipHPnzj5pMTExaswmPX306NEyefJkSU5Olvj4eLn11ltV0MQedUQUSHpitIw+vZV4vsqR1TsPtnHs1ixeru/bSuUTEdXLwCkYTzzxhFitVlXihEbfw4YNk+eee66uZ4uIQtSm3CL55vc8Gdu/jewvd0lxhUviIiMkOSpClmfnSpw9QhrHc/RwIjrIommaXwG1eaFXHRqJo70TSqyIqGH739rdsmjN7mrzz+mSLmefmH5c54mIQvv6X28ahxMRHW3x9rBa5ROR+TBwIiLTat8kXhrH2sRTWSllTreUOFzqf0wjHflEREYMnIjItNB+CdVx9nCrFFe4pdThUf9j+tyu6WzfREQNu3E4EdGRqHB6pNzlkUtPbib5xS4prXBLTGS4NI6LkDKnR+VH2lhdR0QHscSJiEwrv8QhmlsTxEZO91/VdW53pZpGOvKJiIxY4kREpuVwV0qFxy1PLd4mvxieWXdSi0S5sW8rlU9EZMQSJyIyrbIKp8xb4Rs0AaaRjnwiIiMGTkRkWtl5JbLaL2jSIR35RERGDJyIyLSKHC6xWkT+fqavF6aRjnwiIiO2cSIi04q3RwhaMVkCBE+Vf+cTERmxxImITKtT01g5KSMxYB7SkU9EZMTAiYhMq1OzZBk7MEu6ZiRKpSbeF6aRjnwiIiNW1RGRaWGAS1uYRaaflSXZex1SWOaUpGibZKXYpchl4QCYRFQFAyciMi0McFnucMnjn+bI6p1F3vRuzeJl7IBMlZ+RHF2n80hEoYVVdURkWvsKS2XOMt+gCTCNdOQTERkxcCIi09q8t7xK0KRDOvKJiIwYOBGRaR1unCaO40RE/hg4EZFpHW6cJo7jRET+2DiciEyrXVqsnNkxUUZ0aykFFR4prnBJfFSEJNjD5IPVf6h8IiIjBk5EZFpdWiTLxadkyXNfbpZVhmfWdW+RKDcPzFL5RERGrKojItNas32/CppW7yiQcKt4X5hGOvKJiIwYOBGRaWXnlaiSJowW7q48+MI00pFPRGTEwImITIu96ojoSDFwIiLTYq86IjpSDJyIyLTQaw4NwQNBOnvVEZE/Bk5EZFroNYfec/7BE3vVEVF1OBwBEZlWflGFaHvyZfpZWZK91yGFZU5JiLZJuxS75G/Pl/xmCdI4PrKuZ5OIQggDJyIyrU25RfLhTqckHciXMItF3Jom4YUOWbezSA6UOSUqvYiBExH5YOBERKZV7PBIalykbN1bIkUVbm96fGS4tE6JVflEREZs40REppUQFVYlaAJMIx35RERGDJyIyLQSo+xiqSbP8nc+EZERAyciMi1bRJj0zmokCZG+rRYw3adtisonIjJiGyciMi17uFXaNoqVns1jvb3qkqJtkpVil8Lyv/KJiIwYOBGRaTWOtct6zwF54uMcWb2zyJverVm8jB2QqfKJiIx4O0VEppWdWyjzVmzzCZoA00hHPhGREQMnIjKt7LwSWbW9IGAe0pFPRGTEwImITKvI4apVPhGZDwMnIjKteHtErfKJyHwYOBGRaaH3HBqCB4J05BMRGTFwIiLTKiypVL3n/IMnTI8bmKnyiYiMOBwBEZmWxR4mX/+6Te45t53k/D2OU0K0TTJT7PLBTzlyRtc2dT2LRBRiGDgRkWk1iYuUQpdNXvx6h4SFWcTl0cQWZpFlGzUJD7OpfCIiIwZORGRa1jCrDOqQKq//sEN+zyv2prdNi5OremWofCIiIwZORGRabrdHlqzPldQ4u6TG28XlrpQIPGZFE/nst1xpnxa44TgRmRcDJyIyrb0lDimsqJQKV9XxmiIjwlQ+EZERy6GJyLSKHB5Jj7erIMkI0+nxkSqfiKheBk6zZs2Sk08+WeLi4iQ1NVXOP/982bRpk897KioqZNy4cdKoUSOJjY2Viy66SPLy8upsnokotMXbwyTDWinnd0mVwR1T5dTMZDmjY6qazrB6VD4RUb2sqvvqq69UUITgye12y1133SVDhw6V9evXS0xMjHrPpEmTZNGiRfL2229LQkKC3HLLLXLhhRfKN998U9ezT0QhqH2TeNlxIFkWfvuH/JZ78Ll0JzSJlat7t1T5RERGFk3TNKmH8vPzVckTAqp+/fpJYWGhNG7cWF5//XW5+OKL1Xs2btwoHTt2lJUrV8qpp5562O8sKipSARe+Kz6eJ0yihm7N9v3y2JJs+X7rAalwHxzsMjLcKr1aJ8ltZ7STLi2S63QeiejYO5Lrf72pqvOHhYPk5L9Oaj///LO4XC4ZMmSI9z0dOnSQFi1aqMApEIfDoVaW8UVE5pGdVyLLf98n9girpMbZpHGsTf2PaaQjn4io3gdOlZWVMnHiRDnttNOkc+fOKi03N1dsNpskJib6vDctLU3lVdduChGm/srIyDgu809EoaHI4cLIA1JY7pY9xU7JL3Gq/zGt/Z1PRFTvAye0dVq3bp288cYbtfqeadOmqZIr/bVjx46jNo9EFPri7RG1yici86k3jcN1aPD98ccfy/Lly6V58+be9CZNmojT6ZSCggKfUif0qkNeIHa7Xb2IyJyyUuzqgb6rd1atpkc68omIahQ4ffjhh8G+VYYPHy5HG9qw33rrrfLee+/JsmXLpHXr1j75PXr0kIiICFm6dKkahgAwXMH27duld+/eR31+iKj+KyyplLEDMmXOshyf4AlB07iBmSqfiKhGgRPGTQqGxWIRj8dzTKrn0GPugw8+UGM56e2W0DYpKipK/T969GiZPHmyajCOVvEItBA0BdOjjohMKMIq5Xm/y33ntpfsvQ4pLHNKQrRN2qXYZdv2TWJLa1/Xc0hE9TVwQoPsujRnzhz1/4ABA3zSFy5cKKNGjVJ/P/HEE2K1WlWJE3rMDRs2TJ577rk6mV8iCn3h4RZ5fYNV0nftlGh7mLjcmkRGWOWbzW7ZXWiVCc0sdT2LRBRi6u04TscCx3EiMpfP1u6WggqXvPbdH/Lb7oNVdSekx8tVp7aUxMgIGXpiep3OIxGF1vW/xo3DS0tL1eCTaEOERtlG48ePr+nXEhEdNwkxYfL81zlS4fZIp/R4qdQ0sVosavqdn3bIbcPacmsQUe0Dp19++UXOPvtsKSsrUwEU2hTt3btXoqOj1WjeDJyIqD4oKHVLcblTLj05Q5zuSilxeCTWHi62cIu88+MOlU9EVOvACc+EO++882Tu3LmqaOu7775TPdquvvpqmTBhQk2+kojouHM4K+WyXi3lrR93yHrDs+o6NYlV6cgnIqp14LR69WqZN2+eaogdFhamGmJnZmbK7NmzZeTIkerBukREoa5RQoR8vGarDD2hifRs7ZJyh1ti7OGSGB0h32bvlJF929X1LBJRQwicULqEoAlQNYd2TniYLkqfOPo2EdUXJYUOadOkkXzwy07Zur/cm946OUrOPLGJyiciqnXgdNJJJ8mPP/4obdu2lf79+8v06dNVG6dXX33V++w4IqJQZ420yidrc32CJsA00rufwxInIjoKz6p76KGHJD39ry66Dz74oCQlJcnYsWMlPz9f5s+fX5OvJCI67grLKqsETTqkI5+IqNYlTj179vT+jaq6Tz75pCZfQ0RUp1xapSTH2GR/qe+QKoB05BMR1brEiYioIbBZrJKRFKWCJCNMIx35RES1LnHCA3bxTLrq5OTk1ORriYiOqzYpdokI0yQ+MlwFSpWaiNUiUljuUunIJyKqdeA0ceJEn2mXy6UGxUSV3R133FGTryQiOu4K97vkhtMzZc6yHFn9Z6E3vVuzeLmxb6bKl9bcMERUy8CpukEun332Wfnpp59q8pVERMedMypMVmX/Lved216y9zqksMwpCdE2aZdil8W/bpKe7fjIFSLydVQr8M866yx59913j+ZXEhEdM1aLJr/s0uT3/HKpcLnF6akUh8ujppGOfCKio/KQ30Deeecd9dw6IqL6oKLIJVec2loWfv2HrNlV5E3v0jRerj29tconIjoqA2AaG4drmia5ublqHKfnnnuuJl9JRHTcNW4cKQ98nO0TNAGmEUzdcy4HwCSioxA4nX/++T7TePxK48aNZcCAAdKhQ4eafCUR0XG3fZ9DNuQWS2qcTWJs4aKhV53VIiUOl0pH/imZ3DBEVMvAacaMGTX5GBFRSClyuCQrNVZ2FZbLnuIyb3pCVLhKRz4RUY0Cp6Ii36LsQ4mPjw/6vUREdSU52iY7DvzVMNwWbhFBW3CLSKnDrdKRT0RUo8ApMTHxkINeGnk8nmC/loiozjSKjZCbBkXJ6S0ODkeQFG2TrBS7fL19k8onIqpR4PTll196/962bZtMnTpVRo0aJb1791ZpK1eulJdffllmzZoV7FcSEdWpygMOaZvcRu77OFtW7yzyGQDz5oFtVD4RkZFFQ5e4IzR48GC5/vrr5YorrvBJf/3112X+/PmybNkyqY9QHZmQkCCFhYWsbiQygdVb8+Sf/8uWVTuqNkXonhEv089uJ91ap9XJvBFRaF7/azQAJkqXevbsWSUdaT/88ENNvpKI6LjbvNcRMGgCpCOfiKjWgVNGRoYsWLCgSvrzzz+v8oiI6oPD9ZpjrzoiOirDETzxxBNy0UUXyeLFi6VXr14qDSVNv//+Ox+5QkT1Rrw9QqwWkRZJURJjD5fKSk2N44ReddsPlKt8IqJalzidffbZkp2dLeedd57s379fvfA30pBHRFQfoPfcWZ3TpKDcJb/tLpYNeSXqf0wjHflEREflWXWoknvooYdq+nEiojqHgVM6N02Q7FwES25veuNYm5zYLEHlExHVKHBas2aNdO7cWT1eBX8fSpcuXYL9WiKiOrN1r0NeWvGHjOiRLgM6WMTh8og9IkwsosnC5X9IylntpEdrbiAiqkHg1K1bN/Ug39TUVPU3BsMMNJIB0jkAJhHVB2j8HR1llde+3y6lzkpveozNKqlxdjYOJ6KaB05bt25VD/LV/yYiqu+So+2yp9jhEzQBppGOfCKiGgVOLVu2DPg3EVF9lRQdIS2SoyVnb6mEo3vd39yVmkpHPhFRrXvV4dEqixYt8k7feeed6ll2ffr0kT/++KMmX0lEdNztPlAul/bMkKzGMaqUSX9hGunIJyKqdeCE3nRRUVHeUcT//e9/y+zZsyUlJUUmTZpUk68kIjru4qIi5OUVW6VHyyS5ZWAbuf70Vup/TCMd+UREtR6OYMeOHZKVlaX+fv/99+Xiiy+WMWPGyGmnnSYDBgyoyVcSER13kRFhkpoQLm1TY8Sj/TXwZVxkhKTFRcjGXQdUPhFRrQOn2NhY2bdvn7Ro0UI+++wzmTx5skqPjIyU8nIWbRNR/eA4UCrXnp4l877KkdU7Dz6zrluzeLmxf5bKJyKqdeB0xhlnyPXXXy8nnXSSz2jhv/32m7Rq1aomX0lEdNylNYuVBxdly+6iCmmWGOl95AqmFyzPkbvPacetQkS1b+P07LPPSu/evSU/P189m65Ro0Yq/eeff5YrrriiJl9JRFQnA2DuLnZIUblbdhZUyO4ih/of00hHPhFRrUuc0IMODcL9zZw5syZfR0RUJ0qdbikoc0m523ccJzVd5lL5RES1LnGCFStWyNVXX62GINi5c6dKe/XVV+Xrr7+u6VcSER1XMbZwcXo0adUoWro2T1DPp+uWkaimkY58IqJaB06onhs2bJgakmDVqlXicPxVnF1YWMgH/xJRvdEoJkLO7pwqRRVu+fXPQlm7s1BW7yhQ00hHPhFRrQOnBx54QObOnSsLFiyQiIiDJxYMR4BAioioPnA6PJIWFymxNt9TIaaRjnwiIqMalUNv2rRJ+vXrVyU9ISFBCgoKavKVRETHXUSkVVZu2SPdWybJya0s4nBXij3cKh5NU+l9spK5VYio9oFTkyZNZPPmzVWGHkD7pszMzJp8JRHRcbdnX7mcmpUqS9fvkW37D45B1yo5Ss7olKbyiYhqHTjdcMMNMmHCBHnxxRfFYrHIrl271KNXbrvtNpk+fXpNvpKI6LhLSoiUz7/Z9neJU7K3xMldWSmf/ZYrd53dgVuFiGofOE2dOlUqKytl8ODBUlZWpqrt7Ha73HHHHWpgTCKiekGzSKVY5INfd4vVcjC5UhNplhil8omIat04HKVMd999t+zfv1/WrVsn3333nRoME22cWrduXZOvJCI67nYXlcupmcmSkRQlGLpJf2Ea6cgnIqpx4IRhB6ZNmyY9e/ZUPej+97//SadOndSjVtq3by9PPfWUTJo0SeoaRjZH+ys8O69Xr17yww8/1PUsEVEIirVHyKI1u6VZUpQM75IuZ3duov7HNNKRT0RU46o6tF+aN2+eDBkyRL799lu55JJL5Nprr1UlTo899piaDgur26eJv/nmm+qhwxguAUHTk08+qcacQk/A1NTUOp03IgotGY3sMmdIrCS0yJTsvQ4pLHNKUrRNslLsUrg9WyIb2et6FomoPgdOb7/9trzyyisyfPhwVUXXpUsXcbvd8uuvv6rqu1Dw+OOPq8brCOgAAdSiRYtUQ3a0zSIi0lXsKpGK5Ex54uNsWb2zyJverVm8jBuYKbKrRCQzjSuMiGpWVffnn39Kjx491N+dO3dWDcJRNRcqQZPT6VQPGkaJmM5qtapp9PojIjJKaBYr81bk+ARNgOk5y3NUPhFRjUucPB6P2Gy2gx8OD5fY2NA5sezdu1fNY1qa7x0ipjdu3BiwzZb+uBgoKvr75Ll6tUgILRcRHRt7f9stjh82ywkB8hy5InvjikQOpHP1EzV0JSXHJnDSNE1GjRqlSpqgoqJCbrrpJomJifF533//+1+pD2bNmiUzZ86smtG/f13MDhEdZyibPlg+HcDLx29eiKh+OKLAaeTIkT7TV199tYSSlJQU1Tg9Ly/PJx3TGO3cH3oIoiG5scQpIyND5KuvWOJEZAKf/7Zbnli6udr8SYOzZMgJLHEiMkWJU//+Rz9wWrhwoYQyVCOiDdbSpUvl/PPPV2kYqBPTt9xyS5X3o+RMLz3z0a2bSHz88ZhlIqpDKUl5Yi9OlFXbqz5js3uLREk5PUukNRuHEzV4Rb7tHI/6AJihDCVICxYskJdfflk2bNggY8eOldLSUm8vOyIinSYiN/ZtpXrRGWF6bL9WKp+IqNaPXAlll112mRrFHGNO5ebmSrdu3eSTTz6p0mCciGjLXofYy7bKfed28I7jlBBtk3YpdtmxfaNsKW8tJ/FhCETUkAMnQLVcoKo5IiKjIodLHv28TCrcP6nn0+nw3LrIcKvcPszFFUZEDbuqjogoWPH2CHG6K32CJsA00pFPRNTgS5yIiILRPNku3ZrHS1KsXRKjbOL0eMQeHiYHypxyoMSh8omIjBg4EZFp7dhdItf3y5S5y3JkyYZ838bhAzJV/qlt2D6SiA5i4EREppWeGiuPfpot4WFWOT0zWVyVlRJhtUqFu1LmLMuR24e1q+tZJKIQw8CJiExrT7FT8goqZET3pmIJs0qF0yPRtjDxeCrlg1W7VD4RkREDJyIyrfJyp1zeu4UsWrNbsveUetPbpcaodOQTERmxVx0RmVbzlJgqQRNgGunIJyIyYuBERKZVUOaW3EJHwDykI5+IyIiBExGZ1r4yhzRLipR4u2+rBUw3S4pS+URERmzjRESmhQEuN+WVSPPESElPjJRKTROrxSLlTrdsyivmAJhEVAUDJyIyrTYpdjVm05Z9eOxKpWgaHrdikQq3R6Ujn4jIiFV1RGRa+3eVyY39M6V1crQaeiC/xCl5xQ41fdOATJVPRGTEEiciMi1rcpR8vjpbrunTSoaXu6XE4ZY4e7gkRIXLkrVb5OxuHACTiHwxcCIi03JWuKVJcqrMWbZFfs8/WLrUtnG0nNk5XeUTERmxqo6ITMtis8rSDXmyxRA0AaaRjnwiIiOWOBGRaRWVuWV9bomEW0UirBZvuqdSU+nIJyIyYuBERKaF3nNoz1RY7hZ3peaTh3TkExEZsRyaiEwrMjxM0uMjJT7SbwDMyHBJT4hU+URERixxIiLTykqxS/MEq1zQrbVUWqxq4MsYW7iIVik/bs1T+URERgyciMi0Sj1hcsnJmTJnWY6s3lnkTcfgl+MGZqp8IiIjVtURkWnFhHlk3ooc2VlYIc3w2JUEu/of03OW56h8IiIjljgRkWlt3uuQVdsPljQZYRRx5Hdrfdxni4hCGEuciMi0ihyuWuUTkfkwcCIi04q3R9Qqn4jMh1V1RGRa6DXXvUW8rN9VImF/30ZaLBZxezTp1DSWveqIqAqWOBGRaaHX3I19M6VDWqyUOivVq8ThUdNj+7FXHRFVxRInIjKt6LBK+WhTjlzTu6Vc6HRLSYVbYiPDJdYWLks35shlPdrX9SwSUYhh4EREprVlb4W8s6pI3vhpbZU8PL+uZ8sKOYm96ojIgFV1RGRa7FVHREeKJU5EZFroNeepFEmKCpewMKtomkiYRcTlqZSCcjd71RFRFQyciMi0WiTbpU+bZPnpjwPiKHd70+3hFpWOfCIiI1bVEZFpuTwi53ZtKm0bx/qkY3p416Yqn4jIiCVORGRaB8pcsvvANnlgeAfJ3uuQwjKnJEXb1PhNX2RvlBhbm7qeRSIKMQyciMi04hweOaFpltz3cbas3nnwmXXdmsXL2AFZEll2sPqOiAhYVUdEppWQGilzluX4BE2AaaQjn4jIiCVORGRam/c6ZOOeEhnRtYkkx9jE6dbEHmGVfSUO+XT9HpXfjeM4EZEBAyciMq0Sh0v+0aulfLEpT7bkl3nT2zSOVunIJyIyYlUdEZlW86ToKkETYBrpyCciMmLgRESm5XBVyr4SZ8A8pCOfiMiIgRMRmdaekgrJSo2VxCjfVguYRjryiYiM2MaJiEwrzh4hv+0skvZNMACmRdyeSgkPw/2kptIv75lR17NIRCGGgRMRmRYGumyfFiur//QdjkAfywn5RERGrKojItMq9YTJ2AGZKkgywvS4gZkqn4jIiCVORGRaHrdb3vhus0w9u4Ns3//XI1cSo22SkWyXeV9slGv7tq/rWSSiEMPAiYhMq6jcLS6xy82vr5H9pQd712EwzE5N4lQ+EZERq+qIyLRKXG4pd7nF5fYddgDTSEc+EVG9C5y2bdsmo0ePltatW0tUVJS0adNGZsyYIU6n7/gra9askb59+0pkZKRkZGTI7Nmz62yeiSj0hVus0tjmlClD28j4QVky+rRWMmFQlppGOvKJiOpdVd3GjRulsrJS5s2bJ1lZWbJu3Tq54YYbpLS0VB599FH1nqKiIhk6dKgMGTJE5s6dK2vXrpXrrrtOEhMTZcyYMXW9CEQUgpKirdItK0Ne/f5P2bSn1JvePjVGLuieofKJiIwsmqZpUg898sgjMmfOHMnJyVHT+Pvuu++W3NxcsdlsKm3q1Kny/vvvq8ArGAi+EhISpLCwUOLjfXvZEFHD8212rsz83ybZlFtSJQ9jO804u730adekTuaNiI6fI7n+19vbKSxccnKyd3rlypXSr18/b9AEw4YNk02bNsmBAwcCfofD4VAry/giIvPYU+KSnQcCjw6OdOQTEdX7wGnz5s3yzDPPyI033uhNQ0lTWlqaz/v0aeQFMmvWLBVh6i+0iyIi89hf5pSUWJvE2n1bLWAa6cgnIgqZwAlVaRaL5ZAv/2q2nTt3yplnnimXXHKJaudUG9OmTVMlV/prx44dtVwiIqpP4u0Rsn1/mUTbrNKqUbS0TI5W/2Ma6cgnIgqZxuG33XabjBo16pDvyczM9P69a9cuGThwoPTp00fmz5/v874mTZpIXl6eT5o+jbxA7Ha7ehGROWWlREq3jHhZtR3V9L6lS91b4JErkXU2b0QUmuo0cGrcuLF6BQMlTQiaevToIQsXLhSr1bewrHfv3qpxuMvlkoiIv+4SlyxZIu3bt5ekpKRjMv9EVL/FxkTLjX0zZc6yHFm9s8jnkStj+2WqfCKiejccAYKmAQMGSMuWLdXwA/n5+d48vTTpyiuvlJkzZ6rxnqZMmaKGLHjqqafkiSeeqMM5J6JQVlRaIdvzcuTuc9rJ1n1/PXIlIdomrRvZZXVOtjSK6Yjwqq5nk4hCSL0InFByhAbheDVv3twnTx9NAY27P/vsMxk3bpwqlUpJSZHp06dzDCciqta2/WXy5PISafrrBomyhUmlpkmY1SJlDo/sKiyX+5PKpHtrrkAiagDjOB0LHMeJyFxe/HqLLF2/RzblFcve0oNDD6TEREiHtDgZ1ClVrju9TZ3OIxEde6YYx4mIqLZSYyJlY16xFDncqiddVIRV/Y/pDXnFKp+IqN5V1RERHQvR9jBpnxIpl/RqJcUVHilxuCU+Mlxi7GHy9vfbVD4RkREDJyIyLU+xQ0ae3iZwr7oBbVQ+EZERq+qIyLQap0bK81/niFhETmmVJD1aJEqv1klqGunIJyIyYokTEZnWlr0OCbOGS87eYtlnaBzeKCZC2qbGqfxu7FVHRAYMnIjItJyVlfL7Ht+gCdT0nmKVT0RkxKo6IjKtCKu1StCkQzryiYiMeFYgItOyhVkks1FUwDykI5+IyIiBExGZltujSf/2qVWCJ0wjHflEREZs40REppWZYpe3Vx2QHq2SpV+7MKlwV0pkuFVKnR7ZuPuAjDgxuIeQE5F5MHAiItMq3O+SUX0yA47jNG5gpsoX9qojIgMGTkRkXrER8tmqbLm8V3MZVuaRUodbYuzhkhQdJp+szZHh3dvV9RwSUYhh4EREpoXRBjbscUmRM1+cbk2cnkqxh1klItwiOwtcci5HIyAiPwyciMi0yt2V0jE9Sb7bsk92Fh58vEqzBLv0adNI5RMRGbFXHRGZli1c5PucfXKg3CUJUeHqAb/4H9Mrc/apfCIiI54WiMi0nM5K8VRaxOmuFHelJqKJWCwinkrtr3QnS5yIyBdLnIjItPKKK6Rnq0RpEh/5dxsnTRxuTU0jHflEREYscSIi04qPjJD1fxTJzRh6oOKvXnWxdlTZhckLX26TAe04jhMR+WLgRESm1TzJLlf3bSEvf/uHbNpT6k1vnxqj0pFPRGTEqjoiMi27VWRlzn6fHnWAaaQjn4jIiKcFIjKtzXsd8vmGPRJts0qrRtHSMjla/Y9ppCOfiMiIVXVEZFpFDpegM92eYif62AXMJyIyYokTEZlWvD2iVvlEZD4scSIi08pKscvJLeNFxCLJMXY1lpMt3Cp71TAEmsonIjJiiRMRmVZhSaVcf3qmuNyafLp+jyzdmC+L1+Wp6TF9M1U+EZERS5yIyLQSYq0yd8lmaZ4cJU2TosThqhR7hFWsFpEXv82RKWe0q+tZJKIQw8CJiEwLveZW5hRJjK1EVdGpR65YRQVQpc5Kld+tdV3PJRGFEgZORGRa6DWXFB0upRg13On2ptusFpXOXnVE5I+BExGZVkJkhAqanBiTwEBNV3hUPhGREQMnIjKtpkl2OaFpnMRFRUhStE1cHk3s4VbZV+qQ4nKXyiciMmLgRESmtW9vhVzft7UsWL5Vlv++z5verVm83NAvU+VLmzqdRSIKMRyOgIhMKzE5UgVNtogw6de2kfTOTFb/Y3rB8hyVT0RkxBInIjKtPUVO+X1PicRHR4g9PEwqNU3CLBapcHukqMyl8omIjBg4EZFpFZQ7pU1qrGzfXya7Cw8+0DcxKlylI5+IyIiBExGZVqMYu+w6UCEnZSRIcmykuN2VYov465Era/8sVvlEREYMnIjItBIjI2TESeny2fo9siP7YOPwjKQolY58IiIjNg4nItMqKndKXlGFlDpdPumYRjryiYiMWOJERKbl8GjyyW97pH1arLRqFCZuT6WEh1nF4fKo9AHtUut6FokoxDBwIiLTQi+6pOgI+W13cZW8xrE2lU9EZMSqOiIyrciIMMlKjZXkGN+2TJhumxqr8omIjFjiRESm1SguQpxut3RuGiciFnG4K9UjV0Q0KXW4VT4RkRFLnIjItPbsq5BrereSwjK3euTK91sPqP8xPbJ3a5VPRGTEEiciMq3khEh57LON0qZxnHRvkShOjOMUbpWCMpc8v2KL3Da0Q13PIhGFGAZORGRaxRVu2VXgEJdHk8QYu2oMHm6xyP5Sh+QXO1U+EZERAyciMq0DpU4Z0CFVvt+6Tzbk7femZyRFqnTkExHV6zZODodDunXrJhaLRVavXu2Tt2bNGunbt69ERkZKRkaGzJ49u87mk4hCX0qcXQVN5a5KSY+3S1qcTf2PaaQjn4ioXgdOd955pzRt2rRKelFRkQwdOlRatmwpP//8szzyyCNy3333yfz58+tkPoko9EXZrBJmsUhBmVPySxyyr/Sv/zGNdOQTERnVq7PC4sWL5bPPPpNHH320St5rr70mTqdTXnzxRTnhhBPk8ssvl/Hjx8vjjz9eJ/NKRKGvzKXJ6W0bS/PEKJ90TCMd+URE9bKNU15entxwww3y/vvvS3R0dJX8lStXSr9+/cRms3nThg0bJv/617/kwIEDkpSUdJznmIhCXbw9TIrLXdK/fYqUOiqlwu2RyPAwibFbpaDUpfKJiOpdiZOmaTJq1Ci56aabpGfPngHfk5ubK2lpaT5p+jTyqmsvhSo+44uIzKN9k3hJirHJ3hK3atekaRb1P6aRjnwiopAJnKZOnaoaeR/qtXHjRnnmmWekuLhYpk2bdlR/f9asWZKQkOB9oUE5EZlH4/hIubBHM/VcOp/0WJtc3LO5yiciMrJoKM6pI/n5+bJv375DviczM1MuvfRS+eijj1QgpfN4PBIWFiZXXXWVvPzyy3LNNdeoEiNU5em+/PJLGTRokOzfvz9gVR1KnPDS4fMIngoLCyU+nneaRGaRX1Qhm3KLpMjhUdVzKGli0ERkHkVFRaoAJZjrf522cWrcuLF6Hc7TTz8tDzzwgHd6165dqv3Sm2++Kb169VJpvXv3lrvvvltcLpdERPz1fKklS5ZI+/btq23fZLfb1YuIzA1BEgMlImowjcNbtGjhMx0bG6v+b9OmjTRv3lz9feWVV8rMmTNl9OjRMmXKFFm3bp089dRT8sQTT9TJPBMREVHDUy8Cp2CgiA1DFYwbN0569OghKSkpMn36dBkzZkxdzxoRERE1EHXaxqk+13ESERGR+a7/9WI4AiIiIqJQwMCJiIiIKEgMnIiIiIiCxMCJiIiIyGy96o4GvZ08H71CRERkHkV/P3ItmP5yDJwM8FgX4KNXiIiIzBkHJCQkHPI9HI7AoLKyUo1KHhcX5/N4l6NFf6TLjh07GvRwB1zOhoXbs2Hh9mx4uE1rDyVNCJqaNm0qVuuhWzGxxMkAK0sfifxYQtDUkAMnHZezYeH2bFi4PRsebtPaOVxJk46Nw4mIiIiCxMCJiIiIKEgMnI4ju90uM2bMUP83ZFzOhoXbs2Hh9mx4uE2PLzYOJyIiIgoSS5yIiIiIgsTAiYiIiChIDJyIiIiIgsTAqQbmzJkjXbp08Y6Z0bt3b1m8eLE3Pzc3V/7xj39IkyZNJCYmRrp37y7vvvvuYb/32WeflVatWklkZKT06tVLfvjhB2loy3nfffepwUWNrw4dOkhdO9yybtmyRS644AJp3Lixyr/00kslLy+vwW3TmixnqG5To4cffljN18SJE71pFRUVMm7cOGnUqJHExsbKRRdddNhlxSB506dPl/T0dImKipIhQ4bI77//Lg1tOUeNGlVlm5555pkSyss5f/58GTBggNpvkVdQUBDUd4XaMXosljPUj9GH/ZZz//79cuutt0r79u3VcdaiRQsZP368FBYWhsTxycCpBjBIJjb0zz//LD/99JMMGjRIRowYIb/99pvKv+aaa2TTpk3y4Ycfytq1a+XCCy9UF6Bffvml2u988803ZfLkyarX3apVq6Rr164ybNgw2bNnjzSk5YQTTjhBdu/e7X19/fXXUtcOtaylpaUydOhQdWB/8cUX8s0334jT6ZTzzjtPjTbfULZpTZczVLep7scff5R58+apgNFo0qRJ8tFHH8nbb78tX331lXpqAPbhQ5k9e7Y8/fTTMnfuXPn+++/VDQO2KYKThrScgEDJuE3/85//SCiobjnLysrUPN91111Bf1coHqPHYjlD+Rj9McByYh/F69FHH5V169bJSy+9JJ988omMHj06NI5PjY6KpKQk7fnnn1d/x8TEaK+88opPfnJysrZgwYJqP3/KKado48aN8057PB6tadOm2qxZsxrUcs6YMUPr2rWrVh/oy/rpp59qVqtVKyws9OYVFBRoFotFW7JkSYPZpjVdzlDepsXFxVrbtm3V/Pfv31+bMGGCd7kiIiK0t99+2/veDRs24Ome2sqVKwN+V2VlpdakSRPtkUce8abhe+x2u/af//xHayjLCSNHjtRGjBihhZrqltPoyy+/VMt34MCBw35fqB6jR3s5Q/UYLQ5iOXVvvfWWZrPZNJfLVefHJ0ucasnj8cgbb7yh7tZR7QF9+vRRdzIobsSdOvIR8aJ4NRDc1aMEAMWKxse/YHrlypXSUJZTh6JTPA8oMzNTrrrqKtm+fbuEEv9ldTgcqhTGOP4WivWxjaq7a6uP27Qmyxnq2xRVVOecc47PdgBsG5fL5ZOOqgtUCVS3fbZu3aqqp42fwSMaUL1T19v0aC6nbtmyZZKamqqqS8aOHSv79u2TulbdctZEKB+jR3M5Q/kYHXcEy4lqOlRPhoeH1/nxyWfV1RCqpnCxQaCAdgPvvfeedOrUSeW99dZbctlll6k2BdjI0dHRKj8rKyvgd+3du1ddxNLS0nzSMb1x40ZpKMsJ2IlR7IqTMYqLZ86cKX379lXFsXi4ciguK9r7oMh3ypQp8tBDD6l69KlTp6pthmVoKNu0JssZytsUQSGqX1AV4A8nWJvNJomJiVW2D/IC0dMDbdPqPlMflxNQFYTqvNatW6t2b6gWOuuss9QFKCwsTEJtOWsiVI/Ro72coXqMvnEEy4ltdf/998uYMWOqfc/xPD4ZONUQdsDVq1erKPidd96RkSNHqvYDuADde++9qsHe559/LikpKfL++++rtj8rVqyQE088Ucy8nDj56lCnjQO6ZcuWKgg7XP11XS4r2ofgrhv157grveKKK1Rj+MM9RTsUHe3lDMVtumPHDpkwYYIsWbJElZo1VMdqOS+//HLv3ziWsV3btGmjSqEGDx4sxxu3Z+2E2jG64wj226KiIlUqhfMTGrmHhKNa8WdigwcP1saMGaNt3rxZ1TuvW7euSv6NN94Y8LMOh0MLCwvT3nvvPZ/0a665Rhs+fLjWUJazOj179tSmTp2qhRp9WY3y8/O9bQrS0tK02bNnN5htWpPlDNVtivWO/RPbQH9hGu218Pfnn38esH1IixYttMcffzzgd27ZskV95pdffvFJ79evnzZ+/HitoSxndVJSUrS5c+dqobicbrf7iNv+hOIxeiyWMxSP0feCXM6ioiKtd+/e6hxVXl5+yO88nsdn/btdDlFo44M2IujxAP536Cjerq5nEorSe/ToIUuXLvX5Pkzr7YkawnIGUlJSoqoC0H001OjLaoSSNVR7oNcZet4MHz68wWzTmixnqG5TlIqgShIla/qrZ8+eqm2H/ndERITP9kEPUbT7qG77oNoKQ28YP4O7YfTeqatteiyWM5A///xTtXGqq216uOWsSfVhKB6jx2I5Q/EYHRzEcuLYQi9fbCf03D5cydRxPT6PahhmEojSv/rqK23r1q3amjVr1DQi5c8++0xzOp1aVlaW1rdvX+37779XJTOPPvqoyl+0aJH3OwYNGqQ988wz3uk33nhDtf5/6aWXtPXr16sSgMTERC03N7dBLedtt92mLVu2TH3nN998ow0ZMkTdye7Zs0erS4daVnjxxRdVLyQs56uvvqp6D06ePNnnO+r7Nq3pcobqNvXn32vnpptuUiUvX3zxhfbTTz+pO1u8jNq3b6/997//9U4//PDDaht+8MEHav2h51nr1q0Pezdcn5YTPZ1uv/12tR9gm6LUqnv37qr3U0VFhRaqy7l7925V2oBevbi0LV++XE3v27evXh2jx2I568Mx2t+wnOjZ26tXL+3EE09U5yIss/4ylrrV1fHJwKkGrrvuOq1ly5aqa2Tjxo1VMaJ+4YHs7Gztwgsv1FJTU7Xo6GitS5cuVbrt4/PoImqEHR0nOHwvusl+9913WkNbzssuu0xLT09X39msWTM1jQOjrh1uWadMmaKqrNC1GxeQxx57THV/bWjbtCbLGarb9HAXIJxMb775ZjUcA/bfCy64QJ2YjXBhWrhwoXca6+Lee+9V6wgXXKy/TZs2aQ1pOcvKyrShQ4eq/QP7Abb3DTfcEFLBRKDlxD6J5fB/GbdffThGj8Vy1odjtL9hOfVqyEAvBH91fXxa/v5xIiIiIjoMtnEiIiIiChIDJyIiIqIgMXAiIiIiChIDJyIiIqIgMXAiIiIiChIDJyIiIqIgMXAiIiIiChIDJyIiIqIgMXAioqNq1KhRcv7553unBwwYIBMnTjzua3nZsmVisVikoKCg2vcg//333z+u81VfbNu2Ta0fPDuMiA5i4ERkkmAGF0G88NDMrKws+ec//ylut/uY//Z///tfuf/++49asGMm5eXlEhMTI5s3b67rWSGiv4XrfxBRw3bmmWfKwoULxeFwyP/+9z8ZN26cREREyLRp06q81+l0qgDraEhOTj4q39OQuFwute4PZ8mSJdKyZUsV6BJRaGCJE5FJ2O12adKkiboQjx07VoYMGSIffvihT/Xagw8+KE2bNpX27dur9B07dsill14qiYmJKgAaMWKEqsLReTwemTx5sspv1KiR3HnnnXhwuM/v+lfVIXCbMmWKZGRkqHlCUPDCCy+o7x04cKB6T1JSkip5wnxBZWWlzJo1S1q3bi1RUVHStWtXeeedd3x+B8Fgu3btVD6+xzifh7J371654IILJDo6Wtq2betdJ7qvvvpKTjnlFDWv6enpMnXqVJ+SulatWsmTTz7p85lu3brJfffd553GssyZM0eGDx+uSpCwng8cOCBXXXWVNG7cWM0zfhuBrdEHH3ygPlNYWChhYWHy008/edcHtsepp57qfe///d//qXWqO9y2g+eff146duwokZGR0qFDB3nuueeqXU/Y1tddd5163/bt24Nat0QNEQMnIpPCxRolS7qlS5fKpk2bVCnHxx9/rEpFhg0bJnFxcbJixQr55ptvJDY2VpVc6Z977LHH5KWXXpIXX3xRvv76a9m/f7+89957h/zda665Rv7zn//I008/LRs2bJB58+ap78VF/91331XvwXzs3r1bnnrqKTWNoOmVV16RuXPnym+//SaTJk2Sq6++WgU1epBw4YUXynnnnafa5Fx//fUqwAnGzJkzVYCxZs0aOfvss1Uwg+WAnTt3qrSTTz5Zfv31VxX8IMh74IEHjnh9I5BCgLZ27VoVgNx7772yfv16Wbx4sVoP+O6UlBTv+xEcYTsg4ElISFDBGKoyAd+BYOyXX36RkpISlYZ10b9/f/V3MNvutddek+nTp6sgDr//0EMPqXl6+eWXq8w7gt1LLrlErVt8X4sWLY54+YkaDI2IGryRI0dqI0aMUH9XVlZqS5Ys0ex2u3b77bd789PS0jSHw+H9zKuvvqq1b99evV+H/KioKO3TTz9V0+np6drs2bO9+S6XS2vevLn3t6B///7ahAkT1N+bNm1CcZT6/UC+/PJLlX/gwAFvWkVFhRYdHa19++23Pu8dPXq0dsUVV6i/p02bpnXq1Mknf8qUKVW+yx/y77nnHu90SUmJSlu8eLGavuuuu6qsg2effVaLjY3VPB6Pmm7ZsqX2xBNP+Hxv165dtRkzZvj8zsSJE33ec95552nXXntttfP2zTffaKmpqd7fmTx5snbOOeeov5988kntsssuU7+jz2tWVpY2f/78oLddmzZttNdff93nN++//36td+/e6u+tW7eq+V6xYoU2ePBg7fTTT9cKCgqqnV8is2AbJyKTQOkFSh1QGoHSjCuvvNKnOunEE0/0adeEEhY0SkaphVFFRYVs2bJFVR+hVKhXr17evPDwcOnZs2eV6jodSixQ5aSXjAQD81BWViZnnHGGTzpKTk466ST1N0pMjPMBvXv3Dur7u3Tp4v0b1Wjx8fGyZ88e7/fie1C6ozvttNNUKc+ff/55RCUvWC9GqC696KKLZNWqVTJ06FBVVdqnTx+farpzzz1XrNa/KgawzlDahSozlC7hM6h6RSkUlgHrCdWiwWy70tJS9f/o0aPlhhtu8OajChKlW0ZXXHGFNG/eXL744gtVSklkdgyciEwC7X5QHYTgCO2YEOQYIWgwQnDQo0cPVaXjD+1yaqImF169KmrRokXSrFkznzy0O6ot/0baCJIQWAYLgY1/oIjg1J//+j3rrLPkjz/+UG2zUD06ePBg1WD/0UcfVfloa/Xwww9739+vXz8pLi5Wgdby5ctV1RoCJ7wHbb6wTdFOKphtp6/TBQsWVAk4EdgaoaoS7adWrlwpgwYNCnq9EDVUDJyITAIX7iPpndW9e3d58803JTU1VZXCBILG0t9//726qOslFj///LP6bCAo1UJQghITNE73p5d4oVRF16lTJxUgoUFydSVVaODs36j7u+++k9rC96LdFQIjvdQJ7YVQkoNSGD0QQcmbrqioSLZu3RrU9+OzI0eOVK++ffvKHXfcoQKn33//XQVVxlI2NPJGydK///1vFeyhkTa2zWWXXaZKE43r5nDbDqVKCLRycnJUm65DQclY586dVSN1BK9HUlpI1BCxcTgRBYQLKhoro3EyGgQjGEC10Pjx41U1FUyYMEGVeGAQyY0bN8rNN998yDGY0AMNQQIaR+Mz+ne+9dZbKh89/hCgIBDIz89XJSMIUm6//XbVIBwNl1HFhFKXZ555xtuQ+aabblLBBgIPNCx//fXXVaP12sLyoOH5rbfeqpYP1WczZsxQPQn1KjSUwrz66qtqHaHRNpbPv9QmEDTMxvehSg0N3rHMCNQA6Qgs0dPPCFVxKEXSgxf0lsNnECQZA5pgth0axaPRPRrpZ2dnq3lHr77HH3+8yrxi+dEgHlWH6ARAZGYMnIgoIFy0USWEdjzosYYLNNrEoJ2MXopx2223yT/+8Q8VLKAtEIIc9Bw7FFQXXnzxxSooQakJ2tigzQ2gKg4XdPSIS0tLk1tuuUWlYwBN9PjChR7zgd5hKP3A8ASAeUTJEIIxVFuh9x2qsmoL84OqtB9++EF9LwI0rIN77rnH+x6Mg4WgBUHFOeeco9oqtWnT5rDfjdI1fBalSCixQ7D1xhtv+AxD4A+/g9I4vS0T4G//tGC2HXoeYjgCBEsoCcR3I9jU16k/DCmBbYOqu2+//fYI1yRRw2FBC/G6ngkiIjo4rhSqQFEyhOCRiEILS5yIiEIIxpBCdRmDJqLQxBInIiIioiCxxImIiIgoSAyciIiIiILEwImIiIgoSAyciIiIiILEwImIiIgoSAyciIiIiILEwImIiIgoSAyciIiIiILEwImIiIgoSAyciIiIiCQ4/w/OTwDlZ6rszwAAAABJRU5ErkJggg==",
            "text/plain": [
              "<Figure size 600x400 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# %% Residual diagnostics for the best model on TEST\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "feat_cols = experiment_features.get(best_name, X_train_i.columns)\n",
        "# Pick the matching test frame for best model\n",
        "if \"poly\" in best_name:\n",
        "    X_test_match = Xte_poly\n",
        "elif \"interactions\" in best_name:\n",
        "    X_test_match = X_test_exp\n",
        "else:\n",
        "    X_test_match = X_test_i\n",
        "\n",
        "y_pred_test = best_model.predict(X_test_match)\n",
        "residuals = y_test - y_pred_test\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.histplot(residuals, bins=40, kde=True, edgecolor=\"black\")\n",
        "plt.title(f\"Residuals distribution — {best_name} (test)\")\n",
        "plt.xlabel(\"Residual (y - ŷ) [hours]\"); plt.ylabel(\"Frequency\")\n",
        "plt.tight_layout(); plt.show()\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "sns.scatterplot(x=y_pred_test, y=residuals, alpha=0.3)\n",
        "plt.axhline(0, color=\"red\", lw=1)\n",
        "plt.title(f\"Residuals vs Predictions — {best_name} (test)\")\n",
        "plt.xlabel(\"Predicted hours/week\"); plt.ylabel(\"Residual\")\n",
        "plt.tight_layout(); plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Top experiments by validation MAE (after HuberRegressor_strict):\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>experiment</th>\n",
              "      <th>train_MAE</th>\n",
              "      <th>train_MSE</th>\n",
              "      <th>train_RMSE</th>\n",
              "      <th>train_R2</th>\n",
              "      <th>val_MAE</th>\n",
              "      <th>val_MSE</th>\n",
              "      <th>val_RMSE</th>\n",
              "      <th>val_R2</th>\n",
              "      <th>test_MAE</th>\n",
              "      <th>test_MSE</th>\n",
              "      <th>test_RMSE</th>\n",
              "      <th>test_R2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Dummy_median</td>\n",
              "      <td>7.403928</td>\n",
              "      <td>152.986505</td>\n",
              "      <td>12.368771</td>\n",
              "      <td>-0.000978</td>\n",
              "      <td>7.336919</td>\n",
              "      <td>150.332693</td>\n",
              "      <td>12.261023</td>\n",
              "      <td>-0.001191</td>\n",
              "      <td>7.453749</td>\n",
              "      <td>153.351414</td>\n",
              "      <td>12.383514</td>\n",
              "      <td>-0.002563</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>HuberRegressor</td>\n",
              "      <td>7.403599</td>\n",
              "      <td>152.949577</td>\n",
              "      <td>12.367278</td>\n",
              "      <td>-0.000736</td>\n",
              "      <td>7.338164</td>\n",
              "      <td>150.324899</td>\n",
              "      <td>12.260705</td>\n",
              "      <td>-0.001140</td>\n",
              "      <td>7.457233</td>\n",
              "      <td>153.390449</td>\n",
              "      <td>12.385090</td>\n",
              "      <td>-0.002818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>HuberRegressor_strict</td>\n",
              "      <td>7.403599</td>\n",
              "      <td>152.949577</td>\n",
              "      <td>12.367278</td>\n",
              "      <td>-0.000736</td>\n",
              "      <td>7.338164</td>\n",
              "      <td>150.324899</td>\n",
              "      <td>12.260705</td>\n",
              "      <td>-0.001140</td>\n",
              "      <td>7.457233</td>\n",
              "      <td>153.390449</td>\n",
              "      <td>12.385090</td>\n",
              "      <td>-0.002818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>SGD_interactions_huber</td>\n",
              "      <td>7.485635</td>\n",
              "      <td>152.513053</td>\n",
              "      <td>12.349618</td>\n",
              "      <td>0.002120</td>\n",
              "      <td>7.421685</td>\n",
              "      <td>149.923194</td>\n",
              "      <td>12.244313</td>\n",
              "      <td>0.001536</td>\n",
              "      <td>7.544989</td>\n",
              "      <td>153.057285</td>\n",
              "      <td>12.371632</td>\n",
              "      <td>-0.000640</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SGD_baseline_huber</td>\n",
              "      <td>7.494254</td>\n",
              "      <td>152.614605</td>\n",
              "      <td>12.353728</td>\n",
              "      <td>0.001455</td>\n",
              "      <td>7.429802</td>\n",
              "      <td>150.016083</td>\n",
              "      <td>12.248105</td>\n",
              "      <td>0.000917</td>\n",
              "      <td>7.551661</td>\n",
              "      <td>153.196184</td>\n",
              "      <td>12.377245</td>\n",
              "      <td>-0.001548</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>SGD_poly2_huber</td>\n",
              "      <td>7.488265</td>\n",
              "      <td>152.547157</td>\n",
              "      <td>12.350998</td>\n",
              "      <td>0.001897</td>\n",
              "      <td>7.430721</td>\n",
              "      <td>149.990826</td>\n",
              "      <td>12.247074</td>\n",
              "      <td>0.001085</td>\n",
              "      <td>7.550120</td>\n",
              "      <td>153.200700</td>\n",
              "      <td>12.377427</td>\n",
              "      <td>-0.001578</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Dummy_mean</td>\n",
              "      <td>7.565096</td>\n",
              "      <td>152.837052</td>\n",
              "      <td>12.362728</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>7.495684</td>\n",
              "      <td>150.155110</td>\n",
              "      <td>12.253779</td>\n",
              "      <td>-0.000009</td>\n",
              "      <td>7.606176</td>\n",
              "      <td>153.016736</td>\n",
              "      <td>12.369993</td>\n",
              "      <td>-0.000375</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Lasso_alpha0.01</td>\n",
              "      <td>7.739182</td>\n",
              "      <td>151.396846</td>\n",
              "      <td>12.304343</td>\n",
              "      <td>0.009423</td>\n",
              "      <td>7.677394</td>\n",
              "      <td>149.199258</td>\n",
              "      <td>12.214715</td>\n",
              "      <td>0.006357</td>\n",
              "      <td>7.810460</td>\n",
              "      <td>153.222153</td>\n",
              "      <td>12.378294</td>\n",
              "      <td>-0.001718</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Lasso_alpha0.001</td>\n",
              "      <td>7.761449</td>\n",
              "      <td>151.372543</td>\n",
              "      <td>12.303355</td>\n",
              "      <td>0.009582</td>\n",
              "      <td>7.703332</td>\n",
              "      <td>149.319716</td>\n",
              "      <td>12.219645</td>\n",
              "      <td>0.005555</td>\n",
              "      <td>7.836077</td>\n",
              "      <td>153.367269</td>\n",
              "      <td>12.384154</td>\n",
              "      <td>-0.002667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Lasso_baseline</td>\n",
              "      <td>7.761449</td>\n",
              "      <td>151.372543</td>\n",
              "      <td>12.303355</td>\n",
              "      <td>0.009582</td>\n",
              "      <td>7.703332</td>\n",
              "      <td>149.319716</td>\n",
              "      <td>12.219645</td>\n",
              "      <td>0.005555</td>\n",
              "      <td>7.836077</td>\n",
              "      <td>153.367269</td>\n",
              "      <td>12.384154</td>\n",
              "      <td>-0.002667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Lasso_alpha0.0005</td>\n",
              "      <td>7.762870</td>\n",
              "      <td>151.372264</td>\n",
              "      <td>12.303344</td>\n",
              "      <td>0.009584</td>\n",
              "      <td>7.704952</td>\n",
              "      <td>149.326581</td>\n",
              "      <td>12.219926</td>\n",
              "      <td>0.005509</td>\n",
              "      <td>7.837587</td>\n",
              "      <td>153.374009</td>\n",
              "      <td>12.384426</td>\n",
              "      <td>-0.002711</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Ridge_alpha10.0</td>\n",
              "      <td>7.763872</td>\n",
              "      <td>151.372181</td>\n",
              "      <td>12.303340</td>\n",
              "      <td>0.009585</td>\n",
              "      <td>7.706066</td>\n",
              "      <td>149.330448</td>\n",
              "      <td>12.220084</td>\n",
              "      <td>0.005483</td>\n",
              "      <td>7.838653</td>\n",
              "      <td>153.378972</td>\n",
              "      <td>12.384626</td>\n",
              "      <td>-0.002743</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                experiment  train_MAE   train_MSE  train_RMSE  train_R2  \\\n",
              "0             Dummy_median   7.403928  152.986505   12.368771 -0.000978   \n",
              "1           HuberRegressor   7.403599  152.949577   12.367278 -0.000736   \n",
              "2    HuberRegressor_strict   7.403599  152.949577   12.367278 -0.000736   \n",
              "3   SGD_interactions_huber   7.485635  152.513053   12.349618  0.002120   \n",
              "4       SGD_baseline_huber   7.494254  152.614605   12.353728  0.001455   \n",
              "5          SGD_poly2_huber   7.488265  152.547157   12.350998  0.001897   \n",
              "6               Dummy_mean   7.565096  152.837052   12.362728  0.000000   \n",
              "7          Lasso_alpha0.01   7.739182  151.396846   12.304343  0.009423   \n",
              "8         Lasso_alpha0.001   7.761449  151.372543   12.303355  0.009582   \n",
              "9           Lasso_baseline   7.761449  151.372543   12.303355  0.009582   \n",
              "10       Lasso_alpha0.0005   7.762870  151.372264   12.303344  0.009584   \n",
              "11         Ridge_alpha10.0   7.763872  151.372181   12.303340  0.009585   \n",
              "\n",
              "     val_MAE     val_MSE   val_RMSE    val_R2  test_MAE    test_MSE  \\\n",
              "0   7.336919  150.332693  12.261023 -0.001191  7.453749  153.351414   \n",
              "1   7.338164  150.324899  12.260705 -0.001140  7.457233  153.390449   \n",
              "2   7.338164  150.324899  12.260705 -0.001140  7.457233  153.390449   \n",
              "3   7.421685  149.923194  12.244313  0.001536  7.544989  153.057285   \n",
              "4   7.429802  150.016083  12.248105  0.000917  7.551661  153.196184   \n",
              "5   7.430721  149.990826  12.247074  0.001085  7.550120  153.200700   \n",
              "6   7.495684  150.155110  12.253779 -0.000009  7.606176  153.016736   \n",
              "7   7.677394  149.199258  12.214715  0.006357  7.810460  153.222153   \n",
              "8   7.703332  149.319716  12.219645  0.005555  7.836077  153.367269   \n",
              "9   7.703332  149.319716  12.219645  0.005555  7.836077  153.367269   \n",
              "10  7.704952  149.326581  12.219926  0.005509  7.837587  153.374009   \n",
              "11  7.706066  149.330448  12.220084  0.005483  7.838653  153.378972   \n",
              "\n",
              "    test_RMSE   test_R2  \n",
              "0   12.383514 -0.002563  \n",
              "1   12.385090 -0.002818  \n",
              "2   12.385090 -0.002818  \n",
              "3   12.371632 -0.000640  \n",
              "4   12.377245 -0.001548  \n",
              "5   12.377427 -0.001578  \n",
              "6   12.369993 -0.000375  \n",
              "7   12.378294 -0.001718  \n",
              "8   12.384154 -0.002667  \n",
              "9   12.384154 -0.002667  \n",
              "10  12.384426 -0.002711  \n",
              "11  12.384626 -0.002743  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Best experiment by validation MAE: Dummy_median\n",
            "Best model saved -> best_model.pkl\n",
            "Best HGBR params: {'min_samples_leaf': 20, 'max_leaf_nodes': 15, 'max_depth': 4, 'learning_rate': 0.05, 'l2_regularization': 0.0}\n",
            "\n",
            "Top 12 experiments by validation MAE (updated):\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>experiment</th>\n",
              "      <th>train_MAE</th>\n",
              "      <th>train_MSE</th>\n",
              "      <th>train_RMSE</th>\n",
              "      <th>train_R2</th>\n",
              "      <th>val_MAE</th>\n",
              "      <th>val_MSE</th>\n",
              "      <th>val_RMSE</th>\n",
              "      <th>val_R2</th>\n",
              "      <th>test_MAE</th>\n",
              "      <th>test_MSE</th>\n",
              "      <th>test_RMSE</th>\n",
              "      <th>test_R2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Dummy_median</td>\n",
              "      <td>7.403928</td>\n",
              "      <td>152.986505</td>\n",
              "      <td>12.368771</td>\n",
              "      <td>-0.000978</td>\n",
              "      <td>7.336919</td>\n",
              "      <td>150.332693</td>\n",
              "      <td>12.261023</td>\n",
              "      <td>-0.001191</td>\n",
              "      <td>7.453749</td>\n",
              "      <td>153.351414</td>\n",
              "      <td>12.383514</td>\n",
              "      <td>-0.002563</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>HuberRegressor</td>\n",
              "      <td>7.403599</td>\n",
              "      <td>152.949577</td>\n",
              "      <td>12.367278</td>\n",
              "      <td>-0.000736</td>\n",
              "      <td>7.338164</td>\n",
              "      <td>150.324899</td>\n",
              "      <td>12.260705</td>\n",
              "      <td>-0.001140</td>\n",
              "      <td>7.457233</td>\n",
              "      <td>153.390449</td>\n",
              "      <td>12.385090</td>\n",
              "      <td>-0.002818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>HuberRegressor_strict</td>\n",
              "      <td>7.403599</td>\n",
              "      <td>152.949577</td>\n",
              "      <td>12.367278</td>\n",
              "      <td>-0.000736</td>\n",
              "      <td>7.338164</td>\n",
              "      <td>150.324899</td>\n",
              "      <td>12.260705</td>\n",
              "      <td>-0.001140</td>\n",
              "      <td>7.457233</td>\n",
              "      <td>153.390449</td>\n",
              "      <td>12.385090</td>\n",
              "      <td>-0.002818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>SGD_interactions_huber</td>\n",
              "      <td>7.485635</td>\n",
              "      <td>152.513053</td>\n",
              "      <td>12.349618</td>\n",
              "      <td>0.002120</td>\n",
              "      <td>7.421685</td>\n",
              "      <td>149.923194</td>\n",
              "      <td>12.244313</td>\n",
              "      <td>0.001536</td>\n",
              "      <td>7.544989</td>\n",
              "      <td>153.057285</td>\n",
              "      <td>12.371632</td>\n",
              "      <td>-0.000640</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SGD_baseline_huber</td>\n",
              "      <td>7.494254</td>\n",
              "      <td>152.614605</td>\n",
              "      <td>12.353728</td>\n",
              "      <td>0.001455</td>\n",
              "      <td>7.429802</td>\n",
              "      <td>150.016083</td>\n",
              "      <td>12.248105</td>\n",
              "      <td>0.000917</td>\n",
              "      <td>7.551661</td>\n",
              "      <td>153.196184</td>\n",
              "      <td>12.377245</td>\n",
              "      <td>-0.001548</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>SGD_poly2_huber</td>\n",
              "      <td>7.488265</td>\n",
              "      <td>152.547157</td>\n",
              "      <td>12.350998</td>\n",
              "      <td>0.001897</td>\n",
              "      <td>7.430721</td>\n",
              "      <td>149.990826</td>\n",
              "      <td>12.247074</td>\n",
              "      <td>0.001085</td>\n",
              "      <td>7.550120</td>\n",
              "      <td>153.200700</td>\n",
              "      <td>12.377427</td>\n",
              "      <td>-0.001578</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Dummy_mean</td>\n",
              "      <td>7.565096</td>\n",
              "      <td>152.837052</td>\n",
              "      <td>12.362728</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>7.495684</td>\n",
              "      <td>150.155110</td>\n",
              "      <td>12.253779</td>\n",
              "      <td>-0.000009</td>\n",
              "      <td>7.606176</td>\n",
              "      <td>153.016736</td>\n",
              "      <td>12.369993</td>\n",
              "      <td>-0.000375</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>HGBR_tuned</td>\n",
              "      <td>7.602901</td>\n",
              "      <td>150.840334</td>\n",
              "      <td>12.281707</td>\n",
              "      <td>0.013064</td>\n",
              "      <td>7.573243</td>\n",
              "      <td>149.539457</td>\n",
              "      <td>12.228633</td>\n",
              "      <td>0.004091</td>\n",
              "      <td>7.678988</td>\n",
              "      <td>152.488921</td>\n",
              "      <td>12.348640</td>\n",
              "      <td>0.003075</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>HGBR_baseline</td>\n",
              "      <td>7.611764</td>\n",
              "      <td>148.748540</td>\n",
              "      <td>12.196251</td>\n",
              "      <td>0.026751</td>\n",
              "      <td>7.654214</td>\n",
              "      <td>150.163696</td>\n",
              "      <td>12.254130</td>\n",
              "      <td>-0.000066</td>\n",
              "      <td>7.754728</td>\n",
              "      <td>152.822224</td>\n",
              "      <td>12.362129</td>\n",
              "      <td>0.000896</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Lasso_alpha0.01</td>\n",
              "      <td>7.739182</td>\n",
              "      <td>151.396846</td>\n",
              "      <td>12.304343</td>\n",
              "      <td>0.009423</td>\n",
              "      <td>7.677394</td>\n",
              "      <td>149.199258</td>\n",
              "      <td>12.214715</td>\n",
              "      <td>0.006357</td>\n",
              "      <td>7.810460</td>\n",
              "      <td>153.222153</td>\n",
              "      <td>12.378294</td>\n",
              "      <td>-0.001718</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>ElasticNet_a0.01_l1r0.8</td>\n",
              "      <td>7.742518</td>\n",
              "      <td>151.390567</td>\n",
              "      <td>12.304087</td>\n",
              "      <td>0.009464</td>\n",
              "      <td>7.681334</td>\n",
              "      <td>149.212866</td>\n",
              "      <td>12.215272</td>\n",
              "      <td>0.006266</td>\n",
              "      <td>7.814517</td>\n",
              "      <td>153.242893</td>\n",
              "      <td>12.379131</td>\n",
              "      <td>-0.001854</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>ElasticNet_a0.01_l1r0.5</td>\n",
              "      <td>7.747694</td>\n",
              "      <td>151.382910</td>\n",
              "      <td>12.303776</td>\n",
              "      <td>0.009514</td>\n",
              "      <td>7.687433</td>\n",
              "      <td>149.238753</td>\n",
              "      <td>12.216331</td>\n",
              "      <td>0.006094</td>\n",
              "      <td>7.820639</td>\n",
              "      <td>153.276828</td>\n",
              "      <td>12.380502</td>\n",
              "      <td>-0.002076</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 experiment  train_MAE   train_MSE  train_RMSE  train_R2  \\\n",
              "0              Dummy_median   7.403928  152.986505   12.368771 -0.000978   \n",
              "1            HuberRegressor   7.403599  152.949577   12.367278 -0.000736   \n",
              "2     HuberRegressor_strict   7.403599  152.949577   12.367278 -0.000736   \n",
              "3    SGD_interactions_huber   7.485635  152.513053   12.349618  0.002120   \n",
              "4        SGD_baseline_huber   7.494254  152.614605   12.353728  0.001455   \n",
              "5           SGD_poly2_huber   7.488265  152.547157   12.350998  0.001897   \n",
              "6                Dummy_mean   7.565096  152.837052   12.362728  0.000000   \n",
              "7                HGBR_tuned   7.602901  150.840334   12.281707  0.013064   \n",
              "8             HGBR_baseline   7.611764  148.748540   12.196251  0.026751   \n",
              "9           Lasso_alpha0.01   7.739182  151.396846   12.304343  0.009423   \n",
              "10  ElasticNet_a0.01_l1r0.8   7.742518  151.390567   12.304087  0.009464   \n",
              "11  ElasticNet_a0.01_l1r0.5   7.747694  151.382910   12.303776  0.009514   \n",
              "\n",
              "     val_MAE     val_MSE   val_RMSE    val_R2  test_MAE    test_MSE  \\\n",
              "0   7.336919  150.332693  12.261023 -0.001191  7.453749  153.351414   \n",
              "1   7.338164  150.324899  12.260705 -0.001140  7.457233  153.390449   \n",
              "2   7.338164  150.324899  12.260705 -0.001140  7.457233  153.390449   \n",
              "3   7.421685  149.923194  12.244313  0.001536  7.544989  153.057285   \n",
              "4   7.429802  150.016083  12.248105  0.000917  7.551661  153.196184   \n",
              "5   7.430721  149.990826  12.247074  0.001085  7.550120  153.200700   \n",
              "6   7.495684  150.155110  12.253779 -0.000009  7.606176  153.016736   \n",
              "7   7.573243  149.539457  12.228633  0.004091  7.678988  152.488921   \n",
              "8   7.654214  150.163696  12.254130 -0.000066  7.754728  152.822224   \n",
              "9   7.677394  149.199258  12.214715  0.006357  7.810460  153.222153   \n",
              "10  7.681334  149.212866  12.215272  0.006266  7.814517  153.242893   \n",
              "11  7.687433  149.238753  12.216331  0.006094  7.820639  153.276828   \n",
              "\n",
              "    test_RMSE   test_R2  \n",
              "0   12.383514 -0.002563  \n",
              "1   12.385090 -0.002818  \n",
              "2   12.385090 -0.002818  \n",
              "3   12.371632 -0.000640  \n",
              "4   12.377245 -0.001548  \n",
              "5   12.377427 -0.001578  \n",
              "6   12.369993 -0.000375  \n",
              "7   12.348640  0.003075  \n",
              "8   12.362129  0.000896  \n",
              "9   12.378294 -0.001718  \n",
              "10  12.379131 -0.001854  \n",
              "11  12.380502 -0.002076  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "New best experiment by validation MAE: Dummy_median\n",
            "Best model saved -> best_model.pkl\n",
            "\n",
            "Feature importance / coefficients for: Dummy_median\n",
            "This model does not expose importances/coeffs.\n"
          ]
        }
      ],
      "source": [
        "# %% Additional models to beat Dummy: HuberRegressor, ElasticNet, HistGradientBoosting (+tuning), stronger RF\n",
        "\n",
        "from sklearn.linear_model import HuberRegressor, ElasticNet\n",
        "from sklearn.ensemble import HistGradientBoostingRegressor, RandomForestRegressor\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "# 1) HuberRegressor (closed-form robust linear)\n",
        "# %% HuberRegressor (robust linear) — increased iterations + tighter tolerance\n",
        "from sklearn.linear_model import HuberRegressor\n",
        "\n",
        "hub = HuberRegressor(\n",
        "    epsilon=1.35,     # robust to outliers\n",
        "    alpha=1e-4,       # small L2 regularization\n",
        "    max_iter=5000,    # ↑ allow more optimization steps\n",
        "    tol=1e-5,         # ↓ tighter convergence tolerance\n",
        "    fit_intercept=True\n",
        ")\n",
        "res, mdl = run_model(\"HuberRegressor_strict\", hub,\n",
        "                     X_train_i, y_train, X_val_i, y_val, X_test_i, y_test)\n",
        "results.append(res); fitted_models[\"HuberRegressor_strict\"] = mdl\n",
        "experiment_features[\"HuberRegressor_strict\"] = X_train_i.columns\n",
        "\n",
        "# Rebuild results table and persist\n",
        "results_df = pd.DataFrame(results).sort_values(by=\"val_MAE\").reset_index(drop=True)\n",
        "results_df.to_csv(\"experiments_results.csv\", index=False)\n",
        "\n",
        "print(\"\\nTop experiments by validation MAE (after HuberRegressor_strict):\")\n",
        "try:\n",
        "    from IPython.display import display\n",
        "    display(results_df.head(12))\n",
        "except Exception:\n",
        "    print(results_df.head(12))\n",
        "\n",
        "best_name = results_df.loc[0, \"experiment\"]\n",
        "print(f\"\\nBest experiment by validation MAE: {best_name}\")\n",
        "best_model = fitted_models[best_name]\n",
        "import joblib\n",
        "joblib.dump(best_model, \"best_model.pkl\")\n",
        "print(\"Best model saved -> best_model.pkl\")\n",
        "\n",
        "# 2) ElasticNet\n",
        "for a in [0.0005, 0.001, 0.01]:\n",
        "    for l1r in [0.2, 0.5, 0.8]:\n",
        "        en = ElasticNet(alpha=a, l1_ratio=l1r, max_iter=6000)\n",
        "        name = f\"ElasticNet_a{a}_l1r{l1r}\"\n",
        "        res, mdl = run_model(name, en,\n",
        "                             X_train_i, y_train, X_val_i, y_val, X_test_i, y_test)\n",
        "        results.append(res); fitted_models[name] = mdl\n",
        "        experiment_features[name] = X_train_i.columns\n",
        "\n",
        "# 3) HistGradientBoostingRegressor — baseline\n",
        "hgb_base = HistGradientBoostingRegressor(\n",
        "    learning_rate=0.1,\n",
        "    max_depth=6,\n",
        "    max_leaf_nodes=None,     # let depth control complexity\n",
        "    min_samples_leaf=20,\n",
        "    l2_regularization=1.0,\n",
        "    early_stopping=True,\n",
        "    n_iter_no_change=20,\n",
        "    random_state=RANDOM_STATE\n",
        ")\n",
        "res, mdl = run_model(\"HGBR_baseline\", hgb_base,\n",
        "                     X_train_i, y_train, X_val_i, y_val, X_test_i, y_test)\n",
        "results.append(res); fitted_models[\"HGBR_baseline\"] = mdl\n",
        "experiment_features[\"HGBR_baseline\"] = X_train_i.columns\n",
        "\n",
        "# 4) HistGradientBoostingRegressor — quick tuning with RandomizedSearchCV (targeting MAE)\n",
        "param_distributions = {\n",
        "    \"learning_rate\": [0.05, 0.075, 0.1, 0.15, 0.2],\n",
        "    \"max_depth\": [None, 4, 6, 8, 10],\n",
        "    \"max_leaf_nodes\": [15, 31, 63, 127, None],\n",
        "    \"min_samples_leaf\": [10, 20, 30, 50],\n",
        "    \"l2_regularization\": [0.0, 0.1, 0.5, 1.0, 5.0],\n",
        "}\n",
        "hgb = HistGradientBoostingRegressor(\n",
        "    early_stopping=True, n_iter_no_change=20,\n",
        "    random_state=RANDOM_STATE\n",
        ")\n",
        "search = RandomizedSearchCV(\n",
        "    estimator=hgb,\n",
        "    param_distributions=param_distributions,\n",
        "    n_iter=25,              # adjust if you want faster/slower\n",
        "    cv=3,\n",
        "    scoring=\"neg_mean_absolute_error\",\n",
        "    n_jobs=-1,\n",
        "    random_state=RANDOM_STATE,\n",
        "    verbose=0,\n",
        ")\n",
        "search.fit(X_train_i, y_train)\n",
        "best_hgb = search.best_estimator_\n",
        "print(\"Best HGBR params:\", search.best_params_)\n",
        "\n",
        "res, mdl = run_model(\"HGBR_tuned\", best_hgb,\n",
        "                     X_train_i, y_train, X_val_i, y_val, X_test_i, y_test)\n",
        "results.append(res); fitted_models[\"HGBR_tuned\"] = mdl\n",
        "experiment_features[\"HGBR_tuned\"] = X_train_i.columns\n",
        "\n",
        "# 5) Stronger RandomForest (often benefits from more trees + sqrt features)\n",
        "rf_strong = RandomForestRegressor(\n",
        "    n_estimators=1000,\n",
        "    max_features=\"sqrt\",\n",
        "    min_samples_leaf=2,\n",
        "    min_samples_split=4,\n",
        "    random_state=RANDOM_STATE,\n",
        "    n_jobs=-1\n",
        ")\n",
        "res, mdl = run_model(\"RandomForest_strong\", rf_strong,\n",
        "                     X_train_i, y_train, X_val_i, y_val, X_test_i, y_test)\n",
        "results.append(res); fitted_models[\"RandomForest_strong\"] = mdl\n",
        "experiment_features[\"RandomForest_strong\"] = X_train_i.columns\n",
        "\n",
        "# ---- Rebuild results table and persist best ----\n",
        "results_df = pd.DataFrame(results).sort_values(by=\"val_MAE\").reset_index(drop=True)\n",
        "results_df.to_csv(\"experiments_results.csv\", index=False)\n",
        "\n",
        "print(\"\\nTop 12 experiments by validation MAE (updated):\")\n",
        "try:\n",
        "    from IPython.display import display\n",
        "    display(results_df.head(12))\n",
        "except Exception:\n",
        "    print(results_df.head(12))\n",
        "\n",
        "best_name = results_df.loc[0, \"experiment\"]\n",
        "print(f\"\\nNew best experiment by validation MAE: {best_name}\")\n",
        "\n",
        "best_model = fitted_models[best_name]\n",
        "import joblib\n",
        "joblib.dump(best_model, \"best_model.pkl\")\n",
        "print(\"Best model saved -> best_model.pkl\")\n",
        "\n",
        "# Optional: show importances for tree-based winners\n",
        "def show_feature_importance(name, model, feature_names):\n",
        "    print(f\"\\nFeature importance / coefficients for: {name}\")\n",
        "    import numpy as np, pandas as pd\n",
        "    try:\n",
        "        from IPython.display import display\n",
        "    except Exception:\n",
        "        display = print\n",
        "    if hasattr(model, \"feature_importances_\"):\n",
        "        imp = pd.Series(model.feature_importances_, index=feature_names).sort_values(ascending=False)\n",
        "        try:\n",
        "            display(imp.head(25).to_frame(\"importance\"))\n",
        "        except Exception:\n",
        "            print(imp.head(25))\n",
        "    elif hasattr(model, \"coef_\"):\n",
        "        coef = pd.Series(model.coef_, index=feature_names).sort_values(key=np.abs, ascending=False)\n",
        "        try:\n",
        "            display(coef.head(25).to_frame(\"coefficient\"))\n",
        "        except Exception:\n",
        "            print(coef.head(25))\n",
        "    else:\n",
        "        print(\"This model does not expose importances/coeffs.\")\n",
        "\n",
        "feat_cols = experiment_features.get(best_name, X_train_i.columns)\n",
        "show_feature_importance(best_name, best_model, feat_cols)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best HGBR(abs) params: {'min_samples_leaf': 50, 'max_depth': 4, 'learning_rate': 0.1, 'l2_regularization': 0.5}\n",
            "\n",
            "Top 15 experiments by validation MAE (updated):\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>experiment</th>\n",
              "      <th>train_MAE</th>\n",
              "      <th>train_MSE</th>\n",
              "      <th>train_RMSE</th>\n",
              "      <th>train_R2</th>\n",
              "      <th>val_MAE</th>\n",
              "      <th>val_MSE</th>\n",
              "      <th>val_RMSE</th>\n",
              "      <th>val_R2</th>\n",
              "      <th>test_MAE</th>\n",
              "      <th>test_MSE</th>\n",
              "      <th>test_RMSE</th>\n",
              "      <th>test_R2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Dummy_median</td>\n",
              "      <td>7.403928</td>\n",
              "      <td>152.986505</td>\n",
              "      <td>12.368771</td>\n",
              "      <td>-0.000978</td>\n",
              "      <td>7.336919</td>\n",
              "      <td>150.332693</td>\n",
              "      <td>12.261023</td>\n",
              "      <td>-0.001191</td>\n",
              "      <td>7.453749</td>\n",
              "      <td>153.351414</td>\n",
              "      <td>12.383514</td>\n",
              "      <td>-0.002563</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>HGBR_absErr_tuned</td>\n",
              "      <td>7.403928</td>\n",
              "      <td>152.986505</td>\n",
              "      <td>12.368771</td>\n",
              "      <td>-0.000978</td>\n",
              "      <td>7.336919</td>\n",
              "      <td>150.332693</td>\n",
              "      <td>12.261023</td>\n",
              "      <td>-0.001191</td>\n",
              "      <td>7.453749</td>\n",
              "      <td>153.351414</td>\n",
              "      <td>12.383514</td>\n",
              "      <td>-0.002563</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>HuberRegressor_strict</td>\n",
              "      <td>7.403599</td>\n",
              "      <td>152.949577</td>\n",
              "      <td>12.367278</td>\n",
              "      <td>-0.000736</td>\n",
              "      <td>7.338164</td>\n",
              "      <td>150.324899</td>\n",
              "      <td>12.260705</td>\n",
              "      <td>-0.001140</td>\n",
              "      <td>7.457233</td>\n",
              "      <td>153.390449</td>\n",
              "      <td>12.385090</td>\n",
              "      <td>-0.002818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>HuberRegressor</td>\n",
              "      <td>7.403599</td>\n",
              "      <td>152.949577</td>\n",
              "      <td>12.367278</td>\n",
              "      <td>-0.000736</td>\n",
              "      <td>7.338164</td>\n",
              "      <td>150.324899</td>\n",
              "      <td>12.260705</td>\n",
              "      <td>-0.001140</td>\n",
              "      <td>7.457233</td>\n",
              "      <td>153.390449</td>\n",
              "      <td>12.385090</td>\n",
              "      <td>-0.002818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>HGBR_absErr_baseline</td>\n",
              "      <td>7.403928</td>\n",
              "      <td>152.886349</td>\n",
              "      <td>12.364722</td>\n",
              "      <td>-0.000323</td>\n",
              "      <td>7.340430</td>\n",
              "      <td>150.137286</td>\n",
              "      <td>12.253052</td>\n",
              "      <td>0.000110</td>\n",
              "      <td>7.458269</td>\n",
              "      <td>153.379272</td>\n",
              "      <td>12.384639</td>\n",
              "      <td>-0.002745</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>GBR_absErr</td>\n",
              "      <td>7.402915</td>\n",
              "      <td>152.876989</td>\n",
              "      <td>12.364343</td>\n",
              "      <td>-0.000261</td>\n",
              "      <td>7.343707</td>\n",
              "      <td>150.463178</td>\n",
              "      <td>12.266343</td>\n",
              "      <td>-0.002060</td>\n",
              "      <td>7.463302</td>\n",
              "      <td>153.507006</td>\n",
              "      <td>12.389794</td>\n",
              "      <td>-0.003580</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>SGD_interactions_huber</td>\n",
              "      <td>7.485635</td>\n",
              "      <td>152.513053</td>\n",
              "      <td>12.349618</td>\n",
              "      <td>0.002120</td>\n",
              "      <td>7.421685</td>\n",
              "      <td>149.923194</td>\n",
              "      <td>12.244313</td>\n",
              "      <td>0.001536</td>\n",
              "      <td>7.544989</td>\n",
              "      <td>153.057285</td>\n",
              "      <td>12.371632</td>\n",
              "      <td>-0.000640</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>SGD_baseline_huber</td>\n",
              "      <td>7.494254</td>\n",
              "      <td>152.614605</td>\n",
              "      <td>12.353728</td>\n",
              "      <td>0.001455</td>\n",
              "      <td>7.429802</td>\n",
              "      <td>150.016083</td>\n",
              "      <td>12.248105</td>\n",
              "      <td>0.000917</td>\n",
              "      <td>7.551661</td>\n",
              "      <td>153.196184</td>\n",
              "      <td>12.377245</td>\n",
              "      <td>-0.001548</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>SGD_poly2_huber</td>\n",
              "      <td>7.488265</td>\n",
              "      <td>152.547157</td>\n",
              "      <td>12.350998</td>\n",
              "      <td>0.001897</td>\n",
              "      <td>7.430721</td>\n",
              "      <td>149.990826</td>\n",
              "      <td>12.247074</td>\n",
              "      <td>0.001085</td>\n",
              "      <td>7.550120</td>\n",
              "      <td>153.200700</td>\n",
              "      <td>12.377427</td>\n",
              "      <td>-0.001578</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Dummy_mean</td>\n",
              "      <td>7.565096</td>\n",
              "      <td>152.837052</td>\n",
              "      <td>12.362728</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>7.495684</td>\n",
              "      <td>150.155110</td>\n",
              "      <td>12.253779</td>\n",
              "      <td>-0.000009</td>\n",
              "      <td>7.606176</td>\n",
              "      <td>153.016736</td>\n",
              "      <td>12.369993</td>\n",
              "      <td>-0.000375</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Stacking_linear+trees</td>\n",
              "      <td>7.939727</td>\n",
              "      <td>164.512185</td>\n",
              "      <td>12.826230</td>\n",
              "      <td>-0.076389</td>\n",
              "      <td>7.567340</td>\n",
              "      <td>149.409705</td>\n",
              "      <td>12.223326</td>\n",
              "      <td>0.004955</td>\n",
              "      <td>7.689103</td>\n",
              "      <td>152.995657</td>\n",
              "      <td>12.369141</td>\n",
              "      <td>-0.000237</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>HGBR_tuned</td>\n",
              "      <td>7.602901</td>\n",
              "      <td>150.840334</td>\n",
              "      <td>12.281707</td>\n",
              "      <td>0.013064</td>\n",
              "      <td>7.573243</td>\n",
              "      <td>149.539457</td>\n",
              "      <td>12.228633</td>\n",
              "      <td>0.004091</td>\n",
              "      <td>7.678988</td>\n",
              "      <td>152.488921</td>\n",
              "      <td>12.348640</td>\n",
              "      <td>0.003075</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>HGBR_baseline</td>\n",
              "      <td>7.611764</td>\n",
              "      <td>148.748540</td>\n",
              "      <td>12.196251</td>\n",
              "      <td>0.026751</td>\n",
              "      <td>7.654214</td>\n",
              "      <td>150.163696</td>\n",
              "      <td>12.254130</td>\n",
              "      <td>-0.000066</td>\n",
              "      <td>7.754728</td>\n",
              "      <td>152.822224</td>\n",
              "      <td>12.362129</td>\n",
              "      <td>0.000896</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>Lasso_alpha0.01</td>\n",
              "      <td>7.739182</td>\n",
              "      <td>151.396846</td>\n",
              "      <td>12.304343</td>\n",
              "      <td>0.009423</td>\n",
              "      <td>7.677394</td>\n",
              "      <td>149.199258</td>\n",
              "      <td>12.214715</td>\n",
              "      <td>0.006357</td>\n",
              "      <td>7.810460</td>\n",
              "      <td>153.222153</td>\n",
              "      <td>12.378294</td>\n",
              "      <td>-0.001718</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>ElasticNet_a0.01_l1r0.8</td>\n",
              "      <td>7.742518</td>\n",
              "      <td>151.390567</td>\n",
              "      <td>12.304087</td>\n",
              "      <td>0.009464</td>\n",
              "      <td>7.681334</td>\n",
              "      <td>149.212866</td>\n",
              "      <td>12.215272</td>\n",
              "      <td>0.006266</td>\n",
              "      <td>7.814517</td>\n",
              "      <td>153.242893</td>\n",
              "      <td>12.379131</td>\n",
              "      <td>-0.001854</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 experiment  train_MAE   train_MSE  train_RMSE  train_R2  \\\n",
              "0              Dummy_median   7.403928  152.986505   12.368771 -0.000978   \n",
              "1         HGBR_absErr_tuned   7.403928  152.986505   12.368771 -0.000978   \n",
              "2     HuberRegressor_strict   7.403599  152.949577   12.367278 -0.000736   \n",
              "3            HuberRegressor   7.403599  152.949577   12.367278 -0.000736   \n",
              "4      HGBR_absErr_baseline   7.403928  152.886349   12.364722 -0.000323   \n",
              "5                GBR_absErr   7.402915  152.876989   12.364343 -0.000261   \n",
              "6    SGD_interactions_huber   7.485635  152.513053   12.349618  0.002120   \n",
              "7        SGD_baseline_huber   7.494254  152.614605   12.353728  0.001455   \n",
              "8           SGD_poly2_huber   7.488265  152.547157   12.350998  0.001897   \n",
              "9                Dummy_mean   7.565096  152.837052   12.362728  0.000000   \n",
              "10    Stacking_linear+trees   7.939727  164.512185   12.826230 -0.076389   \n",
              "11               HGBR_tuned   7.602901  150.840334   12.281707  0.013064   \n",
              "12            HGBR_baseline   7.611764  148.748540   12.196251  0.026751   \n",
              "13          Lasso_alpha0.01   7.739182  151.396846   12.304343  0.009423   \n",
              "14  ElasticNet_a0.01_l1r0.8   7.742518  151.390567   12.304087  0.009464   \n",
              "\n",
              "     val_MAE     val_MSE   val_RMSE    val_R2  test_MAE    test_MSE  \\\n",
              "0   7.336919  150.332693  12.261023 -0.001191  7.453749  153.351414   \n",
              "1   7.336919  150.332693  12.261023 -0.001191  7.453749  153.351414   \n",
              "2   7.338164  150.324899  12.260705 -0.001140  7.457233  153.390449   \n",
              "3   7.338164  150.324899  12.260705 -0.001140  7.457233  153.390449   \n",
              "4   7.340430  150.137286  12.253052  0.000110  7.458269  153.379272   \n",
              "5   7.343707  150.463178  12.266343 -0.002060  7.463302  153.507006   \n",
              "6   7.421685  149.923194  12.244313  0.001536  7.544989  153.057285   \n",
              "7   7.429802  150.016083  12.248105  0.000917  7.551661  153.196184   \n",
              "8   7.430721  149.990826  12.247074  0.001085  7.550120  153.200700   \n",
              "9   7.495684  150.155110  12.253779 -0.000009  7.606176  153.016736   \n",
              "10  7.567340  149.409705  12.223326  0.004955  7.689103  152.995657   \n",
              "11  7.573243  149.539457  12.228633  0.004091  7.678988  152.488921   \n",
              "12  7.654214  150.163696  12.254130 -0.000066  7.754728  152.822224   \n",
              "13  7.677394  149.199258  12.214715  0.006357  7.810460  153.222153   \n",
              "14  7.681334  149.212866  12.215272  0.006266  7.814517  153.242893   \n",
              "\n",
              "    test_RMSE   test_R2  \n",
              "0   12.383514 -0.002563  \n",
              "1   12.383514 -0.002563  \n",
              "2   12.385090 -0.002818  \n",
              "3   12.385090 -0.002818  \n",
              "4   12.384639 -0.002745  \n",
              "5   12.389794 -0.003580  \n",
              "6   12.371632 -0.000640  \n",
              "7   12.377245 -0.001548  \n",
              "8   12.377427 -0.001578  \n",
              "9   12.369993 -0.000375  \n",
              "10  12.369141 -0.000237  \n",
              "11  12.348640  0.003075  \n",
              "12  12.362129  0.000896  \n",
              "13  12.378294 -0.001718  \n",
              "14  12.379131 -0.001854  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "New best experiment by validation MAE: Dummy_median\n",
            "Best model saved -> best_model.pkl\n",
            "\n",
            "5-fold CV MAE on train+val for top candidates (lower is better):\n",
            "Dummy_median                  CV MAE: 7.3905 +/- 0.0545\n",
            "HGBR_absErr_tuned             CV MAE: 7.3910 +/- 0.0543\n",
            "HuberRegressor_strict         CV MAE: 7.4324 +/- 0.0647\n",
            "HuberRegressor                CV MAE: 7.4324 +/- 0.0647\n",
            "HGBR_absErr_baseline          CV MAE: 7.3980 +/- 0.0511\n",
            "GBR_absErr                    CV MAE: 7.3909 +/- 0.0543\n"
          ]
        }
      ],
      "source": [
        "# %% Strong non-linear models optimized for MAE + ensembling + CV check\n",
        "\n",
        "from sklearn.ensemble import HistGradientBoostingRegressor, GradientBoostingRegressor, RandomForestRegressor, StackingRegressor\n",
        "from sklearn.linear_model import SGDRegressor, Ridge\n",
        "from sklearn.model_selection import RandomizedSearchCV, KFold, cross_val_score\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# 1) HistGradientBoostingRegressor — optimize MAE directly via absolute_error loss\n",
        "hgb_mae = HistGradientBoostingRegressor(\n",
        "    loss=\"absolute_error\",        # optimize MAE\n",
        "    learning_rate=0.1,\n",
        "    max_depth=6,\n",
        "    min_samples_leaf=20,\n",
        "    l2_regularization=1.0,\n",
        "    early_stopping=True,\n",
        "    n_iter_no_change=25,\n",
        "    random_state=RANDOM_STATE\n",
        ")\n",
        "res, mdl = run_model(\"HGBR_absErr_baseline\", hgb_mae,\n",
        "                     X_train_i, y_train, X_val_i, y_val, X_test_i, y_test)\n",
        "results.append(res); fitted_models[\"HGBR_absErr_baseline\"] = mdl\n",
        "experiment_features[\"HGBR_absErr_baseline\"] = X_train_i.columns\n",
        "\n",
        "# 2) Quick RandomizedSearchCV on HGBR (MAE scoring)\n",
        "param_distributions = {\n",
        "    \"learning_rate\": [0.05, 0.075, 0.1, 0.15],\n",
        "    \"max_depth\": [None, 4, 6, 8],\n",
        "    \"min_samples_leaf\": [10, 20, 30, 50],\n",
        "    \"l2_regularization\": [0.0, 0.1, 0.5, 1.0, 5.0],\n",
        "}\n",
        "hgb = HistGradientBoostingRegressor(\n",
        "    loss=\"absolute_error\",\n",
        "    early_stopping=True,\n",
        "    n_iter_no_change=25,\n",
        "    random_state=RANDOM_STATE\n",
        ")\n",
        "search = RandomizedSearchCV(\n",
        "    estimator=hgb,\n",
        "    param_distributions=param_distributions,\n",
        "    n_iter=25,\n",
        "    cv=3,\n",
        "    scoring=\"neg_mean_absolute_error\",\n",
        "    n_jobs=-1,\n",
        "    random_state=RANDOM_STATE,\n",
        "    verbose=0,\n",
        ")\n",
        "search.fit(X_train_i, y_train)\n",
        "best_hgb = search.best_estimator_\n",
        "print(\"Best HGBR(abs) params:\", search.best_params_)\n",
        "\n",
        "res, mdl = run_model(\"HGBR_absErr_tuned\", best_hgb,\n",
        "                     X_train_i, y_train, X_val_i, y_val, X_test_i, y_test)\n",
        "results.append(res); fitted_models[\"HGBR_absErr_tuned\"] = mdl\n",
        "experiment_features[\"HGBR_absErr_tuned\"] = X_train_i.columns\n",
        "\n",
        "# 3) GradientBoostingRegressor — absolute_error loss (MAE) as an alternative\n",
        "gbr_mae = GradientBoostingRegressor(\n",
        "    loss=\"absolute_error\",     # MAE\n",
        "    learning_rate=0.08,\n",
        "    n_estimators=700,\n",
        "    max_depth=3,               # depth is controlled via max_depth in base estimators indirectly; in sklearn GBR, use max_depth via base_estimator? Not available; we control complexity via n_estimators+learning_rate\n",
        "    random_state=RANDOM_STATE\n",
        ")\n",
        "res, mdl = run_model(\"GBR_absErr\", gbr_mae,\n",
        "                     X_train_i, y_train, X_val_i, y_val, X_test_i, y_test)\n",
        "results.append(res); fitted_models[\"GBR_absErr\"] = mdl\n",
        "experiment_features[\"GBR_absErr\"] = X_train_i.columns\n",
        "\n",
        "# 4) Stronger RandomForest\n",
        "rf_strong = RandomForestRegressor(\n",
        "    n_estimators=1200,\n",
        "    max_features=\"sqrt\",\n",
        "    min_samples_leaf=2,\n",
        "    min_samples_split=5,\n",
        "    bootstrap=True,\n",
        "    random_state=RANDOM_STATE,\n",
        "    n_jobs=-1\n",
        ")\n",
        "res, mdl = run_model(\"RandomForest_stronger\", rf_strong,\n",
        "                     X_train_i, y_train, X_val_i, y_val, X_test_i, y_test)\n",
        "results.append(res); fitted_models[\"RandomForest_stronger\"] = mdl\n",
        "experiment_features[\"RandomForest_stronger\"] = X_train_i.columns\n",
        "\n",
        "# 5) StackingRegressor — blend linear + tree models (use MAE as selection metric via our val split)\n",
        "stack = StackingRegressor(\n",
        "    estimators=[\n",
        "        (\"sgd_huber\", SGDRegressor(loss=\"huber\", epsilon=1.35, alpha=1e-4, max_iter=4000, tol=1e-3, random_state=RANDOM_STATE)),\n",
        "        (\"ridge\", Ridge(alpha=1.0)),\n",
        "        (\"rf\", RandomForestRegressor(n_estimators=600, random_state=RANDOM_STATE, n_jobs=-1)),\n",
        "        (\"hgb\", HistGradientBoostingRegressor(loss=\"absolute_error\", early_stopping=True, random_state=RANDOM_STATE)),\n",
        "    ],\n",
        "    final_estimator=Ridge(alpha=1.0),     # simple linear blender\n",
        "    n_jobs=-1,\n",
        "    passthrough=False\n",
        ")\n",
        "res, mdl = run_model(\"Stacking_linear+trees\", stack,\n",
        "                     X_train_i, y_train, X_val_i, y_val, X_test_i, y_test)\n",
        "results.append(res); fitted_models[\"Stacking_linear+trees\"] = mdl\n",
        "experiment_features[\"Stacking_linear+trees\"] = X_train_i.columns\n",
        "\n",
        "# ---- Rebuild results table and persist best ----\n",
        "results_df = pd.DataFrame(results).sort_values(by=\"val_MAE\").reset_index(drop=True)\n",
        "results_df.to_csv(\"experiments_results.csv\", index=False)\n",
        "\n",
        "print(\"\\nTop 15 experiments by validation MAE (updated):\")\n",
        "try:\n",
        "    from IPython.display import display\n",
        "    display(results_df.head(15))\n",
        "except Exception:\n",
        "    print(results_df.head(15))\n",
        "\n",
        "best_name = results_df.loc[0, \"experiment\"]\n",
        "print(f\"\\nNew best experiment by validation MAE: {best_name}\")\n",
        "\n",
        "best_model = fitted_models[best_name]\n",
        "import joblib\n",
        "joblib.dump(best_model, \"best_model.pkl\")\n",
        "print(\"Best model saved -> best_model.pkl\")\n",
        "\n",
        "# 6) (Optional) 5-fold CV on train+val for top contenders (robustness)\n",
        "top_candidates = [name for name in results_df[\"experiment\"].head(6).tolist()\n",
        "                  if name in fitted_models]\n",
        "\n",
        "print(\"\\n5-fold CV MAE on train+val for top candidates (lower is better):\")\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
        "X_trval = pd.concat([X_train_i, X_val_i])\n",
        "y_trval = pd.concat([y_train, y_val])\n",
        "\n",
        "for name in top_candidates:\n",
        "    est = fitted_models[name]\n",
        "    # Some estimators (like Stacking) may not be fully deterministic unless random_state is set everywhere\n",
        "    scores = -cross_val_score(est, X_trval, y_trval, cv=kf,\n",
        "                              scoring=\"neg_mean_absolute_error\", n_jobs=-1)\n",
        "    print(f\"{name:28s}  CV MAE: {scores.mean():.4f} +/- {scores.std():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 3 Summary — Modeling `hours-per-week`\n",
        "\n",
        "**Primary metric:** Mean Absolute Error (MAE).  \n",
        "**Why MAE?** It is directly interpretable in **hours**, robust to the legitimate right‑tail in the target distribution (50–99 hours), and aligns with the project’s interpretability goals. We also report MSE, RMSE, and R².\n",
        "\n",
        "**Models evaluated:**  \n",
        "- Linear (gradient descent): `SGDRegressor` with `squared_error` and `huber` losses  \n",
        "- Linear (OLS, comparison): `LinearRegression`  \n",
        "- Regularized linear: `Ridge`, `Lasso`  \n",
        "- Trees: `DecisionTreeRegressor`, `RandomForestRegressor`  \n",
        "- Naïve baselines: `DummyRegressor` (mean, median)\n",
        "\n",
        "**Best model (by validation MAE):** `SGD_interactions_huber`  \n",
        "- **Val MAE ≈ 7.42 h**, **Test MAE ≈ 7.55 h**, **Test R² ≈ ~0**  \n",
        "- Simple interaction terms (e.g., `age × education-num`, `is_married × sex_Male`) slightly improved linear SGD.  \n",
        "- Huber loss helped robustness to the long right tail in working hours.\n",
        "\n",
        "**Interpretation:**  \n",
        "- R² near 0 indicates the features explain little variance in exact hours worked, matching EDA: weak numeric correlations and categorical effects with modest effect sizes.  \n",
        "- On this dataset, predicting precise weekly hours is intrinsically hard without richer behavioral or job‑specific signals.\n",
        "\n",
        "**Pros & cons of chosen models:**  \n",
        "- **SGD (Huber):** scalable, robust to outliers, easy to regularize; but only captures linear/affine patterns (benefits from engineered interactions).  \n",
        "- **OLS (LinearRegression):** fast baseline; sensitive to outliers and multicollinearity.  \n",
        "- **Ridge/Lasso:** stabilize/infer sparsity; still linear.  \n",
        "- **Decision Tree/Random Forest:** capture non‑linearities and interactions; may overfit without strong signal; moderate gains here.  \n",
        "- **Naïve (Dummy):** provides a sanity‑check baseline.\n",
        "\n",
        "**Data handling:**  \n",
        "- Used preprocessed dataset from Task 1.  \n",
        "- Dropped 24 rows with missing target.  \n",
        "- Imputed feature NaNs/Infs **after split** with `fill_value=0` (mean of standardized features; 0 = “not present” for dummies).  \n",
        "- Train/Val/Test = 64%/16%/20%.\n",
        "\n",
        "**Areas for improvement:**  \n",
        "- Avoid preprocessing leakage by refactoring Task 1 into a `Pipeline` + `ColumnTransformer` (fit scalers on train only).  \n",
        "- Try gradient boosting for tabular regression (e.g., HistGradientBoosting, XGBoost/LightGBM/CatBoost).  \n",
        "- Explore richer feature engineering (non-linear transforms of capital gains/losses, more interactions, or domain‑driven aggregates).  \n",
        "- Consider quantile regression if tolerant to upper-tail variance is desired.  \n",
        "- Use K‑fold CV and Bayesian/Random search on the most promising model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After evaluating a broad set of regression models — including linear models (SGD, OLS, Ridge, Lasso), tree‑based models (Decision Tree, Random Forest), boosted models (HistGradientBoosting, GradientBoosting), regularized robust models (HuberRegressor), and ensembles — the best validation performance was achieved by the Dummy Median baseline:\n",
        "\n",
        "Validation MAE: 7.3369\n",
        "Test MAE: 7.4537\n",
        "5‑fold CV MAE: 7.3905 ± 0.0545\n",
        "\n",
        "More advanced models such as HGBR_absErr_tuned, GBR_absErr, and HuberRegressor_strict achieved nearly identical results (differences under 0.001 MAE, i.e. negligible).\n",
        "This confirms a key insight from the EDA:\n",
        "\n",
        "The available features in the UCI Adult dataset contain almost no predictive signal for the precise number of hours worked per week.\n",
        "\n",
        "Correlations with the target are extremely weak, categorical effects are modest, and the distribution is dominated by a 40‑hour peak with high natural variance. As a result, even sophisticated models converge to behavior extremely close to the median predictor.\n",
        "Thus, the Dummy Median baseline is selected as the final model, with the justification that no model can outperform it meaningfully given the dataset’s characteristics."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv-1 (3.14.2)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
