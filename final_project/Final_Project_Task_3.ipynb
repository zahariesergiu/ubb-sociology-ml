{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Final Project Task 3 - Census Modeling Regression**\n",
    "Requirements\n",
    "\n",
    "- You can use models (estmators) from sklearn, but feel free to use any library for traditional ML. \n",
    "    - Note: in sklearn, the LinearRegression estimator is based on OLS, a statistical method. Please use the SGDRegressor estimator, since this is based on gradient descent. \n",
    "    - You can use LinearRegression estimator, but only as comparison with the SGDRegressor - Optional.\n",
    "\n",
    "- Model Selection and Setup:\n",
    "    - Implement multiple models, to solve a regression problem using traditional ML:\n",
    "        - Linear Regression\n",
    "        - Decision Tree Regression\n",
    "        - Random Forest Regression - Optional\n",
    "        - Ridge Regression - Optional\n",
    "        - Lasso Regression - Optional\n",
    "    - Choose a loss (or experiment with different losses) for the model and justify the choice.\n",
    "        - MSE, MAE, RMSE, Huber Loss or others\n",
    "    - Justify model choices based on dataset characteristics and task requirements; specify model pros and cons.\n",
    "\n",
    "\n",
    "- Data Preparation\n",
    "    - Use the preprocessed datasets from Task 1.\n",
    "    - From the train set, create an extra validation set, if necesarry. So in total there will be: train, validation and test datasets.\n",
    "    - Be sure all models have their data preprocessed as needed. Some models require different, or no encoding for some features.\n",
    "\n",
    "\n",
    "- Model Training and Experimentation\n",
    "    - Establish a Baseline Model:\n",
    "        - For each model type, train a simple model with default settings as a baseline.\n",
    "        - Evaluate its performance to establish a benchmark for comparison.\n",
    "    - Make plots with train, validation loss and metric on epochs (or on steps), if applicable. - Optional\n",
    "    - Feature Selection:\n",
    "        - Use insights from EDA in Task 2 to identify candidate features by analyzing patterns, relationships, and distributions.\n",
    "    - Experimentation:\n",
    "        - For each baseline model type, iteratively experiment with different combinations of features and transformations.\n",
    "        - Experiment with feature engineering techniques such as interaction terms, polynomial features, or scaling transformations.\n",
    "        - Identify the best model which have the best performance metrics on test set.\n",
    "    - Hyperparameter Tuning:\n",
    "        - Perform hyperparameter tuning only on the best-performing model after evaluating all model types and experiments.\n",
    "        - Avoid tuning models that do not show strong baseline performance or are unlikely to outperform others based on experimentation.\n",
    "        - Ensure that hyperparameter tuning is done after completing feature selection, baseline modeling, and experimentation, ensuring that the model is stable and representative of the dataset.\n",
    "\n",
    "\n",
    "- Model Evaluation\n",
    "    - Evaluate models on the test dataset using regression metrics:\n",
    "        - Mean Absolute Error (MAE)\n",
    "        - Mean Squared Error (MSE)\n",
    "        - Root Mean Squared Error (RMSE)\n",
    "        - RÂ² Score\n",
    "    - Compare the results across different models. Save all experiment results into a table.\n",
    "\n",
    "Feature Importance - Optional\n",
    "- For applicable models (e.g., Decision Tree Regression), analyze feature importance and discuss its relevance to the problem.\n",
    "\n",
    "\n",
    "\n",
    "Deliverables\n",
    "\n",
    "- Notebook code with no errors.\n",
    "- Code and results from experiments. Create a table with all experiments results, include experiment name, metrics results.\n",
    "- Explain findings, choices, results.\n",
    "- Potential areas for improvement or further exploration.\n",
    "\n",
    "import pandas as pd\n",
    "data_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
    "columns = [\n",
    "    \"age\", \"workclass\", \"fnlwgt\", \"education\", \"education-num\", \"marital-status\",\n",
    "    \"occupation\", \"relationship\", \"race\", \"sex\", \"capital-gain\", \"capital-loss\",\n",
    "    \"hours-per-week\", \"native-country\", \"income\"\n",
    "]\n",
    "\n",
    "data = pd.read_csv(data_url, header=None, names=columns, na_values=\" ?\", skipinitialspace=True)\n",
    "data.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDRegressor, Ridge, Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "data_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
    "columns = [\n",
    "    \"age\", \"workclass\", \"fnlwgt\", \"education\", \"education-num\", \"marital-status\",\n",
    "    \"occupation\", \"relationship\", \"race\", \"sex\", \"capital-gain\", \"capital-loss\",\n",
    "    \"hours-per-week\", \"native-country\", \"income\"\n",
    "]\n",
    "\n",
    "data = pd.read_csv(data_url, header=None, names=columns, na_values=\" ?\", skipinitialspace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing values\n",
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical variables\n",
    "categorical_features = [\"workclass\", \"education\", \"marital-status\", \"occupation\", \"relationship\", \"race\", \"sex\", \"native-country\"]\n",
    "numerical_features = [\"age\", \"fnlwgt\", \"education-num\", \"capital-gain\", \"capital-loss\"]\n",
    "target = \"hours-per-week\"\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), numerical_features),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_features)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset\n",
    "X = data.drop(columns=[target, \"income\"])\n",
    "y = data[target]\n",
    "\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "models = {\n",
    "    \"SGDRegressor\": SGDRegressor(max_iter=1000, tol=1e-3, random_state=42),\n",
    "    \"DecisionTreeRegressor\": DecisionTreeRegressor(random_state=42),\n",
    "    \"RandomForestRegressor\": RandomForestRegressor(random_state=42),\n",
    "    \"RidgeRegressor\": Ridge(),\n",
    "    \"LassoRegressor\": Lasso()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate models\n",
    "results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    pipeline = Pipeline([\n",
    "        (\"preprocessor\", preprocessor),\n",
    "        (\"regressor\", model)\n",
    "    ])\n",
    "    \n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    \n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "results.append({\n",
    "        \"Model\": name,\n",
    "        \"MAE\": mae,\n",
    "        \"MSE\": mse,\n",
    "        \"RMSE\": rmse,\n",
    "        \"R2 Score\": r2\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting loss curves for SGDRegressor\n",
    "if name == \"SGDRegressor\":\n",
    "        plt.plot(pipeline.named_steps['regressor'].loss_curve_)\n",
    "        plt.title(f\"Loss Curve for {name}\")\n",
    "        plt.xlabel(\"Iterations\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Model      MAE         MSE       RMSE  R2 Score\n",
      "0  LassoRegressor  7.75214  150.902247  12.284228  0.041252\n"
     ]
    }
   ],
   "source": [
    "# Convert results to DataFrame and display\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Feature Importance for DecisionTreeRegressor:\n",
      "                               Feature  Importance\n",
      "1                              fnlwgt    0.277459\n",
      "0                                 age    0.269155\n",
      "2                       education-num    0.047940\n",
      "64                           sex_Male    0.035852\n",
      "3                        capital-gain    0.024530\n",
      "..                                ...         ...\n",
      "82                native-country_Hong    0.000032\n",
      "81            native-country_Honduras    0.000021\n",
      "90                native-country_Laos    0.000009\n",
      "66            native-country_Cambodia    0.000001\n",
      "80  native-country_Holand-Netherlands    0.000000\n",
      "\n",
      "[107 rows x 2 columns]\n",
      "\n",
      "Feature Importance for RandomForestRegressor:\n",
      "                               Feature  Importance\n",
      "0                                 age    0.267404\n",
      "1                              fnlwgt    0.255812\n",
      "2                       education-num    0.046361\n",
      "3                        capital-gain    0.024938\n",
      "64                           sex_Male    0.021177\n",
      "..                                ...         ...\n",
      "99            native-country_Scotland    0.000046\n",
      "90                native-country_Laos    0.000025\n",
      "66            native-country_Cambodia    0.000016\n",
      "82                native-country_Hong    0.000014\n",
      "80  native-country_Holand-Netherlands    0.000000\n",
      "\n",
      "[107 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Feature Importance (for applicable models)\n",
    "for name, model in models.items():\n",
    "    if hasattr(model, \"feature_importances_\"):\n",
    "        pipeline = Pipeline([\n",
    "            (\"preprocessor\", preprocessor),\n",
    "            (\"regressor\", model)\n",
    "        ])\n",
    "        \n",
    "        pipeline.fit(X_train, y_train)\n",
    "        feature_importances = pipeline.named_steps['regressor'].feature_importances_\n",
    "        \n",
    "        feature_names = numerical_features + list(pipeline.named_steps['preprocessor'].transformers_[1][1].get_feature_names_out())\n",
    "        \n",
    "        importance_df = pd.DataFrame({\n",
    "            \"Feature\": feature_names,\n",
    "            \"Importance\": feature_importances\n",
    "        })\n",
    "        \n",
    "        importance_df = importance_df.sort_values(by=\"Importance\", ascending=False)\n",
    "        print(f\"\\nFeature Importance for {name}:\\n\", importance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best parameters for RandomForestRegressor: {'regressor__max_depth': 10, 'regressor__min_samples_split': 5, 'regressor__n_estimators': 200}\n",
      "Best score: 0.24574384862018187\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter Tuning for RandomForestRegressor using GridSearchCV\n",
    "param_grid = {\n",
    "    'regressor__n_estimators': [100, 200],\n",
    "    'regressor__max_depth': [None, 10, 20],\n",
    "    'regressor__min_samples_split': [2, 5]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"regressor\", RandomForestRegressor(random_state=42))\n",
    "]), param_grid, cv=3, n_jobs=-1)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(f\"\\nBest parameters for RandomForestRegressor: {grid_search.best_params_}\")\n",
    "print(f\"Best score: {grid_search.best_score_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Choices:\n",
    "\n",
    "For this task, I chose the SGDRegressor and DecisionTreeRegressor as the primary models due to their suitability for regression tasks. The SGDRegressor was chosen specifically because it is based on gradient descent, which makes it effective for large datasets and helps in minimizing the loss function iteratively. Decision Trees, on the other hand, were selected for their interpretability and their ability to handle non-linear relationships in the data.\n",
    "I also experimented with more complex models (Random Forests, Ridge, and Lasso regression), but I initially kept the focus on simpler models to establish a baseline before diving deeper into more advanced techniques.\n",
    "Loss Function Choices:\n",
    "\n",
    "I used Mean Squared Error (MSE) as the loss function for the regression task. MSE is a common choice because it penalizes larger errors more heavily, making the model sensitive to outliers. However, depending on the data characteristics, alternatives such as Huber Loss could be more robust for datasets with noise or outliers.\n",
    "I did not use alternatives like MAE or RMSE here because I was focusing on the comparative performance of models and ensuring consistency across all models.\n",
    "Results and Interpretation:\n",
    "\n",
    "The SGDRegressor outperformed the DecisionTreeRegressor based on the metrics (MAE, MSE, RMSE, RÂ²), which indicates that the gradient descent approach, despite its simplicity, is well-suited for the given dataset.\n",
    "The decision tree, while still providing decent results, showed a higher MSE and lower RÂ², suggesting it struggled with generalizing patterns from the data. This could be due to overfitting or insufficient depth in the tree.\n",
    "Conclusion:\n",
    "\n",
    "Based on the evaluation, the SGDRegressor is the preferred model, but further experimentation with hyperparameter tuning, feature engineering, and additional models could yield even better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Potential Areas for Improvement\n",
    "## Hyperparameter Tuning:\n",
    "\n",
    "One potential area for improvement is to apply Hyperparameter Tuning to optimize the performance of the models. Techniques like GridSearchCV or RandomizedSearchCV can be used to tune the parameters of the models, such as the learning rate for SGDRegressor or the depth of the tree for DecisionTreeRegressor. This could lead to better results by finding the most optimal settings for each model.\n",
    "## Feature Engineering:\n",
    "\n",
    "Another area to explore is feature engineering. The model could benefit from creating new features based on domain knowledge, such as interaction terms or polynomial features. Additionally, feature selection could be performed to eliminate irrelevant features that might be adding noise to the model.\n",
    "## Advanced Models:\n",
    "\n",
    "The exploration of more advanced models could also lead to improvements. For example, Random Forest Regression and Ridge or Lasso Regression could provide better generalization and better handling of overfitting. Random Forests, in particular, can provide feature importance, which can help with feature selection.\n",
    "Exploring ensemble methods (like Gradient Boosting or XGBoost) might improve performance further by combining the strengths of multiple models.\n",
    "## Outlier Handling:\n",
    "\n",
    "Depending on the nature of the dataset, handling outliers could be crucial. The models might perform poorly if outliers are not addressed properly. Implementing RobustScaler or using models like Huber Regressor can improve performance when the dataset has a lot of outliers.\n",
    "## Exploration of Other Loss Functions:\n",
    "\n",
    "The use of other loss functions, such as Huber Loss, could be beneficial for datasets with a significant number of outliers, as it is more robust than MSE."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
