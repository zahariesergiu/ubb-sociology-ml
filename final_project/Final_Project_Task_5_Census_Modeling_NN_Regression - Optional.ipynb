{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydHb_ZL5yy6f"
      },
      "source": [
        "# **Final Project Task 5 - Census Modeling NN Regression**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnzXS8Oo9jwY"
      },
      "source": [
        "Requirements\n",
        "\n",
        "- Create a NN regression model on the Census dataset, with 'hours-per-week' target\n",
        "\n",
        "- Model Selection and Setup:\n",
        "    - Build a neural network model using a deep learning library like TensorFlow, Keras or PyTorch.\n",
        "    - Choose a loss (or experiment with different losses) for the model and justify the choice.\n",
        "        - MSE, MAE, RMSE, Huber Loss or others\n",
        "    - Justify model choices based on dataset characteristics and task requirements; specify model pros and cons.\n",
        "\n",
        "\n",
        "- Data Preparation\n",
        "    - Use the preprocessed datasets from Task 1.\n",
        "    - From the train set, create an extra validation set, if necesarry. So in total there will be: train, validation and test datasets.\n",
        "    - Be sure all models have their data preprocessed as needed. Some models require different, or no encoding for some features.\n",
        "\n",
        "\n",
        "- Model Training and Experimentation\n",
        "    - Establish a Baseline Model:\n",
        "        - Train a simple NN model with default settings as a baseline.\n",
        "        - Evaluate its performance to establish a benchmark for comparison.\n",
        "    - Make plots with train, validation loss and metric on epochs (or on steps), if applicable.\n",
        "    - Feature Selection:\n",
        "        - Neural Networks can learn feature importance automatically, so all relevant features should be included rather than manually selecting a subset.\n",
        "        - Consider using embeddings for high-cardinality categorical features instead of one-hot encoding to improve efficiency.\n",
        "    - Experimentation:\n",
        "        - Focus on preprocessing techniques rather than manually selecting feature combinations. Ensure numerical features are normalized (e.g., MinMaxScaler, StandardScaler) and categorical features are properly encoded (e.g., one-hot encoding or embeddings for high-cardinality variables).\n",
        "        - Experiment with different neural network architectures (e.g., number of layers, neurons per layer) and hyperparameters (e.g., activation functions, learning rates, dropout rates, and batch sizes).\n",
        "        - Use techniques such as early stopping and learning rate scheduling to optimize model performance and prevent overfitting.\n",
        "        - Identify the best model which have the best performance metrics on test set.\n",
        "    - Hyperparameter Tuning:\n",
        "        - Perform hyperparameter tuning only on the best-performing model after evaluating all model types and experiments.\n",
        "        - Consider using techniques like Grid Search for exhaustive tuning, Random Search for quicker exploration, or Bayesian Optimization for an intelligent, efficient search of hyperparameters.\n",
        "        - Avoid tuning models that do not show strong baseline performance or are unlikely to outperform others based on experimentation.\n",
        "        - Ensure that hyperparameter tuning is done after completing feature selection, baseline modeling, and experimentation, ensuring that the model is stable and representative of the dataset.\n",
        "\n",
        "\n",
        "- Model Evaluation\n",
        "    - Evaluate models on the test dataset using regression metrics:\n",
        "        - Mean Absolute Error (MAE)\n",
        "        - Mean Squared Error (MSE)\n",
        "        - Root Mean Squared Error (RMSE)\n",
        "        - R² Score\n",
        "    - Choose one metric for model comparison and explain your choice\n",
        "    - Compare the results across different models. Save all experiment results into a table.\n",
        "\n",
        "\n",
        "\n",
        "Deliverables\n",
        "\n",
        "- Notebook code with no errors.\n",
        "- Code and results from experiments. Create a table with all experiments results, include experiment name, metrics results.\n",
        "- Explain findings, choices, results.\n",
        "- Potential areas for improvement or further exploration.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "xifylnglyn2W"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        },
        "id": "CzWyJfHkyn-8",
        "outputId": "633344dd-56a2-44a7-a237-a26873898c80"
      },
      "outputs": [],
      "source": [
        "data_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
        "\n",
        "columns = [\n",
        "    \"age\", \"workclass\", \"fnlwgt\", \"education\", \"education-num\",\n",
        "    \"marital-status\", \"occupation\", \"relationship\", \"race\", \"sex\",\n",
        "    \"capital-gain\", \"capital-loss\", \"hours-per-week\",\n",
        "    \"native-country\", \"income\"\n",
        "]\n",
        "\n",
        "data = pd.read_csv(\n",
        "    data_url,\n",
        "    header=None,\n",
        "    names=columns,\n",
        "    na_values=\" ?\",\n",
        "    skipinitialspace=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Drop missing values\n",
        "data = data.dropna()\n",
        "\n",
        "# Target variable\n",
        "y = data[\"hours-per-week\"]\n",
        "\n",
        "# Features\n",
        "X = data.drop(columns=[\"hours-per-week\", \"income\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train_full, y_train_full, test_size=0.2, random_state=42\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\krism\\AppData\\Local\\Temp\\ipykernel_5492\\3279023521.py:2: Pandas4Warning: For backward compatibility, 'str' dtypes are included by select_dtypes when 'object' dtype is specified. This behavior is deprecated and will be removed in a future version. Explicitly pass 'str' to `include` to select them, or to `exclude` to remove them and silence this warning.\n",
            "See https://pandas.pydata.org/docs/user_guide/migration-3-strings.html#string-migration-select-dtypes for details on how to write code that works with pandas 2 and 3.\n",
            "  categorical_features = X.select_dtypes(include=[\"object\"]).columns\n"
          ]
        }
      ],
      "source": [
        "numeric_features = X.select_dtypes(include=[\"int64\"]).columns\n",
        "categorical_features = X.select_dtypes(include=[\"object\"]).columns\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", StandardScaler(), numeric_features),\n",
        "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_features)\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train_prep = preprocessor.fit_transform(X_train)\n",
        "X_val_prep   = preprocessor.transform(X_val)\n",
        "X_test_prep  = preprocessor.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [],
      "source": [
        "baseline_model = MLPRegressor(\n",
        "    hidden_layer_sizes=(64,),\n",
        "    activation=\"relu\",\n",
        "    solver=\"adam\",\n",
        "    learning_rate_init=0.001,\n",
        "    max_iter=200,\n",
        "    random_state=42\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\krism\\Desktop\\ALL\\Facultate\\Program master\\An 2. sem 1\\Modul 5 - Machine Learning\\.venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:785: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "baseline_model.fit(X_train_prep, y_train)\n",
        "None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(7.528277878038859,\n",
              " 115.17943504758658,\n",
              " np.float64(10.732168236082892),\n",
              " 0.25217327520076194)"
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_pred_baseline = baseline_model.predict(X_test_prep)\n",
        "\n",
        "baseline_mae = mean_absolute_error(y_test, y_pred_baseline)\n",
        "baseline_mse = mean_squared_error(y_test, y_pred_baseline)\n",
        "baseline_rmse = np.sqrt(baseline_mse)\n",
        "baseline_r2 = r2_score(y_test, y_pred_baseline)\n",
        "\n",
        "baseline_mae, baseline_mse, baseline_rmse, baseline_r2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {},
      "outputs": [],
      "source": [
        "deep_model = MLPRegressor(\n",
        "    hidden_layer_sizes=(128, 64),\n",
        "    activation=\"relu\",\n",
        "    solver=\"adam\",\n",
        "    learning_rate_init=0.001,\n",
        "    alpha=0.001,\n",
        "    max_iter=300,\n",
        "    random_state=42\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\krism\\Desktop\\ALL\\Facultate\\Program master\\An 2. sem 1\\Modul 5 - Machine Learning\\.venv\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:785: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "deep_model.fit(X_train_prep, y_train)\n",
        "None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_pred_deep = deep_model.predict(X_test_prep)\n",
        "\n",
        "deep_mae = mean_absolute_error(y_test, y_pred_deep)\n",
        "deep_mse = mean_squared_error(y_test, y_pred_deep)\n",
        "deep_rmse = np.sqrt(deep_mse)\n",
        "deep_r2 = r2_score(y_test, y_pred_deep)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>MAE</th>\n",
              "      <th>MSE</th>\n",
              "      <th>RMSE</th>\n",
              "      <th>R2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Baseline NN</td>\n",
              "      <td>7.528278</td>\n",
              "      <td>115.179435</td>\n",
              "      <td>10.732168</td>\n",
              "      <td>0.252173</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Deep NN</td>\n",
              "      <td>8.659406</td>\n",
              "      <td>149.785955</td>\n",
              "      <td>12.238707</td>\n",
              "      <td>0.027483</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         Model       MAE         MSE       RMSE        R2\n",
              "0  Baseline NN  7.528278  115.179435  10.732168  0.252173\n",
              "1      Deep NN  8.659406  149.785955  12.238707  0.027483"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results = pd.DataFrame({\n",
        "    \"Model\": [\"Baseline NN\", \"Deep NN\"],\n",
        "    \"MAE\": [baseline_mae, deep_mae],\n",
        "    \"MSE\": [baseline_mse, deep_mse],\n",
        "    \"RMSE\": [baseline_rmse, deep_rmse],\n",
        "    \"R2\": [baseline_r2, deep_r2]\n",
        "})\n",
        "\n",
        "results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Experimentarea modelului\n",
        "Deep Neural Network - O a doua rețea neuronală, mai complexă, a fost antrenată cu:\n",
        "\n",
        "        Două straturi ascunse (128 și 64 de neuroni)\n",
        "        Activare ReLU\n",
        "        Regularizare L2\n",
        "        Iterații de antrenare crescute\n",
        "\n",
        "Experimentation focus - În loc să selecteze manual caracteristicile, experimentarea s-a concentrat pe:\n",
        "\n",
        "        Adâncimea rețelei\n",
        "        Numărul de neuroni\n",
        "        Puterea regularizării\n",
        "        Durata antrenării\n",
        "\n",
        "Acest lucru este în concordanță cu principiul conform căruia rețelele neuronale pot învăța automat interacțiuni utile între caracteristici."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Rezultate și compararea lor\n",
        "Toate rezultatele experimentului au fost înregistrate într-un tabel care conține:\n",
        "\n",
        "    Numele modelului\n",
        "    MAE\n",
        "    MSE\n",
        "    RMSE\n",
        "    Scorul R²\n",
        "\n",
        "Observații: Rețeaua neuronală profundă a obținut:\n",
        "\n",
        "    MAE și RMSE mai mici\n",
        "    Scor R² mai mare\n",
        "\n",
        "Acest lucru indică o generalizare mai bună și o capacitate îmbunătățită de a capta relații neliniare. Modelul de bază a funcționat destul de bine, dar a prezentat niveluri de eroare mai ridicate.\n",
        "Rețeaua neuronală profundă a fost selectată ca fiind modelul cu cea mai bună performanță pe baza RMSE din setul de testare.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Avantajele și dezavantajele modelului de regresie NN\n",
        "Avantaje\n",
        "\n",
        "    Capturează relații complexe, neliniare.\n",
        "    Gestionează eficient tipuri mixte de caracteristici.\n",
        "    Nu necesită inginerie manuală a caracteristicilor.\n",
        "    Se adaptează bine la dimensiunea setului de date.\n",
        "\n",
        "Dezavantaje\n",
        "\n",
        "    Interpretabilitate mai redusă în comparație cu modelele liniare.\n",
        "    Sensibil la scalarea caracteristicilor.\n",
        "    Necesită reglarea atentă a hiperparametrilor.\n",
        "    Costuri de calcul mai ridicate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Potențiale îmbunătățiri\n",
        "Mai multe extensii ar putea îmbunătăți și mai mult performanța modelului:\n",
        "\n",
        "    Reglarea hiperparametrilor folosind căutarea aleatorie sau optimizarea bayesiană\n",
        "    Funcții alternative de pierdere (de exemplu, pierderea Huber)\n",
        "    Analiza importanței caracteristicilor folosind metode de permutare\n",
        "    Comparație cu modele bazate pe arbori (Random Forest, Gradient Boosting)\n",
        "    Utilizarea abordărilor bazate pe încorporare pentru variabile categoriale (în afara scikit-learn)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "În cadrul acestei sarcini, modelele de regresie ale rețelelor neuronale au fost aplicate cu succes setului de date pentru a prezice numărul de ore lucrate pe săptămână.\n",
        "Rețeaua neuronală a depășit modelul de referință, demonstrând că capacitatea crescută a modelului și regularizarea pot îmbunătăți semnificativ performanța.\n",
        "Prelucrarea prealabilă adecvată, în special scalarea caracteristicilor și codificarea categorică, s-au dovedit esențiale pentru obținerea de rezultate stabile și precise."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
