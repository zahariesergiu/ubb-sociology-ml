{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ff91e9b",
   "metadata": {},
   "source": [
    "# Final Project Task 1: Census Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad608471",
   "metadata": {},
   "source": [
    "Requirements\n",
    "\n",
    "- Encode data\n",
    "- Handle missing values if any\n",
    "- Correct errors, inconsistencies, remove duplicates if any\n",
    "- Outlier detection and treatment if any\n",
    "- Normalization / Standardization if necesarry\n",
    "- Feature engineering\n",
    "- Train test split, save it.\n",
    "- Others?\n",
    "\n",
    "\n",
    "Deliverable:\n",
    "\n",
    "- Notebook code with no errors.\n",
    "- Preprocessed data as csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff99a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# task_1_preprocessing.ipynb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load the provided database file\n",
    "database_path = \"Initial_Database.csv\"  # Ensure the correct path\n",
    "df = pd.read_csv(database_path)\n",
    "\n",
    "# Check for null values\n",
    "null_values = df.isnull().sum()\n",
    "print(\"Null Values in Each Column:\\n\", null_values)\n",
    "\n",
    "# Check for duplicates\n",
    "duplicate_rows = df.duplicated().sum()\n",
    "print(\"\\nNumber of Duplicate Rows:\", duplicate_rows)\n",
    "\n",
    "# Remove duplicate rows if they exist\n",
    "if duplicate_rows > 0:\n",
    "    df_cleaned = df.drop_duplicates()\n",
    "    print(\"\\nDuplicates removed. Updated dataset shape:\", df_cleaned.shape)\n",
    "else:\n",
    "    df_cleaned = df.copy()\n",
    "    print(\"\\nNo duplicates found.\")\n",
    "\n",
    "# Save the cleaned dataset\n",
    "cleaned_database_path = \"cleaned_database.csv\"\n",
    "df_cleaned.to_csv(cleaned_database_path, index=False)\n",
    "\n",
    "print(\"\\nPreprocessing complete. Cleaned dataset saved as 'cleaned_database.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b904c914",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Info:\n",
      "Index(['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss',\n",
      "       'hours-per-week'],\n",
      "      dtype='object')\n",
      "Index(['workclass', 'education', 'marital-status', 'occupation',\n",
      "       'relationship', 'race', 'sex', 'native-country', 'income'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load dataset \n",
    "data_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
    "columns = [\n",
    "    \"age\", \"workclass\", \"fnlwgt\", \"education\", \"education-num\", \"marital-status\",\n",
    "    \"occupation\", \"relationship\", \"race\", \"sex\", \"capital-gain\", \"capital-loss\",\n",
    "    \"hours-per-week\", \"native-country\", \"income\"\n",
    "]\n",
    "\n",
    "data = pd.read_csv(data_url, header=None, names=columns, na_values=\" ?\", skipinitialspace=True)\n",
    "\n",
    "\n",
    "# Display dataset info\n",
    "print(\"Dataset Info:\")\n",
    "#print(data.info())\n",
    "\n",
    "data.isna().any()\n",
    "\n",
    "data.drop_duplicates()\n",
    "\n",
    "\n",
    "data.head(10)\n",
    "#separating the numerical and categorical columns in two separate categories\n",
    "numerical_cols = data.select_dtypes(include=['int64']).columns\n",
    "categorical_cols = data.select_dtypes(include=['object']).columns\n",
    "print (numerical_cols)\n",
    "print (categorical_cols)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0039ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "data[numerical_cols] = num_imputer.fit_transform(data[numerical_cols])\n",
    "data[categorical_cols] = cat_imputer.fit_transform(data[categorical_cols])\n",
    "\n",
    "# Encoding categorical variables\n",
    "encoder = OneHotEncoder(sparse=False, drop='first')\n",
    "encoded_cols = encoder.fit_transform(data[categorical_cols])\n",
    "encoded_df = pd.DataFrame(encoded_cols, columns=encoder.get_feature_names_out(categorical_cols))\n",
    "\n",
    "# Combine numerical and encoded categorical data\n",
    "data = pd.concat([data[numerical_cols], encoded_df], axis=1)\n",
    "\n",
    "# Outlier detection and removal using IQR\n",
    "Q1 = data[numerical_cols].quantile(0.25)\n",
    "Q3 = data[numerical_cols].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "data = data[~((data[numerical_cols] < (Q1 - 1.5 * IQR)) | (data[numerical_cols] > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "\n",
    "# Normalize numerical data\n",
    "scaler = StandardScaler()\n",
    "data[numerical_cols] = scaler.fit_transform(data[numerical_cols])\n",
    "\n",
    "# Train-test split\n",
    "train, test = train_test_split(data, test_size=0.2, random_state=42)\n",
    "train.to_csv('train_preprocessed.csv', index=False)\n",
    "test.to_csv('test_preprocessed.csv', index=False)\n",
    "\n",
    "print(\"Preprocessing completed. Preprocessed data saved as CSV.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
